[
  {
    "objectID": "projects/fitbit/watchtrain/index.html",
    "href": "projects/fitbit/watchtrain/index.html",
    "title": "ML watchtrain",
    "section": "",
    "text": "About\nThis repository contains code to stream ML training progress (currently only fastai) to a Fitbit Versa 3. More details on this project can be found in my blog post.\n\n\n\nThe project consists of three components, which can be found in the github repository:\n\nAn API server that coordinates the communication between ML training and the watch.\nCode that runs on the Fitbit Versa 3 to display the training progress received from the API server.\nA websocket logger that uses fastai‚Äôs callback architecture to fetch training progress and metrics to send to the API server. The WebsocketLogger was heavily influenced by the original fastai CSVLogger and Progress Callback.\n\n\n\nHow to run it\nThe repository contains a Dockerfile that launches the API server. The fitbit code needs to be be deployed to the watch via Fitbit Studio. The API server address can be adjusted in the /fitbit/companion/index.js. A running example on how to use the fastai Callback for training can also be found in the repository.\n\nThe watch, the API server and the training scripts should run in the same network, if run locally. On a local net the subnet mask needs to be 192.168.0.*. Alternatively the API server needs to run on a trusted HTTPS host. (see blog post)\n\n\n\nReferences\n\nBlog post about this project\nFastapi Docker setup: https://fastapi.tiangolo.com/deployment/docker/\nFitbit IDE: https://studio.fitbit.com/\nFastai CSVLogger and Progress Callback: https://docs.fast.ai/callback.progress.html"
  },
  {
    "objectID": "projects/calliope_mini/calliopris/index.html",
    "href": "projects/calliope_mini/calliopris/index.html",
    "title": "Calliopris",
    "section": "",
    "text": "About\nGame for Calliope Mini.\nGot some fun out of the 5x5 display. I bet you know how to play this one ;).\nI just tried out the Calliope Mini. I decided to start with a homage to an endlessly played game from my childhood.\n\nRestart: 0+GND\nleft: A\nright: B\ndown (fast): A+B"
  },
  {
    "objectID": "projects/calliope_mini/callirun/index.html",
    "href": "projects/calliope_mini/callirun/index.html",
    "title": "Callirun",
    "section": "",
    "text": "About\nJump ‚Äòn‚Äô run game for Calliope Mini\n\n\nHow to play\nJump over the obstacles. Obstacle sizes:\n\nPurple: 2\nBlue: 4\nGreen: 7\nYellow: 10\nOrange: 13\nRed: 15\n\n\n\nKeys\n\nB: Jump\nShake: Restart"
  },
  {
    "objectID": "projects/calliope_mini/callipianotech/index.html",
    "href": "projects/calliope_mini/callipianotech/index.html",
    "title": "Callipianotech",
    "section": "",
    "text": "About\nCalliope Mini as a learning epiano. The piano knows six tones and contains three learning modes. The project was a first time afternoon coding session with my daughter :).\n\n\nHow it works\nFirst choose a learning mode (toggle with A or B):\n\n‚ÄúP‚Äù: Play the tones as recorded\n‚ÄúM‚Äù: Play 20 of the recorded tones in a Monte Carlo-style simulation. The tones as randomly played by recorded frequency. For example, if C is played 3 times and D is played 6 times during recording session. The probability of C being played is 1/3.\n‚ÄúB‚Äù: Play 20 of the recorder tones in a Baysiean-style simulation. The tones are randomly played, but depend on the frequency of recordings after the previous tone. For example, if the recording is C-D-E-F-G-G-E then the probability of G beeing played after F is 100%, but after G there is a probability of G or E beeing played of 50 % each.\n\nPress A+B to select a learning mode.\nThen record some tones.\nKeys:\n\nA: Tone C\nP0: Tone D\nP1: Tone E\nP2: Tone F\nP3: Tone G\nB: Tone A\n\nPress A+B to stop recording and start playing the record depending on the chosen learning mode."
  },
  {
    "objectID": "projects/calliope_mini/calliobingo/index.html",
    "href": "projects/calliope_mini/calliobingo/index.html",
    "title": "Calliobingo",
    "section": "",
    "text": "About\n\n\n\nThe kids wanted to play Bingo. Unfortunately we don‚Äôt own a bingo game with numbers. So we made the Calliope Mini draw bingo numbers randomly.\n\n\nHow to play\n\npress A+B: Toggle View\n\nDrawing view: Draw and show a number out of a bag (default 25 numbers).\n\npress A: draw Number\npress B: show last drawn number\n\nBoard view: Show numbers already being drawn. Left upper corner = 1. Right lower corner 25.\n\npress A: Page to next board page (if bag is bigger then 25 numbers). Page 1 (1-25), Page 2 (26-50) ‚Ä¶\npress B: Page to preceding board page.\n\n\n\n\n\nConfiguration\nTo change the amount of numbers in the bag increase the bagsize variable, e.g.\nlet bagsize = 50\nYou can increase the bag size up to 100."
  },
  {
    "objectID": "projects/art/playing-with-rob/index.html#edgeprocessor",
    "href": "projects/art/playing-with-rob/index.html#edgeprocessor",
    "title": "Playing with Rob",
    "section": "EdgeProcessor",
    "text": "EdgeProcessor\nThe program was written in June 2014 and cleaned up a bit in 2019. It was a small private side project to try out some image processing techniques and learn about edge detection. The code is writen in Java (I didn‚Äôt know much about Python or cv2 in 2014). The code is rather slow and memory consuming.\nThe algorithm was used by my brother and me in August 2016 in an art project.\n\n\n\n\nHow to run\nPut a picture in the pix folder (e.g.¬†Test.jpg). Run App.class Test.jpg. The generated pictures will be placed in the pix folder. Use small image sizes only due to memory consumption.\nRed dots: start/end point of an edge\nExample\nInput:\n\n\n\nOutput:\n\n\n\nMixed:"
  },
  {
    "objectID": "snippets.html",
    "href": "snippets.html",
    "title": "Snippets",
    "section": "",
    "text": "This is a collection of helpful code snippets.\n\n\n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\nCategories\n\n\n\n\n\n\nNumeric Dataset\n\n\nGeneric dataset for numbers\n\n\nPytorch,ML\n\n\n\n\nPandas styles\n\n\nPut some charts and colour into pandas tables.\n\n\nPandas,Layout\n\n\n\n\nRecursive SQL\n\n\nLooping in SQL\n\n\nSQL,Sqlite,Oracle,SQL Magic\n\n\n\n\nMelt & pivot\n\n\nHow to rotate tables üö¶ üö• with pandas\n\n\nPandas,Python\n\n\n\n\nUn/pivot\n\n\nHow to rotate tables üö¶ üö• with sql\n\n\nSQL,Oracle\n\n\n\n\nPartitioned Join\n\n\nFilling gaps without cross join\n\n\nSQL,Oracle\n\n\n\n\nUser defined aggregation function\n\n\nBuildung a customized Recall function\n\n\nSQL,Oracle,ML,Metric\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "legal/impressum.html",
    "href": "legal/impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "üá∫üá∏ üá©üá™\n\nKontakt\n\n\n\n\n\nVerantwortlicher i.S.d. ¬ß 18 Abs. 2 MStV:\nJohannes Tomasoni"
  },
  {
    "objectID": "legal/privacy.html",
    "href": "legal/privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "üá∫üá∏ üá©üá™\n\nWebsite and Blog Comments\nThe comments are stored by the utteranc github plug-in as issues in my public github repo https://github.com/joatom/blog. The comments are accessable by everybody. If you want me to delete a comment you wrote, please reach out to me and I will remove it as soon as possible.\n\n\nData collection\nThis site is hosted on GitHub Pages. Your IP address is logged and stored for security purposes, regardless of whether you have signed into GitHub or not. For more information about GitHub‚Äôs security practices, see GitHub Privacy Statement. More details on processing terms can be found at https://docs.github.com/en/site-policy/privacy-policies. I can‚Äôt influence the data collection by the host and I don‚Äôt have access to the data."
  },
  {
    "objectID": "legal/imprint.html",
    "href": "legal/imprint.html",
    "title": "Imprint",
    "section": "",
    "text": "üá∫üá∏ üá©üá™\n\nContact\n\n\n\n\n\nResponsible for content\nJohannes Tomasoni"
  },
  {
    "objectID": "legal/datenschutz.html",
    "href": "legal/datenschutz.html",
    "title": "Datenschutzerkl√§rung",
    "section": "",
    "text": "üá∫üá∏ üá©üá™\n\nWebsite- und Blog-Kommentare\nDie Kommentare werden mit Hilfe des utteranc-github-plug-ins als Issues in meinem √∂ffentlichen Github-Repository https://github.com/joatom/blog gespeichert. Die Kommentar sind f√ºr jeden zug√§nglich. Falls ich einen deiner Kommentare l√∂schen soll, lass es mich wissen und ich werde ihn schnellst m√∂glich entfernen.\n\n\nDatensammlung- und verarbeitung\nDie Seite wird auf GitHub Pages gehosted. Beim Besuch der Seite wird deine IP-Adresse erfasst und aus Sicherheitsgr√ºnden gespeichert, unabh√§ngig ob Du gerade bei Github eingeloggt bist oder nicht. Weitere Inforamtionen zu GitHub‚Äôs Sicherungsverfahren findest Du unter GitHub Privacy Statement. Zus√§tzliche Information zur Datenverarbeitung sind hier beschrieben: https://docs.github.com/en/site-policy/privacy-policies. Ich habe keine Einfluss auf die Datenerfassung durch den Host und kann die Daten nicht einsehen."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This are some of my hobby projects.\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\nML watchtrain\n\n\nStreaming ML training progress to a Fitbit\n\n\n\n\n\n¬†\n\n\n\nDec 6, 2019\n\n\nCallipianotech\n\n\nCalliope Mini as a learning epiano\n\n\n\n\n\n\n\nNov 3, 2019\n\n\nCalliobingo\n\n\nBingo game for Calliope Mini\n\n\n\n\n\n¬†\n\n\n\nOct 3, 2018\n\n\nCallirun\n\n\nCalliope jumo n run fun\n\n\n\n\n\n¬†\n\n\n\nSep 11, 2018\n\n\nCalliopris\n\n\nGame of falling blocks on Calliope Mini\n\n\n\n\n\n\n\nJun 1, 2014\n\n\nPlaying with Rob\n\n\nLearning about edge detection algorithms\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-01-31-learn_along/index.html",
    "href": "posts/2022-01-31-learn_along/index.html",
    "title": "Learning along",
    "section": "",
    "text": "It‚Äôs said, that the most effective way to learn on kaggle is to participate in a competition and afterwards wrangling the top solutions. At the current state (as of January 2022) of my ML skills this approach doesn‚Äôt work well for me. 1. Top notebook solutions are often complicated, contain implementations that are difficult to grasp or are made out of tons of ensembled models. Solutions in the discussion section contain broad overviews that are inspiring, yet hard to rebuild. 2. The solutions are often different from my own solution. It is difficult to compare them with my own code and to derive how to improve my own code specifically.\nThe second best way to learn on kaggle is to follow the discussions and read the notebooks during a competition. This approach suites me better, since there are often small insights to be discovered that are shared by others and that I can easily integrated in my solution. Filtering the valuable information can be hard, because the notebook and discussion sections are often flooded with similar content. You can easily get distracted by too many different techniques and ideas.\nI often enjoy the Playground competitions to focus on a few skills to improve. They are also less challenging and it is easier to get a baseline implementation up running because the data is already prepared to quickly get started.\nIn January 2022 there is a community competition hosted by Abhishek Thakur, that provides the opportunity for yet another learning approach. During the competition there are sessions being recoreded on YouTube, with two Grandmasters covering the topics EDA and Imputation. So we get high quality guidance for two important topics while working on the competition. I took the opportunity to learn along.\nI wrote down my experience with this learning approach and shared it with my solution in the following notebook."
  },
  {
    "objectID": "posts/2023-01-03-fill-her-up/index.html",
    "href": "posts/2023-01-03-fill-her-up/index.html",
    "title": "Fill‚Äôer up!",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis blog post is not a financial advice! This is a toy example. This blog post is full of unrealistic assumptions. All numbers are made up. Reach out to a professional financial advisor you trust, if you need financial advise. And most important, take a spreadsheet and do the math yourself with your numbers and your assumptions.\n\n\n\nA (not that) funny joke\nOnce upon a time, in the last century, a teacher of mine was complaining about the latest increase of the gasoline price. The old man was finishing his complains with a misogynistic joke:\nüêó: The gas price went up a gain. It doesn‚Äôt matter for my wife, though. She always refuels for 10 bugs. Hahaha.\nBesides he shouldn‚Äôt have talked like this about his wife, the idea of always spending the same amount of money on the gas station to avoid price raises seamed to be pretty funny. I since used a variation of this da(e)d joke every now and then, like ‚ÄúI always refuel for 10 bugs, lol.‚Äù and you certainly have heard this joke often, too.\n\n\n\n\nImage generated with Stable Diffusion v2 (Rombach et al.¬†(2022))  ‚ÄúGas station at night as pixel graphics.‚Äù\n\nNow, 25 years later ‚Ä¶\nI was driving with my wife to a party and she realized that I‚Äôve filled up the car after shopping in the afternoon. She was very happy that she wouldn‚Äôt need to stop at the gas station during the upcoming week. I told here proudly that the price for gas was low and I saved 30 ct per liter compared to four weeks ago. Once again I quietly had to think about this old joke. Suddenly, when I was just about to grin, I was struck by a though: ‚ÄúIs it possible that the person, who is always refueling for exactly 10 bugs, is right and the other person, that is laughing about this strategy, is actually the idiot?!‚Äù.\nThis thought come up because the night before I‚Äôve watched some YouTube videos about investment strategies. One example was to follow a saving plan and put the same amount of money into ETFs on a regular basis, say 100 Euro every month. In the interview a question come up: What would happen when the market goes down and the portfolio looses value? The answer was pretty simple: If you invest regularly then you hit the market when it goes up the one day and when it goes down the other day. So the volatility doesn‚Äôt matter that much on the long run. This intuitively made sense to me: The regular investments smooth out the volatility. Since you can‚Äôt predict the stock prices, it‚Äôs hard to time the market (predicting when buying is cheap over a short period of time).\nSo, I left the party with two thoughts to investigate on:\n\n\nWhat is a suitable refueling strategy?\n\n\nIs my intuition about the constant ETF investment reasonable?\n\n\nThe first question is tackled in this blog post. The second one I examined in another blog post.\n\n\nRefueling Strategies\nThese are the average prices for fuel in Germany from January 2021 to November 2022 from Statistisches Bundesamt (Destatis) (2022).\n\n\n\n\n\n2021 was a pretty normal year, the fuel prices slightly increased, with some noisy up and downs. 2022 was really crazy with high raises and steep dives of the fuel price.\nSince our car is using diesel I‚Äôll do the following simulations for diesel. Our car is driving approximately 12,000 km per year and has a consumption of 6.4 liters per 100 km. In our example we assume we drive the same distance with the same consumption on everyday of the year.\n\n\n12,000 km/year\n0.064 l/km\n2.104 l/day\nWe‚Äôll compare three different refueling strategies:\n\nStrategy 1: Fixed payment:\nWe refuel for exactly ‚Ç¨ 45 when tank has less than 10 liters. The tank has a capacity of 55 liters, hence fuel for ‚Ç¨ 45 fits when we refuel. This refers to the ‚Äújoke‚Äù strategy.\nStrategy 2: Regularly, fixed amount of fuel:\nIn this strategy we refill every 15 days the same amount of fuel regardless of the price. That‚Äôs viable, because we are consuming the same amount of fuel everyday.\nStrategy 3: Fill on lowest price:\nWe have a magic crystal ball and know every price in advance. So we (partially) refuel always on the lowest price in a 20 day window.\n\n\n\n\n\n\nThe chart shows the price of the consumed fuel on every day for each of the strategy. For example, when we refuel with strategy 2 on January 16th for ‚Ç¨ 1.24/l we‚Äôll consume diesel worth ‚Ç¨ 1.24/l for the next 15 days.\nObviously, strategy 3 always has the lowest price. The average price and the yearly consumption for the three strategies are:\n\n\n\nStrategy 1: ‚Ç¨ 1.3812/l => ‚Ç¨ 1,061/year\nStrategy 2: ‚Ç¨ 1.3829/l => ‚Ç¨ 1,062/year\nStrategy 3: ‚Ç¨ 1.3628/l => ‚Ç¨ 1,047/year\n\nWhich leads to a total saving of ‚Ç¨ 14.10/year if we could predict the cheapest price for the next 20 days compared to strategy 1 and ‚Ç¨ 15.38/year compared to strategy 2.\nThat doesn‚Äôt sound much, but it‚Äôs still a saving of approximately 1.33% or additional fuel for a 159 km drive.\nWhen we look a bit closer at the chart, we see that the best price strategy line is very fine grained and the other two strategies are more bumpy with long flat plateaus. This indicates, that to accomplish the best price strategy, we need to hit the gas station more often. Here are the number of pit stops needed for the three strategies:\n\nStrategy 1: 24 stops\nStrategy 2: 24 stops\nStrategy 3: 54 stops\n\nThis is a total different picture. To achieve a saving of ‚Ç¨ 14.10/year I would not only need the magic crystal ball, but I would also need to make 30 more stops at the gas station per year. Let‚Äôs say one stop takes 10 minutes (waiting in line, refueling, paying), that would result in an additional 5.00 hours per year spending at the gas station.\nDuring this additional 5.00 hours I can walk with my daughter at 4km/h her daily 2x2km march to kindergarten 5 times. Which is 20.0 km or a diesel saving of ‚Ç¨ 1.8 in strategy 1 and if I take insurance, taxes, regular maintenance and car depreciation into account it‚Äôs a saving of ‚Ç¨ 7.2 (with 0.36 ct/km in 2021).\n\n\nNow, that we have a picture of 2021 and can decide on our personal preference and clairvoyant abilities to abondon or to go for Strategy 3, we still don‚Äôt have a clear picture of Strategy 1 and 2. In 2021 they look pretty similar with the given parameters. But it‚Äôs also clear, that we would have to make more gas station stops, if we‚Äôd either lower the fixed paying amount of strategy 1 or if we‚Äôd shorten the refilling interval of strategy 2.\nLet‚Äôs see what happens when the prices go crazy like in 2022.\n\n\n\n\n\n\n\nThese are the numbers for 2022:\n\nStrategy 1: ‚Ç¨ 1.9517/l => ‚Ç¨ 1,499/year => 33 stops\nStrategy 2: ‚Ç¨ 1.9481/l => ‚Ç¨ 1,496/year => 24 stops\nStrategy 3: ‚Ç¨ 1.8818/l => ‚Ç¨ 1,445/year => 111 stops\n\nWhich result in savings and additional hours spent at the gas station as shown below:\n\n\n\n\n\n\n\nIn 2022 strategy 2 is superior to strategy 1. The higher number of gas station stops results from not adjusting the fixed payment amount to the higher prices.\n\n\nConclusion\nThe old joke is worn out.\n\nNo one can predict the prices precisely.\nWaiting at the gas station for a ‚Ç¨ 3-4/hours ‚Äúsalary‚Äù doesn‚Äôt make sense, if you can effort a car in Germany.\n\nAfter comparing the three strategies with the data from 2021 and 2022 I personally prefer strategy number 2. Which leads me to these life hacks:\n\n\n\n\n\n\nLife hacks\n\n\n\n\nDefault to not taking the car, instead\n\ncommute by walking, train, bus, bike, stay at home, share rides,\ngo shopping by walking, train, bus, bike, shop online,\ngo to kindergarten/School by walking, train, bus, bike.\n\nDon‚Äôt bother about the fuel price.\nRefuel the entire tank.\nRefuel early if you fear traffic jams in winter. A diesel fueled car is basically a moving heating. A diesel car only uses about half of the fuel provided energy to actually move the car (‚ÄúWirkungsgrade von Elektroautos  T√úV NORD Mobilit√§t‚Äù (2023)). And even less if we don‚Äôt press the pedal all the way down. The rest results in heat. And if we leave the heating on, while waiting in traffic jam in winter, we can watch the tank go empty, quickly.\n\n\n\n\n\n\n\n\nReferences\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. ‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models.‚Äù In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10684‚Äì95. https://huggingface.co/stabilityai/stable-diffusion-2.\n\n\nStatistisches Bundesamt (Destatis). 2022. ‚ÄúKraftstoffpreise an √ñffentlichen Tankstellen.‚Äù https://www.dashboard-deutschland.de/indicator/tile_1667921381760?1671822853.\n\n\n‚ÄúWirkungsgrade von Elektroautos  T√úV NORD Mobilit√§t.‚Äù 2023. https://www.tuev-nord.de/de/privatkunden/verkehr/auto-motorrad-caravan/elektromobilitaet/wirkungsgrad/."
  },
  {
    "objectID": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html",
    "href": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html",
    "title": "A handful of bricks - from SQL to Pandas",
    "section": "",
    "text": "Original article published at datamuni.com."
  },
  {
    "objectID": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#conditional-join",
    "href": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#conditional-join",
    "title": "A handful of bricks - from SQL to Pandas",
    "section": "Conditional Join",
    "text": "Conditional Join\nThere is no intuitive way to do a conditional join on DataFrames. The easiest I‚Äôve seen so far is a two step solution. As substitution for the SQL WITH-clause we can reuse df_missing_parts.\n# 1. merge on the equal conditions\ndf_sets_with_missing_parts = df_inventory_list.merge(df_missing_parts, how = 'inner', on = ['part_num', 'color'], suffixes = ('_found', '_missing'))\n# 2. apply filter for the qreater equals condition\ndf_sets_with_missing_parts = df_sets_with_missing_parts[df_sets_with_missing_parts['quantity_found'] >= df_sets_with_missing_parts['quantity_missing']]\n\n# select columns\ncols = ['set_num', 'set_name', 'part_name', 'num_parts']\ndf_sets_with_missing_parts = df_sets_with_missing_parts[['set_name_missing'] + [c + '_found' for c in cols]]\ndf_sets_with_missing_parts.columns = ['searching_for_set'] + cols"
  },
  {
    "objectID": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#aggregation",
    "href": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#aggregation",
    "title": "A handful of bricks - from SQL to Pandas",
    "section": "Aggregation",
    "text": "Aggregation\nIn the next step the aggregation of the analytic function\nCOUNT(*) OVER (PARTITION BY il.set_num) matches_per_set\nneeds to be calculated. Hence the number of not-NaN values will be counted per SET_NUM group and assigned to each row in a new column (matches_per_set).\nBut before translating the analytic function, let‚Äôs have a look at a regular aggregation, first. Say, we simply want to count the entries per set_num on group level (without assigning the results back to the original group entries) and also sum up all parts of a group. Then the SQL would look something like this:\nSELECT s.set_num,\n       COUNT(*) AS matches_per_set\n       SUM(s.num_parts) AS total_num_parts\n  FROM ...\n WHERE ...\n GROUP BY \n       s.set_num;\nAll selected columns must either be aggregated by a function (COUNT, SUM) or defined as a group (GROUP BY). The result is a two column list with the group set_num and the aggregations matches_per_set and total_num_part.\nNow see how the counting is done with Pandas.\ndf_sets_with_missing_parts.groupby(['set_num']).count()  .sort_values('set_num', ascending = False)\n\n# for sum and count:\n# df_sets_with_missing_parts.groupby(['set_num']).agg(['count', 'sum']) \n\n\n\n\nsearching_for_set\nset_name\npart_name\nnum_parts\n\n\n\n\nset_num\n\n\n\n\n\n\nllca8-1\n1\n1\n1\n1\n\n\nllca21-1\n1\n1\n1\n1\n\n\nfruit1-1\n1\n1\n1\n1\n\n\nMMMB026-1\n1\n1\n1\n1\n\n\nMMMB003-1\n1\n1\n1\n1\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n10021-1\n1\n1\n1\n1\n\n\n088-1\n1\n1\n1\n1\n\n\n080-1\n2\n2\n2\n2\n\n\n066-1\n1\n1\n1\n1\n\n\n00-4\n1\n1\n1\n1\n\n\n\nWow, that‚Äôs different! The aggregation function is applied to every column independently and the group is set as row index. But it is also possible to define the aggregation function for each column explicitly like in SQL:\ndf_sets_with_missing_parts.groupby(['set_num'], as_index = False) \\\n    .agg(matches_per_set = pd.NamedAgg(column = \"set_num\", aggfunc = \"count\"), \n         total_num_parts = pd.NamedAgg(column = \"num_parts\", aggfunc = \"sum\"))\n\n\n\n\nset_num\nmatches_per_set\ntotal_num_parts\n\n\n\n\n0\n00-4\n1\n126\n\n\n1\n066-1\n1\n407\n\n\n2\n080-1\n2\n1420\n\n\n3\n088-1\n1\n615\n\n\n4\n10021-1\n1\n974\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n463\nMMMB003-1\n1\n15\n\n\n464\nMMMB026-1\n1\n43\n\n\n465\nfruit1-1\n1\n8\n\n\n466\nllca21-1\n1\n42\n\n\n467\nllca8-1\n1\n58\n\n\n\nThis looks more familiar. With the as_index argument the group becomes a column (rather than a row index).\nSo, now we return to our initial task translating the COUNT(*) OVER(PARTITION BY) clause. One approach could be to join the results of the above aggregated DataFrame with the origanal DataFrame, like\ndf_sets_with_missing_parts.merge(my_agg_df, on = 'set_num')\nA more common why is to use the transform() function:\n# add aggregatiom\ndf_sets_with_missing_parts['matches_per_set'] = df_sets_with_missing_parts.groupby(['set_num'])['part_name'].transform('count')\n\ndf_sets_with_missing_parts.head(5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nsearching_for_set\nset_num\nset_name\npart_name\nnum_parts\nmatches_per_set\n\n\n\n\n0\nHeartlake Pizzeria\n00-4\nWeetabix Promotional Windmill\nSlope 45¬∞ 2 x 2\n126\n1\n\n\n1\nHeartlake Pizzeria\n066-1\nBasic Building Set\nSlope 45¬∞ 2 x 2\n407\n1\n\n\n2\nHeartlake Pizzeria\n080-1\nBasic Building Set with Train\nSlope 45¬∞ 2 x 2\n710\n2\n\n\n3\nHeartlake Pizzeria\n088-1\nSuper Set\nSlope 45¬∞ 2 x 2\n615\n1\n\n\n4\nHeartlake Pizzeria\n10021-1\nU.S.S. Constellation\nSlope 45¬∞ 2 x 2\n974\n1\n\n\n\nLet‚Äôs elaborate the magic that‚Äôs happening.\ndf_sets_with_missing_parts.groupby(['set_num'])['part_name']\nreturns a GroupByDataFrame which contains the group names (from set_num) and all row/column indicies and values related to the groups. Here only one column ['part_name'] is selected. In the next step transform applies the given function (count) to each column individually but only with the values in the current group. Finaly the results are assigned to each row in the group as shown in Fig. 4.\n\nFig. 4: Aggregation with transform\nNow that we have gathered all the data we arange the results so that they can be compared to the SQL data:\n# sort and pick top 16\ndf_sets_with_missing_parts = df_sets_with_missing_parts.sort_values(['matches_per_set', 'num_parts', 'set_num', 'part_name'], ascending = [False, True, True, True]).reset_index(drop = True).head(16)\n\ndf_sets_with_missing_parts\n\n\n\n\n\n\n\n\n\n\n\n\n\nsearching_for_set\nset_num\nset_name\npart_name\nnum_parts\nmatches_per_set\n\n\n\n\n0\nHeartlake Pizzeria\n199-1\nScooter\nSlope 45¬∞ 2 x 2\n41\n2\n\n\n1\nHeartlake Pizzeria\n199-1\nScooter\nSlope 45¬∞ 2 x 2 Double Convex\n41\n2\n\n\n2\nHeartlake Pizzeria\n212-2\nScooter\nSlope 45¬∞ 2 x 2\n41\n2\n\n\n3\nHeartlake Pizzeria\n212-2\nScooter\nSlope 45¬∞ 2 x 2 Double Convex\n41\n2\n\n\n4\nHeartlake Pizzeria\n838-1\nRed Roof Bricks Parts Pack, 45 Degree\nSlope 45¬∞ 2 x 2\n58\n2\n\n\n5\nHeartlake Pizzeria\n838-1\nRed Roof Bricks Parts Pack, 45 Degree\nSlope 45¬∞ 2 x 2 Double Convex\n58\n2\n\n\n6\nHeartlake Pizzeria\n5151-1\nRoof Bricks, Red, 45 Degrees\nSlope 45¬∞ 2 x 2\n59\n2\n\n\n7\nHeartlake Pizzeria\n5151-1\nRoof Bricks, Red, 45 Degrees\nSlope 45¬∞ 2 x 2 Double Convex\n59\n2\n\n\n8\nHeartlake Pizzeria\n811-1\nRed Roof Bricks, Steep Pitch\nSlope 45¬∞ 2 x 2\n59\n2\n\n\n9\nHeartlake Pizzeria\n811-1\nRed Roof Bricks, Steep Pitch\nSlope 45¬∞ 2 x 2 Double Convex\n59\n2\n\n\n10\nHeartlake Pizzeria\n663-1\nHovercraft\nSlope 45¬∞ 2 x 2\n60\n2\n\n\n11\nHeartlake Pizzeria\n663-1\nHovercraft\nSlope 45¬∞ 2 x 2 Double Convex\n60\n2\n\n\n12\nHeartlake Pizzeria\n336-1\nFire Engine\nSlope 45¬∞ 2 x 2\n76\n2\n\n\n13\nHeartlake Pizzeria\n336-1\nFire Engine\nSlope 45¬∞ 2 x 2 Double Convex\n76\n2\n\n\n14\nHeartlake Pizzeria\n6896-1\nCelestial Forager\nSlope 45¬∞ 2 x 2\n92\n2\n\n\n15\nHeartlake Pizzeria\n6896-1\nCelestial Forager\nSlope 45¬∞ 2 x 2 Double Convex\n92\n2\n\n\n\n# assert equals\nif not USE_BIGQUERY:\n    sets_with_missing_parts = sets_with_missing_parts.DataFrame()\n    \npd._testing.assert_frame_equal(sets_with_missing_parts, df_sets_with_missing_parts)\nThe results are matching!\n: We got it. We can buy the small Fire Engine to fix the roof of the fireplace. Now need for a new Pizzeria. :-)\n: (#@¬ß?!*#) Are you sure your data is usefull for anything?"
  },
  {
    "objectID": "posts/2022-08-30-art_diffusion/index.html",
    "href": "posts/2022-08-30-art_diffusion/index.html",
    "title": "Art and Diffusion",
    "section": "",
    "text": "Human-machine interaction\nIn summer 2016 I spent a day with my brother at his studio in the Monastery of Bentlage to try out fine art printing. The monastery is in the countryside and is a very calming place to escape from everyday life. As preparation for the workshop my brother asked me to prepare a drawing. I‚Äôm not a good drawer, my hands are shaky and whatever comes out of the pencil never looks like what I intended to draw. Luckily I was experimenting with edge-detecting algorithms at that time. The algorithm I was implementing was suitable to create an image that looked like a drawing. I took a picture of my son where he was playing with the vacuum robot and run my program to extract the contours of my son and the vacuum robot. The setting where I generate a drawing with an algorithm and the theme of my son playing with a robot was very appealing. In the workshop we used the generated drawing as template for my artwork. The printing technique we applied to create the artwork is called etching. The classical look and feel of this centuries old technique was a nice contrast to the rather modern theme - a kid playing with a smart robot. Yet, it made the interaction between humans and robots look pretty normal. And I was proud of my amateurish accomplishments.\n\nA few weeks later I was attending a TEDx event in M√ºnster. One of the talks was given by the artist Roman Lipski and the art collective YQP. Surprisingly, they introduced their concept of art where the artist interacts with an Artificial Muse. The Muse (an ML algorithm) learned to imitate Lipski‚Äôs style of painting by ‚Äúanalysing‚Äù his artworks. The painter than used the generated images as inspiration (Muse) to paint new pictures on canvas. This was in 2016 shortly after google shared their Deep Dream accomplishments and Style Transfer was a hot topic in the tech community. Lipski and his team envisioned the natural interaction between AI and humans to create art. In 2016 I was not practicing ML yet, nor was I heavily involved into art, apart from seeing my brother from time to time. But after this TEDx talk it was obvious that computer generated images will have a high impact on arts in the future. But will it be interruptive in the sense that artists become obsolete?\n\n\nWhat is art? Or, why is art?\nThere are countless definitions and strong opinions about What is Art? and what not. Let‚Äôs ignore this question for now and rather focus on another question. Why does Art exists? As I said, I‚Äôm not an art professional, but a few answers are obvious to me.\nArt exists because of,\n\nthe human nature:\n\npeople love to be creative,\npeople love beauty,\npeople use different forms to express feelings,\npeople love stories and imagination\n\npolitical and sociological activities:\n\npeople, who are suppressed, can hide critics in art,\npeople use art to defame opponents,\npeople can visualize opinions ‚Ä¶\n\n\nThere certainly are many more categories and reasons why art exists. However, a few things stand out.\n\nArt is a highly influential medium to form opinions using the imaginary power of the viewer. To the point where the viewer‚Äôs imagination can go way beyond the obvious object. For example, if you look at a picture with a house in a landscape and the rich and shiny oil colors, your imagination can take you from the museum right to Tuscany.\nPeople also practice art for their own sake.\n\nSo why do we get confused about the question What is art? Some of the techniques that are used in art are also used for other purpose. Let‚Äôs take drawing as an example. An architect uses drawing technics to plot the layout of a house. The plot may including information about wall measures, connection for water and electricity, etc. Usually you wouldn‚Äôt consider that kind of plot as a piece of art, though it still requires a bit of imagination from the viewer. The technical drawing must be easy accessible for the interpreter and thus provides the obvious information (e.g.¬†measures of the wall in centimeters). However, sometimes a technical drawing can diffuses into art. although it still serves the original purpose. A technical drawing can be of such of an elegance that it my raise strong feelings in the viewer - or the drawer. Than it becomes art. Think of Christo‚Äôs drawings. Don‚Äôt you feel joy or fascination when you are looking at them?\nBack to the question, will advancements in AI/ML disrupt and takeover Art? No! Every argument I used to describe the purpose of art involves humans at some point. There is no perception of beauty or expression of feelings without humans nor is there any political or social implications without humans. Will AI/ML heavily influence the arts and creativity? Yes! We will gain an impressive tool set and new techniques. And we can see that Computers are already being creative in certain environments (e.g.¬†generative ML-models).\n\n\nDiffusion\nComputers are magical and support you being creative. I remember in the 90‚Äôs when we used to create flyers for our parties. The visual effects of the graphic programs were so fascinating that we had to use almost all of them. The flyers always ended up looking like invitations to psychedelic 70‚Äôs parties rather then to underground techno parties. Some 25 years later the algorithms are getting very sophisticated. Images can be created out of noise or text. Existing images can be transformed to adapt the style of another painter or to look totally different. At the time of writing this article Diffusion Models are trending. Their strength lies in modification of existing images by adding noise to the image and then transforming the image while reducing the added noise again. The models can be combined with text encoders. You can basically tell the algorithm how you want a given image to change and how you want it to look like.\nThis summer my brother called me because someone dropped out of a workshop he was offering. And if I wanted to join spontaneously. I signed up for the two day offside and because I missed the first day of the workshop I needed to come up with a good template quickly. I decided to use one of the publicly available Diffusion Models to get started. I picked Disco Diffusion, where a handy tutorial was available. I added one of my favorite photos of the kids to the model and as text input I asked the model to change the picture into something like ‚ÄúThree children waiting for a bus in a photorealistic cyberpunk town. Red, blue, black, dark cyberpunk scheme.‚Äù. I did a bit of parameter tuning and after two hours I had a few templates to work with on the workshop.\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal photo (censored)\n\n\nCombination of two generated images\n\n\n\n\nOn this workshop we applied screen printing technique and due to time restrictions we were limited to two (color) layers. I used the original photo and the generated images as inspiration for the print. I wouldn‚Äôt call it Muse in my case because it used the images as template with a few abstractions. But if I‚Äôd be a professional artist I would definitely use the algorithms somehow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor me as an amateur the fulfillment lays rather in the relaxing hours, escaping daily routines, interacting with people, while handcrafting some little artwork to use as Christmas card or to hang on my walls at home.\n\n\n\nResources\n\nHomepage Maximilian Tomasoni: http://www.maximilian-tomasoni.com/\nHomepage Monastery of Bentlage: https://www.kloster-bentlage.de/en/kunst-kultur-en/bentlage-print-society\nEdge-detecting algo for workshop in 2016: https://github.com/joatom/ART-playing-with-rob\nHow Art Meets Artificial Intelligence, YQP and Roman Lipski, TEDxM√ºnster, 2016. https://www.youtube.com/watch?v=oVE5rRJa0D8.\n‚ÄúInceptionism: Going Deeper into Neural Networks.‚Äù Accessed August 29, 2022. http://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html.\nGatys, Leon A., Alexander S. Ecker, and Matthias Bethge. ‚ÄúA Neural Algorithm of Artistic Style.‚Äù arXiv, September 2, 2015. https://doi.org/10.48550/arXiv.1508.06576.\nHo, Jonathan, Ajay Jain, and Pieter Abbeel. ‚ÄúDenoising Diffusion Probabilistic Models.‚Äù arXiv, December 16, 2020. https://doi.org/10.48550/arXiv.2006.11239.\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. ‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models.‚Äù arXiv, April 13, 2022. https://doi.org/10.48550/arXiv.2112.10752.\nQuick Start on Using AI to Render Images Using Disco Diffusion, 2022. https://www.youtube.com/watch?v=wIw59kAU6u8.\nhttps://github.com/alembics/disco-diffusion, and Katherine Crowson. ‚ÄúDisco Diffusion v5.61 - Now with Portrait_generator_v001,‚Äù n.d. https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb."
  },
  {
    "objectID": "posts/2021-04-14-nonlinearity/index.html",
    "href": "posts/2021-04-14-nonlinearity/index.html",
    "title": "Bending the space with nonlinearity",
    "section": "",
    "text": "I‚Äôm trying out a new kaggle feature which allows us to include kaggle notebooks in our personal blogs. This articel was initialy published as kaggle notebook in November 2020."
  },
  {
    "objectID": "posts/2021-05-28-hotel/index.html",
    "href": "posts/2021-05-28-hotel/index.html",
    "title": "Identifying Hotels",
    "section": "",
    "text": "‚ÄúRecognizing a hotel from an image of a hotel room is important for human trafficking investigations. Images directly link victims to places and can help verify where victims have been trafficked.‚Äù (Stylianou et al., 2019).\n\nAs part of the Eight Workshop on Fine-Grained Visual Categorization a kaggle competition was launched to support investigations by advancing models to identify hotels from images.\nThis post contains some parts of my contribution to the Hotel-ID to Combat Human Trafficking 2021 - FGVC8 kaggle competition.\n\nThe challenge\nThe competition contained 97000+ images of hotel rooms from 7700! different hotels around the world. The objective was to identify the hotels of 13000 images from the hidden test set. The metric of the competition was Mean Average Precision of the top 5 picks (MAP@5). My solution scored 14th place out of 92 teams with a 0.6164 MAP@5 on the private leaderboard.\nMy solution contained six CNN models with various configurations. More technical details and why I ended up with rather simple models is described in a kaggle discussion topic.\nHere I post the training and inference of one of the six models as well as the ensemble inference code.\n\n\nTraining\nThe final training was done on the entire train dataset. I didn‚Äôt choose a cross validation strategy to safe training time. To keep variance low nonetheless I relied on the usual regularization strategies, such as dropout and augmentation and in particular on test time augmentation during inference.\nTo refine the model a validation set can be created by setting the debug flag as described in the notebook. \n\n\nInference\n\n\n\n\nInference ensemble\n\n\n\n\nReferences\nStylianou, Abby and Xuan, Hong and Shende, Maya and Brandt, Jonathan and Souvenir, Richard and Pless, Robert (2019). Hotels-50K: A Global Hotel Recognition Dataset. The AAAI Conference on Artificial Intelligence (AAAI)"
  },
  {
    "objectID": "posts/2020-12-26-blog-translator/index.html",
    "href": "posts/2020-12-26-blog-translator/index.html",
    "title": "Automatically translate blog posts",
    "section": "",
    "text": "üá©üá™ üá∫üá∏\n\nAttention! This text has been automatically translated!\n\nSince I made so many mistakes in my first Blog post, I write this post in German and have it automatically translated.\nFor translation I use the popular NLP framework of huggingface.co. On their website is a simple example to implement a translation application and I will use it.\nAs expected, the Markdown syntax does not immediately work correctly when translating. So I had to make some adjustments at the beginning and afterwards.\nThe code (including pre- and post-processing) I used for the translation of the markdown files can be found here. But since it‚Äôs just a few lines of code, we can also look at it here:\nfrom transformers import MarianMTModel, MarianTokenizer \n \n# load pretrained model and tokenizer \nmodel_name = 'Helsinki-NLP/opus-mt-de-en' \ntokenizer = MarianTokenizer.from_pretrained(model_name) \nmodel = MarianMTModel.from_pretrained(model_name) \n \n# load german block post \nf_in = open(\"blog_translator_de.md\", \"r\") \nsrc_text = f_in.readlines() \nf_in.close() \n \n# preprocessing \n## line break (\\n) results to \"I don't know.\"  We make it more specific: \nsrc_text = [s.replace('\\n',' ') for s in src_text] \n \n## remove code block \ncode = [] \ninside_code_block = False \nfor i, line in enumerate(src_text): \n    if line.startswith('```') and not inside_code_block: \n        # entering codeblock \n        inside_code_block = True \n        code += [line] \n        src_text[i] = '<<code_block>>' \n    elif inside_code_block and not line.startswith('```'): \n        code += [line] \n        src_text[i] = '<<code_block>>' \n    elif inside_code_block and line.startswith('```'): \n        # leaving code block \n        code += [line] \n        src_text[i] = '<<code_block>>' \n        inside_code_block = False \n \n# translate \ntranslated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\")) \ntgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] \n \n# postprocessing \n## replace code_blog tags with code \nfor i, line in enumerate(tgt_text): \n    if line == '<<code_block>>': \n        tgt_text[i] = code.pop(0) \n \n## remove the eol (but keep empty list entries / lines) \ntgt_text = [s.replace('', '',) for s in tgt_text] \n## remove space between ]( to get the md link syntax right \ntgt_text = [s.replace('](', '](',) for s in tgt_text] \n \n# write english blog post \nwith open('2020-12-26-blog-translator.md', 'w') as f_out: \n    for line in tgt_text: \n        f_out.write(\"%s\\n\" % line) \nf_out.close() \nSince this is my first NLP application, I left it with this Hello World code. Surely there are clever ways to map the markdown syntax in tokenizer. Maybe I‚Äôll write a follow up when I find out.\nBy the way, the translation just made me adapt my German writing style. For example, sarcasm doesn‚Äôt work so well after translation, so I avoided it. Also, it often depends on the correct choice of words (e.g.¬†there is no markdown command, but there is markdown syntax). <\nBest regards\nJohannes & the Robot"
  },
  {
    "objectID": "posts/2022-11-25-project-success/index.html",
    "href": "posts/2022-11-25-project-success/index.html",
    "title": "Ingredients for successful projects",
    "section": "",
    "text": "In a nutshell üå∞ (tl;dr)\nAbove all,\nUSE COMMON SENSE\nwhen running projects!\nAdditionally, you can take some inspiration from the techniques described below. Make sure to always keep PROCESS, SCOPE and PEOPLE in mind when making project related decisions.\nHave fun!\n\n\n1 Focus\n[SCOPE]\nThe first thing to do, when a project is ready to start, is to define the scope. Depending on the project size and setup this can be done in various granularity.\nIn any case, I found it to be very useful to always make a very high level scope description. An easy way to implement it, is to draw a Focus Circle like this:\n\nMake sure the developers and product owner participate in the Focus Circle creating session.\nGrab a physical sheet of paper.\nDraw three circles like a dartboard.\nThink of your MVP (minimum viable product).\nNow, write some of the most important features in the innermost circle.\nContinue by writing some less important features in the intermediate circle.\nPut some nice-to-have features to the outer circle.\nThe feature list must not be complete. The listed features should rather be easy to grasp and be a great representation of mandatory, important and nice-to-have features.\nFinally, write some out-of-scope features at the borders of the page.\n\nNow, with the Focus Circle in place, you have a nice tool which can be used through-out the project. Bring it to the first couple of meetings and just pin it to the board or to the wall.\n\n\n\nThen, after a few meetings, let it disappear. When you are a couple of weeks into your project and tension rises, you disagree on the urgency of topics or the team gets stuck implementing off-topic features, then you take out the Focus Circle. Slam it on the desk and remind everybody what the major goal of the project is. I‚Äôm sure you‚Äôll be back on track, at least for a couple of hours or even days.\n\n\n2 Build a diverse Team\n[PEOPLE]\nHave you ever been in a team with lots of ‚Äúalphas‚Äù? I once competed in such an ‚Äúalpha‚Äù team against other more heterogeneous teams. Guess who did the worst?! If everyone wants to decide which dinner to cook, it is hard to agree on a recipe. And afterall, somebody still has to cut the onions and peel the potato.\nIn another project we started out with a team of highly qualified analysts and engineers. The teammates knew each other well and the hierarchies were clear. One seat still had to be filled for a project governance role. This role was assigned to a person we only had very few interactions with before. This person had different characteristics than the rest of us and therefore brought different perspectives to the project. Although the person wasn‚Äôt actually involved in building the product, it was crucial for the project success to have the person in the team.\nIt‚Äôs important to sometimes disrupt thinking in established ways, even if this thinking has been successful before. ‚ÄúWe‚Äôve always done it like this‚Ä¶‚Äù is the tombstone of innovation. Diverse teams will challenge this kind of thinking.\n\n\n3 The Currency of Business\n[PEOPLE]\nüí∞ You think money is the currency of business? - Nope.\nüî∏ Maybe rank? - No.\nüîÑ Then it must be a great network? - Not really.\nThe most valuable currency you can earn in business is, when people want to work with you AGAIN! Money, rank and network will eventually be the outcome, if people want to work with you again.\nWhat does it take for people to want to work with you again?\n\nTrust / reliability\nSympathy / being pleasant\nCompetence / skill\n\nWhat if you only want to work with highly performing teammates?! Here is the trick. Just add a great Environment to Trust, Sympathy and Competence and you‚Äôll have a great chance to see high performance happening in your team.\n\n\n4 Do Dailies, Retros & Planning, but smart!\n[PROCESS]\nJust do them!\nI know, some people hate meetings, especially if the schedule is filled up with meetings. I haven‚Äôt seen projects fail because of daily meetings, but I have seen projects fail because of the absence of regular meetings.\nBut be smart. Daily Meetings don‚Äôt have to be 15 minutes long. Choose what ever suites the project. Do the recurring meeting always on the same time and place. Rather skip one meeting then moving it around in the schedules. This reduces time wasting coordination.\nThere is also no need for a four hour planning session per sprint where everybody is applying to. The planning can be prepared by few relevant people and fine-tuned later on in a half-hour session with everybody being involved.\nRetros? Sure! Hence, no lessons learned, no improvement. No time for emotions, no chaka.\n\n\n5 Train or get help on central Issues\n[SCOPE]\nüêì: ‚ÄúI‚Äôve written HTML once before in the 90‚Äôs, I‚Äôll design the customers landing page.‚Äù\nüò∞: Are you kidding, right?\nüéÖ: ‚ÄúI‚Äôve organized the best Christmas party. Let me draw the BPMN for the main process.‚Äù\nüòü: Noway!\nWe all want to learn, improve and try out new things and it should be part of our job. But it‚Äôs not a good idea to start with critical steps. There is a reason, why people study and gather experience over time. Do some professional training (many of them are for free) or mix teams with experienced and curious people.\n\n\n6 Define Roles, explicitly\n[PEOPLE]\nMake sure everybody is aware of the role they are playing in the project. Not only define the name of the role (e.g.¬†Project Manager), but explicitly define the responsibilities and expectation of the role. Is the business analyst also responsible of writing a technical requirement? - Is the Product Owner joining the daily meetings or only the planning and the sprint review? - Is the project manager only doing governance or also reevaluating and sorting the tasks?\nThere is no one-fits-all answer to the duties of each role. Make sure everyone has the same understanding of the project members‚Äô roles, regardless of the name.\n\n\n7 Trouble with overlapping Roles\n[PROCESS]\nAlthough you may be able to play many different roles in a project, you should avoid to hold too many. Some roles include steady interaction with other people and lot‚Äôs of interruptions throughout the day. Some roles require long lasting focus time to ‚Äúgo deep‚Äù. Sometimes roles have opposite objectives, e.g.¬†implementing tasks perfectly vs.¬†get tasks finished quickly. Uniting to many roles in the same person can case contradictions and harm the project progress.\n\n\n\nAs techie, do not assume that team members that are not actively writing code are a waste of resources. If you have a great analyst, you can code in lightning speed. If someone properly prepares the tasks for the upcoming sprint, you‚Äôll save lots of time-consuming meetings. If you have a bouncer to talk to the stakeholders, you can stay focused.\n\n\n8 Over-Frameworkification\n[SCOPE]\nAvoid complex project management frameworks and don‚Äôt be overly academic about them. The way you run a project should adapt your style of working and your companies culture.\nSometimes it can be helpful to start with an ‚Äúacademic‚Äù approach and adapt a framework for your purpose. But more important than religiously following a framework, is to understand, why a framework defines certain parts and how the parts interact with each other. If you understand why frameworks specify certain actions and roles it is easy to adjust it to your purpose.\n\n\n9 Use Outsourcing\n[PROCESS]\nRemove distractions to keep focused on the project. Delegate duties that are not related to the project to your manager. The manager can manage them. There is a reason why there is a work in progress (WiP) limit, it reduces setup time.\n\n\n\nIf there are tasks in your project that nobody is comfortable with, find someone who can do it. For example, your team is all technical, the stakeholders don‚Äôt understand your language and you also get exhausted following there jargon. Try to recruit somebody to do the communication, there are people who love doing that, your team will shine and you don‚Äôt get distracted doing your job.\nüî®: Delegation, Work in Progress Limits, Recruiting\n\n\n10 Keep a Backlog\n[SCOPE]\nIts easy to loose track about what still needs to be done in the future. Gather the tasks that come up, you can still throw them away, if they get useless. Consider to define a proper User Story (Feature abc, for persona xyz, so that‚Ä¶), so you don‚Äôt end up implementing nonsense.\n\n\n11 Prioritize and reevaluate\n[SCOPE]\nPlanning is an ongoing process in a project. Prioritize important and critical tasks. Keep the big picture in mind. Reevaluate your priorizations regularly. Don‚Äôt forget to communicate your decisions.\n\n\n12 Get regular Feedback\n[SCOPE]\nTransparency is your friend. You want a buy-in from all your stakeholders. You might get hints about issues, you wouldn‚Äôt come up on your own. You also distribute some accountability to the stakeholders. Extract tasks from the feedback and put it in the backlog. Reevaluate your prioritization.\n\n\n13 To build Momentum, you have to start small\n[PROCESS]\nImagine, the first project sessions are over and you have gathered way too many tasks and feature requests. For sure there will be more while the project proceeds. Say, you start your first planning session with these parameters:\n\nProject duration: 3 Months\nFeature requests: 36\nSprint length: 2 weeks.\n\nAt this point it is likely that one of the Epic-Project-Fails will gonna happen:\nüêπ: 36 features in 6 sprints? We have to built 6 features per sprint! Which 6 do we pick for sprint 1?\nüêê: We have to pick at least 8, because there will be new tasks coming up along the way!\nNow you have to be very strong because your arguments are very counter intuitive, if you haven‚Äôt worked through it and experienced it for many times. So remember your mantras:\n\nYou want to start your project with an success.\nYour team is not properly formed yet and is certainly not riding this new project‚Äôs ‚Äúwave‚Äù, yet.\nYou haven‚Äôt thought off every pitfall in advance. E.g. your devops environment might change and on-boarding takes longer as expected.\nBe aware that humans think linearly not exponentially.\n\nNow, you should strongly insist on picking ‚Äúonly‚Äù 1-2 features for the first sprint! In case your team has an outstanding start and everything goes smooth, you still can grab an additional task throughout the sprint. Now, when you are all set and celebrate the success of the first sprint(s) you can gradually pick more task for the future sprints.\nYou build up momentum.\n\n\n\nSome new task will show up during the project and others will become obsolete and disappear. Stay focused and you will end up with an useful product after the three month. And whatever you end up with is exactly what three month of excellent work is worth it.\n\n\n14 Define Measures for the Goal, not the Spreadsheet\n[PROCESS]\nDon‚Äôt use story points!\nIf you are a software developer, you are in the creative business. If you do the same things over and over, you‚Äôll eventually end up automating them (e.g.¬†Templates, UnitTest, DevOps-Pipelines, ‚Ä¶). Hence most of the time you‚Äôll end up doing new things.\nVery often story points are implicitly defined as a time slot. So you end up planing time slots. How do you properly estimate time for work you never did before? Right, very vaguely! You always end up with over or under estimates. But that is not the problem, yet.\nThe problem arises when you link the story points to budgets and personal/team goals.\n\nPersonal/team goals:\nüë©: There are 3 more story points available for the next sprint. Which tasks should we add to the sprint?\nüê∞: Let‚Äôs squeeze in task xyz, it‚Äôs three easy story points.\nüêå: Maybe task abc is not really 2 points, but rather 3 points, let‚Äôs take that one!\nSee, they are fooling themselves. Ok, can‚Äôt happen to you. Because you have a strict planning process where the story points are not renegotiated, when filling the sprint backlog. So your negotiation happens before, when estimating the story points? - Zero-sum. By the way, does anyone still want‚Äôs to work after 4 hours of ‚Äúplanning‚Äù/negotiating?\nBudget:\nüë®: Are you sure getting coffee for the team is only 1 story point? Add some buffer and make it 3, I don‚Äôt want endless discussions with the budgeting team why we are off the plan.\nOnce, again. They fooled themselves, and the budgeting team.\n\nAt this point the agile mindset comes in handy: fixed time, fixed resources, variable outcome.\nüêó: You agile folks are so naive. You need a budget to build your product. And you have to make a commitment and promise a certain outcome to get a budget.\n\nNaive?\n\nNo!\n\nNeed a budged?\n\nYes!\n\nMake commitment?\n\nYes, we commit to give our blood, sweat and tears for the best possible outcome, given time and money.\n\nPromise certain outcome\n\nWe promise best possible outcome, given time and money. To archive this we are constantly reevaluating the project progress using full transparency, prioritizing and reevaluating and including regular feedback from stakeholders. This mitigates the risk for the person providing the budget.\n\n\nIf you have a healthy relationship and want to increase the currency of business you are willing to accept. It needs a lot of trust and the risk remains on the site that provides the budget and the decision makers. That‚Äôs where the risk belongs too.\nWhat if you can‚Äôt convince the budget site to agree on an uncertain outcome? One possibility is to use outsourcing. There are many people who love to do communication. And very often there is a way to ‚Äúspin‚Äù things so everybody looks (and sometimes even feels) like a winner. Stay focused on the project and leave politics to those who enjoy it.\nWhich measures to use instead? Define your measure as complexities rather than timely estimates. Some examples:\n\nDid we do something like this before? - Yes / No\nDo others do this? - Yes / No\nHow clear is the request? - Shallow / crystal clear / to be defined\nWill the expert be available to answer questions? - Sometimes / instant / on holiday\netc.\n\n\n\n15 Always be inclusive\n[PEOPLE]\nHow does it feel to be left out, when decisions are made that effect your tasks? What happens to the motivation of an expert, when the expert is not invited or ignored on discussion of their field?\nIn some case the excluded person feels transactional, they might think:\nüòë: Alright, I just do my job, don‚Äôt ask questions and take the money.\nBut it can also lead to self-doubt.\nüòï: I get mobbed. - My expertise is not as valuable as I thought. - I‚Äôm not making an impact.\nOr result in a ‚ÄúTHEY‚Äù-kind of thinking instead of ‚ÄúWE‚Äù.\nüòí: They probably didn‚Äôt want to wast my time. - They try to hold me short. - They are stupid, anyway. - They can do their shit on their own.\nIt‚Äôs clear that exclusion leads to a decline of trust and loss of identification with the project.\nIf you want people to engage, you have to be inclusive. That doesn‚Äôt mean, I have to be involved in any decision at any time. But I want to have the chance to be involved in topics that are important to me and my work.\nBut what if you don‚Äôt want somebody else involved, like in the planning meeting example? The major part of the planning was done by very few people, to be more efficient. But why not add another person to be more inclusive.\nAlso, be nice to support stuff. a) It‚Äôs the right thing to do! b) It will come around. (Don‚Äôt you want the secretary to warn you, if it‚Äôs ‚Äúnot a good time to knock on that door to ask for a favor‚Äù and guess who doesn‚Äôt make it to the email distribution when you behave like a jerk.)\n\n\n16 Let Outsiders join your Meetings\n[PEOPLE]\nConsider to open your meetings to people outside of the project. Your company‚Äôs working culture could improve. You share ideas and might get some useful feedback from the visitors or insides by visiting others‚Äô meetings.\nIf you fear to be disturbed by the outsiders during the meetings, just set up some rules (e.g.¬†‚ÄúVisitors shouldn‚Äôt actively participate. I‚Äôll talk about your questions afterwards.‚Äù ).\n\n\n17 Watch for Breaks and Needs\n[PEOPLE]\nIf the project really goes well and people are in a flow, the chances are high that people loose track of their health (food, sleep and exercise) and their social well-being (family and friends).\nWe all love the flow! But be aware that you are running a marathon in a project and you still need power and support when situations in project occur that don‚Äôt go smoothly. Besides, you might put pressure on your mates when you are staying late every day.\nIf somebody drops out, it puts a huge burden on the project. You have to restuff, reorganize and eventually find back to the flow. You‚Äôll lose focus in that time.\n\n\n18 Celebrate üéâ\n[PROCESS]\nIf you reach certain milestones, celebrate! Even if the team is tired of the project and everybody is happy that it‚Äôs finally over. At least go out for lunch or dinner with the project mates.\nThere is nothing more hamsterwheelish than going from one project to the next one without a clear breaking event, such as a celebration.\nIt also effects your currency of business. Like the dentist, who gives candy to a kid after a dental session. It makes the kids come back again, with less fear.\nEnding an exhausting project with a nice event is a psychological trick, too. Looking back, the last impression and feelings are those who stick later on."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#who-are-we",
    "href": "posts/2022-04-28-ml-pm/index.html#who-are-we",
    "title": "Tackling Projects like a ML Challenge",
    "section": "Who are we?",
    "text": "Who are we?\nThe dynamics of a team can be described in Tuckman (1965)‚Äôs four stages,\n\nforming, where group members try ‚Äúto identify the boundaries of both interpersonal and task behaviors‚Äù,\nstorming, where interpersonal conflicts arise, hence everyone searches for her/his role in the team,\nnorming, where everyone settles for her/his role in the team and new group ‚Äùstandards evolve‚Äù,\nperforming, where finally ‚Äùthe group energy is channeled into the task‚Äù and the group works efficiently.\n\nBecause everyone in our team was short in time we tried to skip the Storming and Norming stage and go straight to Performing. But Tuckman‚Äôs stages seem the work like physical laws, a group can‚Äôt circumvent certain stages. Even if formal hierarchies already exist, humans need to build trust first and find their role in the team. This is only possible through human interaction, which takes time. Though, the Storming and Norming phase can be very short if you take on a very standardized role to execute very standardized work. Which wasn‚Äôt the case in our small creative (non standardized) project."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#who-is-it-for",
    "href": "posts/2022-04-28-ml-pm/index.html#who-is-it-for",
    "title": "Tackling Projects like a ML Challenge",
    "section": "Who is it for?",
    "text": "Who is it for?\nWe all had a common sense of the technical topic we were supposed to cover. But the field of the technical topic is huge, with all levels of difficulties. At this point there also was only a vague definition of the audience of the curriculum."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#how-are-we-gonna-make-it",
    "href": "posts/2022-04-28-ml-pm/index.html#how-are-we-gonna-make-it",
    "title": "Tackling Projects like a ML Challenge",
    "section": "How are we gonna make it?",
    "text": "How are we gonna make it?\nWe also didn‚Äôt talk in advance what steps we would take to reach our goal. Hence, we didn‚Äôt set a framework.\nThe overall situation could basically be summarized as: > A bunch of ambitious strangers from different cultural background were short in time and wanted to solve a vaguely defined task without a framework."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#obstacle-1-being-short-on-time",
    "href": "posts/2022-04-28-ml-pm/index.html#obstacle-1-being-short-on-time",
    "title": "Tackling Projects like a ML Challenge",
    "section": "Obstacle 1: being short on time",
    "text": "Obstacle 1: being short on time\nThe obvious adjustment would have been to take more time for the task at hand. Unfortunately this was not the preferred option in this case, because we volunteered for a side project that shouldn‚Äôt badly effect the regular work we had to do. But we could save some time by avoiding ineffective meetings, hence being well prepared when attending a meeting. ## Obstacle 2: working in a new team The time restriction was hard, so we sacrificed team building. That‚Äôs always a huge risk and relies on the hope that everyone acts professionally and has the same intrinsic motivation. Luckily, it worked out well. ## Obstacle 3: vague goals We had to define the goals more precisely. ## Obstacle 4: missing framework We had to define a structure on how to approach the task. Given the time limit and the interpersonal constraints this was the part with the highest chance to make a positive impact on the outcome of our project. And that‚Äôs what the rest of this blog will focus on."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#state-your-overall-goal",
    "href": "posts/2022-04-28-ml-pm/index.html#state-your-overall-goal",
    "title": "Tackling Projects like a ML Challenge",
    "section": "1. State your overall goal üñºÔ∏è",
    "text": "1. State your overall goal üñºÔ∏è\nWrite the foremost goal down in huge letters and pin it on the wall. A classifier to predict flowers. Every decision you make from now on must follow this goal. If you lose yourself in details during the process you can always refocus by looking at the wall reading the huge letters stating your overall goal. ## 2. Re/define the objectives üñåÔ∏è Figure out what is important and what not. Define the objectives precisely (e.g.¬†through User Stories). ‚ÄùAs a cook I want the classifier to recognize all herbs in my garden, so that I don‚Äôt use the poisoning ones in my meals.‚Äù ## 3. Look at the data üîé Check the quality and structure of the data. Do exploratory data analysis (EDA). ‚ÄùSome of the pictures of the flowers and herbs are zoomed in, some are in panorama view and show many flowers. Classes are balanced.‚Äù ## 4. Define metrics üéØ Decide what your algorithm should optimize for. The cook will zoom in closely the take the picture. And since the training targets are well balanced, we choose Accuracy as metric. ## 5. Define test strategy üìã With the knowledge from EDA we decide on a test strategy and identify what is missing to implement the test strategy. ‚ÄúA stratified-KFold-split makes sense, but the labels still need some cleanup.‚Äù ## 6. Define architecture üèóÔ∏è We always want to start with a simple baseline in the first iteration. Later-on the architecture can get more complex, e.g.¬†through ensembling. Let‚Äôs start with a Resnet18. ## 7. Prepare data üë∑ Clean and enrich the data. There is enough data. Remove the panorama shots. Clean the labels. Define the train/test/validation data sets. ## 8. Execute ‚ñ∂Ô∏è Build a model and train. ## 9. Evaluate üìä Evaluate the model against the metrics. Accuracy is only 80%. Let‚Äôs look at the most miss classified data and try to change the architecture in the next iteration.\nI follow this procedure when I‚Äôm participating in Kaggle competition almost every time and I‚Äôm sure others follow similar approaches. Depending on how well prepared the data already is, some steps may be executed in slightly different order, e.g.¬†architecture choice can also happen after data preparation. But for sure you want to have a clear picture from the very beginning for what to optimize for and how you evaluate it."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-the-overall-goal",
    "href": "posts/2022-04-28-ml-pm/index.html#define-the-overall-goal",
    "title": "Tackling Projects like a ML Challenge",
    "section": "1. Define the overall goal üñºÔ∏è",
    "text": "1. Define the overall goal üñºÔ∏è\nWe want to collect learning material so that people with different skill-sets and backgrounds can learn about the technical topic to use on in their jobs."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#redefine-the-objectives",
    "href": "posts/2022-04-28-ml-pm/index.html#redefine-the-objectives",
    "title": "Tackling Projects like a ML Challenge",
    "section": "2. Re/define the objectives üñåÔ∏è",
    "text": "2. Re/define the objectives üñåÔ∏è\nThe audience consists of highly educated professionals, that need the technical knowledge for different reasons. Some need a deep technical understanding to use it as a Software Developer, some need a broad overview for their role as Project Manager and some just want to learn about the topic out of curiosity. Because of the heterogeneity of the audience I found it suitable to define different Personas. E.g. Andrea, Project Manager, doesn‚Äôt like to be fooled by the nerds during the meetings when they talk in their jargon. or Peter, Java evangelist, curious about that hot topic everybody is talking about. ‚ÄúCan‚Äôt be that hard, to just learn another language.‚Äù. With the personas in place we created categories where we could instantly assign the resources we already had collected. The human characteristics of the personas also lead to an imagination where the collected resources eventually fit to the the stories of the Personas we had in our minds.\nIn a later iteration we refined the objectives and added two other kind of categories. The one kind of category was concerning the knowledge depth (beginner, intermediate and advanced). The other kind of category distinguished between a business role and a tech role."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#look-at-the-data",
    "href": "posts/2022-04-28-ml-pm/index.html#look-at-the-data",
    "title": "Tackling Projects like a ML Challenge",
    "section": "3. Look at the data üîé",
    "text": "3. Look at the data üîé\nSince we were limited on time we couldn‚Äôt do a deep analysis of every resource. If resources where known in advance by one of the team members we had to rely on her/his judgment. If resources were unknown we briefly scanned them or relied on public recommendations. It‚Äôs fare from optimal, but it must have been sufficient for a first iteration."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-metrics",
    "href": "posts/2022-04-28-ml-pm/index.html#define-metrics",
    "title": "Tackling Projects like a ML Challenge",
    "section": "4. Define Metrics üéØ",
    "text": "4. Define Metrics üéØ\nWe defined soft metrics implicitly.\n\nDo resources fit a Persona?\nAre the curriculum suitable for a certain category (e.g.¬†beginner)?\nAre the resources accessible?\nAre the resources of high quality?\nIs a resource complementing other resources or can it replace other resources in the curriculum?"
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-test-strategy",
    "href": "posts/2022-04-28-ml-pm/index.html#define-test-strategy",
    "title": "Tackling Projects like a ML Challenge",
    "section": "5. Define Test strategy üìã",
    "text": "5. Define Test strategy üìã\nWe had chosen feedback as our test strategy. Feedback was gatherd from outsiders that were not involved in the creation process."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-architecture",
    "href": "posts/2022-04-28-ml-pm/index.html#define-architecture",
    "title": "Tackling Projects like a ML Challenge",
    "section": "6. Define architecture üèóÔ∏è",
    "text": "6. Define architecture üèóÔ∏è\nThe resources were collected in a wiki style manner that was easily accessible. The curriculum and its resources were structured by the categories defined in step 1."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#prepare-data",
    "href": "posts/2022-04-28-ml-pm/index.html#prepare-data",
    "title": "Tackling Projects like a ML Challenge",
    "section": "7. Prepare data üë∑",
    "text": "7. Prepare data üë∑\nCollect the resources. Scan the resources. Save references. Check accessibility."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#execute",
    "href": "posts/2022-04-28-ml-pm/index.html#execute",
    "title": "Tackling Projects like a ML Challenge",
    "section": "8. Execute ‚ñ∂Ô∏è",
    "text": "8. Execute ‚ñ∂Ô∏è\nBuild a baseline model by choosing the relevant resources, sort them by category and write them down in the wiki. Make the page look nice and welcoming."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#evaluate",
    "href": "posts/2022-04-28-ml-pm/index.html#evaluate",
    "title": "Tackling Projects like a ML Challenge",
    "section": "9. Evaluate üìä",
    "text": "9. Evaluate üìä\nGet feedback from stakeholders that are not part of the creation process."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#iterate",
    "href": "posts/2022-04-28-ml-pm/index.html#iterate",
    "title": "Tackling Projects like a ML Challenge",
    "section": "10. Iterate üîÅ",
    "text": "10. Iterate üîÅ\nAfter a few iterations we had a nice curriculum. Additionally we encourage the audience to extend the wiki page."
  },
  {
    "objectID": "posts/2023-12-29-pr-curve/2023-12-29-interactive-precision-recall-curve.html",
    "href": "posts/2023-12-29-pr-curve/2023-12-29-interactive-precision-recall-curve.html",
    "title": "Interactive Precision-Recall Curve",
    "section": "",
    "text": "Recently I talked about metrics for inbalanced binary classification problems.\nI had trouble properly explaining the Precision-Recall Curve in simple words.\nSometimes a nice visualization tells more than a thousand words.\nSo, I made this simple interactive chart:\n\n\n\n\n\n\n\n\n\\(Precision=\\frac{TP}{TP+FP}\\)\n\\(Recall=\\frac{TP}{TP+FN}\\)\n\\(TPR=\\frac{TP}{TP+FN}\\)\n\\(FPR=\\frac{FP}{TN+FP}\\)\n\nReferences\n\nFormulas: F-score\nOriginal notebook: Interactive Precision-Recall Curve"
  },
  {
    "objectID": "posts/2021-11-22-vpp-seq/2021-11-22-vpp-seq.html",
    "href": "posts/2021-11-22-vpp-seq/2021-11-22-vpp-seq.html",
    "title": "Notes about Sequence Modelling",
    "section": "",
    "text": "I lately participated in the Google Brain - Ventilator Pressure Prediction competition. I didn‚Äôt score decent, but I still learned a lot. And some of it is worth to sum up, so I can easily look it up later.\nThe goal of the competition was to predict airway pressure of lungs that are ventilated in a clinician-intensive procedure. Given values of the input pressure (u_in) we had to predict the output pressure for a time frame of a few seconds.\n\n\n<AxesSubplot:xlabel='time_step', ylabel='Pressure'>\n\n\n\n\n\nSince all u_in values for a time frame were given we can build a bidirectional sequence model. Unless in a typical time-series problem where the future points are unknown at a certain time step, we know the future and past input values. Therefore I decided not to mask the sequences while training.\nA good model choice for sequencing tasks are LSTMs and Transformers. I built a model that combines both architectures. I also tried XGBoost with a lot of features (especially windowing, rolling, lead, lag features) engineering, But neural nets (NN) performed better, here. Though I kept some of the engineered features as embeddings for the NN model.\nThe competition metric was mean average error (MAE). Only those pressures were evaluated, that appear while filling the lungs with oxygen.\n\n\nBesides the given features, u_in, u_out, R, C and time_step I defined several features. They can by categorized as:\n\narea (accumulation of u_in over time) from this notebook\none hot encoding of ventilator parameters R and C\nstatistical (mean, max, skewness, quartiles, rolling mean, ‚Ä¶)\nshifted input pressure\ninput pressure performance over window\ninverse features\n\nTo reduce memory consumption I used a function from this notebook.\n\ndef gen_features(df, norm=False):\n    \n    # area feature from https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153\n    df['area'] = df['time_step'] * df['u_in']\n    df['area_crv'] = (1.5-df['time_step']) * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['area_crv'] = df.groupby('breath_id')['area_crv'].cumsum()\n    df['area_inv'] = df.groupby('breath_id')['area'].transform('max') - df['area']\n    \n    df['ts'] = df.groupby('breath_id')['id'].rank().astype('int')\n    \n    df['R4'] = 1/df['R']**4\n    df['R'] = df['R'].astype('str')\n    df['C'] = df['C'].astype('str')\n    \n    df = pd.get_dummies(df)\n    \n    for in_out in [0,1]: #,1\n        for qs in [0.2, 0.25, 0.5, 0.9, 0.95]:\n            df.loc[:, f'u_in_{in_out}_q{str(qs*100)}'] = 0\n            df.loc[df.u_out==in_out, f'u_in_{in_out}_q{str(qs*100)}'] = df[df.u_out==in_out].groupby('breath_id')['u_in'].transform('quantile', q=0.2)\n    \n        for agg_type in ['count', 'std', 'skew','mean', 'min', 'max', 'median', 'last', 'first']:\n            df.loc[:,f'u_out_{in_out}_{agg_type}'] = 0\n            df.loc[df.u_out==in_out, f'u_out_{in_out}_{agg_type}'] = df[df.u_out==in_out].groupby('breath_id')['u_in'].transform(agg_type)\n    \n        if norm:\n            df.loc[:,f'u_in'] = (df.u_in - df[f'u_out_{in_out}_mean']) / (df[f'u_out_{in_out}_std']+1e-6)\n    \n    \n    for s in range(1,8):\n        df.loc[:,f'shift_u_in_{s}'] = 0\n        df.loc[:,f'shift_u_in_{s}'] = df.groupby('breath_id')['u_in'].shift(s)\n        df.loc[:,f'shift_u_in_m{s}'] = 0\n        df.loc[:,f'shift_u_in_m{s}'] = df.groupby('breath_id')['u_in'].shift(-s)\n    \n    df.loc[:,'perf1'] = (df.u_in / df.shift_u_in_1).clip(-2,2)\n    df.loc[:,'perf3'] = (df.u_in / df.shift_u_in_3).clip(-2,2)\n    df.loc[:,'perf5'] = (df.u_in / df.shift_u_in_5).clip(-2,2)\n    df.loc[:,'perf7'] = (df.u_in / df.shift_u_in_7).clip(-2,2)\n    \n    df.loc[:,'perf1'] = df.perf1-1\n    df.loc[:,'perf3'] = df.perf3-1\n    df.loc[:,'perf5'] = df.perf5-1\n    df.loc[:,'perf7'] = df.perf7-1\n    \n    df.loc[:,'perf1inv'] = (df.u_in / df.shift_u_in_m1).clip(-2,2)\n    df.loc[:,'perf3inv'] = (df.u_in / df.shift_u_in_m3).clip(-2,2)\n    df.loc[:,'perf5inv'] = (df.u_in / df.shift_u_in_m5).clip(-2,2)\n    df.loc[:,'perf7inv'] = (df.u_in / df.shift_u_in_m7).clip(-2,2)\n    \n    df.loc[:,'perf1inv'] = df.perf1inv-1\n    df.loc[:,'perf3inv'] = df.perf3inv-1\n    df.loc[:,'perf5inv'] = df.perf5inv-1\n    df.loc[:,'perf7inv'] = df.perf7inv-1\n    \n    df.loc[:,'rol_mean5'] = df.u_in.rolling(5).mean()\n    \n    return df\n\n\n\nThe data was transformed with scikit‚Äôs RobustScaler to reduce influence of outliers.\n\nfeatures =  list(set(train.columns)-set(['id','breath_id','pressure','kfold_2021','kfold']))\nfeatures.sort()\n\nrs = RobustScaler().fit(train[features]) \n\n\n\n\n\nI didn‚Äôt do cross validation here, but instead trained the final model on the entire dataset. Nevertheless it‚Äôs helpful to build kfolds for model evaluation. I build GroupKFold over breath_id to keep the entire time frame in the same fold.\n\n\n\nSince the data is quite small (ca. 800 MB after memory reduction) I decided to load the entire train set in the Dataset object during construction (calling __init__()). In a first attempt I loaded the data as Pandas Dataframe. Then I figured out (from this notebook) that converting the Dataframe into an numpy array speeds up training significantly. The Dataframe is converted to an numpy array by the scaler.\nSince the competition metric only evaluates the pressures where u_out==0 I also provide a mask tensor, which can later on be used feeding the loss and metric functions.\n\nclass VPPDataset(torch.utils.data.Dataset):\n    \n    def __init__(self,df, scaler, is_train = True, kfolds = [0], features = ['R','C', 'time_step', 'u_in', 'u_out']):\n        if is_train:\n            # build a mask for metric and loss function\n            self.mask = torch.FloatTensor(1 - df[df['kfold'].isin(kfolds)].u_out.values.reshape(-1,80))\n            self.target = torch.FloatTensor(df[df['kfold'].isin(kfolds)].pressure.values.reshape(-1,80))\n            \n            # calling scaler also converts the dataframe in an numpy array, which results in speed up while training\n            feature_values = scaler.transform(df[df['kfold'].isin(kfolds)][features]) \n            \n            self.df = torch.FloatTensor(feature_values.reshape(-1,80,len(features)))\n            \n        else:\n            self.mask = torch.FloatTensor(1 - df.u_out.values.reshape(-1,80))\n            \n            feature_values = scaler.transform(df[features]) \n            self.df = torch.FloatTensor(feature_values.reshape(-1,80,len(features)))\n            \n            self.target = None\n        \n        self.features = features\n        self.is_train = is_train\n        \n    def __len__(self):\n        return self.df.shape[0]\n        \n    def __getitem__(self, item):\n        sample = self.df[item]\n        mask = self.mask[item]\n        if self.is_train:\n            targets = self.target[item]\n        else:\n            targets = torch.zeros((1))\n        \n        return torch.cat([sample, mask.view(80,1)],dim=1), targets #.float()\n\n\n\n\nMy model combines a multi layered LSTM and a Transformer Encoder. Additionally I build an AutoEncoder by placing a Transformer Decoder on top of the Transformer encoder. The AutoEncoder predictions are used as auxiliary variables.\n\nSome further considerations:\n\nI did not use drop out. The reason why it performence worse is discussed here.\nLayerNorm can be used in sequential models but didn‚Äôt improve my score.\n\nThe model is influenced by these notebooks:\n\nTransformer part\nLSTM part\nParameter initialization\n\n\n# Influenced by: \n# Transformer: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n# LSTM: https://www.kaggle.com/theoviel/deep-learning-starter-simple-lstm\n# Parameter init from: https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization \n\nclass VPPEncoder(nn.Module):\n\n    def __init__(self, fin = 5, nhead = 8, nhid = 2048, nlayers = 6, seq_len=80, use_decoder = True):\n        super(VPPEncoder, self).__init__()\n                 \n        self.seq_len = seq_len\n        self.use_decoder = use_decoder\n        \n        # number of input features\n        self.fin = fin\n                      \n        #self.tail = nn.Sequential(\n        #    nn.Linear(self.fin, nhid),\n        #    #nn.LayerNorm(nhid),\n        #    nn.SELU(),\n        #    nn.Linear(nhid, fin),\n        #    #nn.LayerNorm(nhid),\n        #    nn.SELU(),\n        #    #nn.Dropout(0.05),\n        #)                \n            \n        encoder_layers = nn.TransformerEncoderLayer(self.fin, nhead, nhid , activation= 'gelu')\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n        \n        decoder_layers = nn.TransformerDecoderLayer(self.fin, nhead, nhid, activation= 'gelu')\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, nlayers)\n        \n        self.lstm_layer = nn.LSTM(fin, nhid, num_layers=3, bidirectional=True)\n        \n        \n        # Head\n        self.linear1 = nn.Linear(nhid*2+fin , seq_len*2)\n        self.linear3 = nn.Linear(seq_len*2, 1)\n       \n        \n        self._reinitialize()\n        \n\n    # from https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization    \n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow/Keras-like initialization\n        \"\"\"\n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    # Set forget-gate bias to 1\n                    n = p.size(0)\n                    p.data[(n // 4):(n // 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n            elif 'fc' in name:\n                if 'weight' in name:\n                    nn.init.xavier_uniform_(p.data,gain=3/4)\n                elif 'bias' in name:\n                    p.data.fill_(0)\n  \n\n    def forward(self, x):\n        out = x[:,:,:-1]\n        \n        out = out.permute(1,0,2)\n        \n        out = self.transformer_encoder( out)\n        out_l,_ = self.lstm_layer(out)\n        \n        if self.use_decoder:\n            out = self.transformer_decoder(out, out) \n            out_dec_diff = (out - x[:,:,:-1].permute(1,0,2)).abs().mean(dim=2)\n        else:\n            out_dec_diff = out*0\n        \n        out = torch.cat([out, out_l], dim=2)\n        \n        # Head\n        out = F.gelu(self.linear1(out.permute(1,0,2)))\n        out = self.linear3(out)\n\n        return out.view(-1, self.seq_len) , x[:,:,-1], out_dec_diff.view(-1, self.seq_len)  \n\n\n\n\n\n\nThe competition metric was Mean Absolute Error (MAE), but only for the time-steps where air flows into the lunge (approx. half of the timesteps). Hence, I masked the predictions (using the flag introduced in the Dataset) ignoring the unnecessary time-steps. The flags are passed through the model (val[1]) and is an output along with the predictions.\n\n\ndef vppMetric(val, target):\n    flag = val[1]\n    \n    preds = val[0]\n    \n    loss = (preds*flag-target*flag).abs()\n    loss= loss.sum()/flag.sum()\n    \n    return loss\n\nThee values produced by the AutoGenerater are additionally measured by the vppGenMetric. It uses MAE to evaluate how good the reconstruction of the input features values evolves.\n\n\ndef vppGenMetric(val, target):\n    gen =val[2]\n    \n    flag = val[1]\n    \n    loss = (gen*flag).abs()\n    loss= loss.sum()/flag.sum()\n    \n    return loss\n\n\n\n\n\nThe loss function is a combination of L1-derived-Loss (vppAutoLoss) for the predictions and the AutoEncoder-predictions.\nDue to this discussion I did some experiments with variations of Huber and SmoothL1Loss. The later (vppAutoSmoothL1Loss) performed better.\n\n\ndef vppAutoLoss(val, target):\n    gen =val[2]\n    \n    flag = val[1]\n    \n    preds = val[0]\n    \n    loss = (preds*flag-target*flag).abs() + (gen*flag).abs()*0.2 #\n    loss= loss.sum()/flag.sum()\n    \n    return loss\n    \n\n# Adapting https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss\ndef vppAutoSmoothL1Loss(val, target):\n    \n    beta = 2\n    fct = 0.5\n    \n    gen =val[2]\n    \n\n    flag = val[1] \n    \n    preds = val[0]\n    \n    loss = (preds*flag-target*flag).abs() + (gen*flag).abs()*0.2\n    \n    loss = torch.where(loss < beta, (fct*(loss**2))/beta, loss)#-fct*beta)\n    \n    # reduction mean**0.5\n    loss = loss.sum()/flag.sum() #()**0.5\n    \n    return loss"
  },
  {
    "objectID": "posts/2021-08-15-model-allocation/2021-08-15-model-allocation.html",
    "href": "posts/2021-08-15-model-allocation/2021-08-15-model-allocation.html",
    "title": "Model allocation",
    "section": "",
    "text": "This notebook was originally published on https://www.kaggle.com/joatom/model-allocation.\n\nIn this notebook I experiment with two ensembling strategies.\nThere are many ways to combine different models to improve predictions. A common technique for regression tasks is taking a weighted average of the model predictions (y_pred = (m1(x)*w1 + ... + mn(x)*wn) / n). Another common technique is building a meta model, that is trained on the models‚Äô outputs.\nThe first chapter starts with a simple linear combination of two models. And we explore with an simple example, why ensembling actually works. These insights will lead, in the second chapter, to the first technique on how to choose weights for a linear ensemble by using residual variance. In the third chapter an alternative for the weight selection is examined. This second technique is inspired by portfolio theory (a theory to combine financial assets). In the fourth chapter the two techniques are applied and compared on the Tabular Playground Series (TPS) - Aug 2021 competition. Finaly cross validation (CV) and leaderboard (LB) Scores are listed in the fith chapter.\n\n\n\n\n\n\nNote\n\n\n\nFor the ease of explanation we make some simplifying assumptions, such as equal distribution of the data, same distribution on unseen data, ‚Ä¶ (just think of a vanilla world).\n\n\n\n1. Why ensembling works\nSuppose there are two fitted regression models and they predict values like shown in the first chart.\n\n\n\n\n\nTo get a better intuition on how good the two models fit the ground truth, we plot the residuals y_true(x)-m(x).\n\n\n\n\n\nIf we had to choose one of the models, which one would we prefer? Model 2 does better on the first data point and perfect on the third, but it contains an outlier the 5th data point.\nLet‚Äôs look at the mean and the variance of the residuals.\n\n\nModel #1. mean:  0.0714, var:  1.6020\nModel #2. mean:  0.0000, var:  2.0000\n\n\nOn the long run Model2 has an average residual of 0. Model 1 carries along a residual of 0.0714. So on average Model 2 seams to do better.\nBut Model 2 also has a higher variance. That implies we have a great chance to do a great prediction (e.g.¬†x=3) but we also have high risk to screw the prediction (e.g.¬†x=5).\nNow we build a simple linear ensemble of the two models like ens = 0.5 * m1 + 0.5 m2.\n\n\n\n\n\nThe ensemble line is closer to the true values. It also looks smoother then m1 and m2.\n\n\n\n\n\nIn the residual chart we can see that the ensemble does a bit worse for x=3 compared to Model 2. But it also decreases the residuals for the outliers (points 1, 5, 7).\nLet‚Äôs check the stats:\n\n\nEnsemble. mean:  0.0357, var:  0.2219\n\n\nWe dramatically reduced the variance, hence reduced the risk/chance. The mean value is now in between Model 1 and Model 2.\nFinally let‚Äôs play around with the model weights in the ensemble and check how mean and variance change.\n\n# generate weights for w1\nweight_m1 = np.linspace(0, 1, 30)\n\nens_mean = np.zeros(30)\nens_var = np.zeros(30)\n\nfor i, w1 in enumerate(weight_m1):\n    # build ensemble for different weights\n    ens = m1*w1 + m2*(1-w1)\n    ens_res = y_true - ens\n    \n    # keep track of mean and var of the differently weighted ensembles\n    ens_mean[i] = ens_res.mean()\n    ens_var[i] = ens_res.var()\n\n\n\n\n\n\nWith the previous 50:50 split the variance seems almost at the lowest point. So we only get a reduction of the mean below 0.0357 if we allow the ensemble to have more variance, hence take more risk.\n\n\n2. Weights by residual variance\nSince the Model 1 and Model 2 are well fitted, their average residuals are pretty close to 0. So let‚Äôs focus on reducing our variance to avoid surprises on later later predictions.\nWe now solve for the optimal weights that minimizes the variance of the residual of our ensemble with this function:\n\nfun = lambda w: (y_true-np.matmul(w, preds)).var()\n\nWe also define a constraint so that the w.sum() == 0:\n\n# w.sum() = 1  <=> 0 = w.sum()-1\ncons = ({'type': 'eq', 'fun': lambda w: w.sum()-1})\n\nIf you want, you can also set bounds, so that the weights want be negative.\nI don‚Äôt. I like the idea of going short with a model. And negative weights really increase the results of TPS predictions in chapter 4.\n\nbnds = ((0,None),\n        (0,None),\n        (0,None),\n        (0,None),\n        (0,None))\n\nNow, we are all set to retrieve the optimal weights.\n\n# predictions of Model1 and Model 2\npreds = np.array([m1, m2])\n\n# init weights\nw_init = np.ones(preds.shape[0])/preds.shape[0]\n\n# run optimization\nres = scipy.optimize.minimize(fun, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\n# get optimal weights\nw_calc = res.x\n\n\nprint(f'Calculated weights: {w_calc}')\n\nCalculated weights: [0.53150242 0.46849758]\n\n\nLet‚Äôs see how the calculated weights perform.\n\nens_ex1 = np.matmul(w_calc, preds)\nens_ex1_res=y_true-ens_ex1\n\nprint(f'Ensemble Ex1. mean: {ens_ex1_res.mean(): .4f}, var: {ens_ex1_res.var(): .4f}')\n\nEnsemble Ex1. mean:  0.0380, var:  0.2157\n\n\nWe con compare the results with the first ensemble 50:50 split. With the calculated weights we could further reduce the variance of the model (0.2219 -> 0.2157). But unfortunately the mean increased a bit (0.0357 -> 0.0380).\nWe see the trade off between mean and variance and have to decide if we prefer a more stable model or take some risk for better results.\n\n\n3. Portfolio theory for ensembling\nIn finance different assets are often combined in a portfolio. There are many criteria for the asset selection/allocation. One of them is by choosing a risk strategy. In 1952 the economist Harry Markowitz defined a Portfolio Selection strategy which built the foundation of many portfolio strategies to come. There is a great summary on Wikipidia, but the original paper can also be found with a google search.\nSo, what it is all about. Let‚Äôs assume we are living in an easy, plain vanilla world. We want to build a portfolio that yields high return with low risk. That‚Äôs not easy. If we only buy stocks of our favorite fruit grower, a rainy summer would result in a low return. Wouldn‚Äôt it be smart to also buy stocks of a raincoat producer, just in case. But what if the summer was sunny, then we would have rather invested the entire money in fruits instead of raincoats. It‚Äôs clearly a trade off. Either we lower the risk of loosing money in a rainy summer and invest in both (fruits and raincoats). Or we take the risk investing all money in fruits to maybe gain more money. And if we lower the risk, in which raincoat producer should we invest? The one with the bumpy stock price or the one with a steady, but slowly growing stock price.\nNow, we already see the first similarities between our ensemble example above and the Portfolio Theory. Risk can be measured through variance and a good return of our ensemble is results in a low expected residual.\nBut there is even more in Portfolio Theory. It also takes dependencies between assets into account. If the summer is sunny the fruit price goes up and the raincoat price goes down, they are somewhat negative correlated.\nSince we expect the average residual of our fitted models to be close to 0 and we build a linear model, we can expect our ensemble average residual also to be close to 0. Therefore, we focus on optimizing the portfolio variance, which can be boiled down to Var_p = w'*Cov*w. The covariance measures the dependency between combined models and also considers the variance.\n\nWhat data can we actually use? In the financial example returns are the increase or decrease of an asset price (p/p_t-1), hence we are looking on returns for a certain period of time. In ML we can take our out-of-fold (oof) predictions and calculate the residuals from the train targets to build a dataset.\n\n\nCan we do this despite we are looking at a time-series in the financial example? Yes, in this basic portfolio theory we don‚Äôt take time dependencies into account. But it‚Äôs important to keep the same order for the different asset returns for correlation/covariance calculation. We want to compare the residual of model 1 and 2 for always the same data item.\n\nThe optimization function for the second ensemble technique is:\n\n## following https://en.wikipedia.org/wiki/Modern_portfolio_theory\n\n# Predictions of Model 1 and Model 2\npreds = np.array([m1,m2])\n# Residuals of Model 1 and Model 2\npreds_res = np.array([m1_res, m2_res])\n\n# handle residuals like asset returns\nR = np.array(preds_res.mean(axis=1))\n# factor by which R is considered during optimization. turned off for our example\nq = 0 #-1\n\n# covariance matrix of model residuals\nCM = np.cov(preds_res)\n\n# optimization function\nfun = lambda w: np.matmul(np.matmul(w.T,CM),w) - q * np.matmul(R,w)\n\n# constraint: weights must sum up to 1.0\ncons = ({'type': 'eq', 'fun': lambda x: x.sum()-1})\n\nRun the optimization.\n\n# init weights\nw_init = np.ones(preds.shape[0])/preds.shape[0]\n\n# run optimization\nres = scipy.optimize.minimize(fun, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\n# get optimal weights\nw_calc = res.x\n\nprint(f'Calculated weights: {w_calc}')\n\nCalculated weights: [0.53150242 0.46849758]\n\n\nThe weights are the same as in the first technique. That really surprised me. And I run a couple of examples with different models. But the weights were only slightly different between the two techniques.\n\n\n4. Ensembling TPS Aug 2021\nNow that we have to techniques to ensemble, let‚Äôs try them on the TPS August 2021 data.\nWe do a 7 kfold split and calculate the residuals on the out-of-fold-predictions, that are used for validation. We train 7 regression models with different architecture so we get some diversity.\n\nN_SPLITS = 7\nSEED = 2021\n\nPATH_INPUT = '/home/kaggle/TPS-AUG-2021/input/'\n\n\n# load and shuffle\ntest = pd.read_csv(PATH_INPUT + 'test.csv')\n\ntrain = pd.read_csv(PATH_INPUT + 'train.csv').sample(frac=1.0, random_state = SEED).reset_index(drop=True)\n\ntrain['fold_crit'] = train.loss\ntrain.loc[train.loss>=39, 'fold_crit']=39\n\n\ntarget = 'loss'\nfold_crit = 'fold_crit'\nfeatures = list(set(train.columns)-set(['id','kfold','loss','fold_crit']+[target]))\n\n\n# apply abhisheks splitting technique\nskf = StratifiedKFold(n_splits = N_SPLITS, random_state = None, shuffle = False)\n\ntrain.kfold = -1\n\nfor f, (train_idx, valid_idx) in enumerate(skf.split(X = train, y = train[fold_crit].values)):\n    \n    train.loc[valid_idx,'kfold'] = f\n\ntrain.groupby('kfold')[target].count()\n\nkfold\n0.0    35715\n1.0    35715\n2.0    35714\n3.0    35714\n4.0    35714\n5.0    35714\n6.0    35714\nName: loss, dtype: int64\n\n\n\n# define models\nmodels = {\n    'LinReg': LinearRegression(n_jobs=-1),\n    'HGB': HistGradientBoostingRegressor(),\n    'XGB': XGBRegressor(tree_method = 'gpu_hist', reg_lambda= 6, reg_alpha= 10, n_jobs=-1),\n    'KNN': KNeighborsRegressor(100, n_jobs=-1),\n    'BayesRidge': BayesianRidge(),\n    'ExtraTrees': ExtraTreesRegressor(max_depth=2, n_jobs=-1),\n    'Poisson': Pipeline(steps=[('scale', StandardScaler()),\n                ('pois', PoissonRegressor(max_iter=100))])    \n}\n\n\nFit models and save oof predictions.\n\nfor (m_name, m) in models.items():\n    print(f'# Model:{m_name}\\n')\n    train[m_name + '_oof'] = 0\n    test[m_name] = 0\n    \n    y_oof = np.zeros(train.shape[0])\n    \n    for f in range(N_SPLITS):\n\n        train_df = train[train['kfold'] != f]\n        valid_df = train[train['kfold'] == f]\n        \n        m.fit(train_df[features], train_df[target])\n        \n        oof_preds = m.predict(valid_df[features])\n        y_oof[valid_df.index] = oof_preds\n        print(f'Fold {f} rmse: {mean_squared_error(valid_df[target], oof_preds, squared = False):0.5f}')\n        \n        test[m_name] += m.predict(test[features]) / N_SPLITS\n    \n    train[m_name + '_oof'] = y_oof\n    \n    print(f\"\\nTotal rmse: {mean_squared_error(train[target], train[m_name + '_oof'], squared = False):0.5f}\\n\")\n\n\noof_cols = [m_name + '_oof' for m_name in models.keys()]\n\nprint(f\"# ALL Mean ensemble rmse: {mean_squared_error(train[target], train[oof_cols].mean(axis=1), squared = False):0.5f}\\n\")        \n\n# Model:LinReg\n\nFold 0 rmse: 7.89515\nFold 1 rmse: 7.90212\nFold 2 rmse: 7.90260\nFold 3 rmse: 7.89748\nFold 4 rmse: 7.89844\nFold 5 rmse: 7.89134\nFold 6 rmse: 7.89643\n\nTotal rmse: 7.89765\n\n# Model:HGB\n\nFold 0 rmse: 7.86447\nFold 1 rmse: 7.87374\nFold 2 rmse: 7.86688\nFold 3 rmse: 7.86255\nFold 4 rmse: 7.86822\nFold 5 rmse: 7.85785\nFold 6 rmse: 7.86566\n\nTotal rmse: 7.86563\n\n# Model:XGB\n\nFold 0 rmse: 7.91179\nFold 1 rmse: 7.92748\nFold 2 rmse: 7.92141\nFold 3 rmse: 7.91901\nFold 4 rmse: 7.91125\nFold 5 rmse: 7.90286\nFold 6 rmse: 7.92340\n\nTotal rmse: 7.91675\n\n# Model:KNN\n\nFold 0 rmse: 7.97845\nFold 1 rmse: 7.97709\nFold 2 rmse: 7.98165\nFold 3 rmse: 7.97895\nFold 4 rmse: 7.97781\nFold 5 rmse: 7.97798\nFold 6 rmse: 7.98711\n\nTotal rmse: 7.97986\n\n# Model:BayesRidge\n\nFold 0 rmse: 7.89649\nFold 1 rmse: 7.90576\nFold 2 rmse: 7.90349\nFold 3 rmse: 7.90007\nFold 4 rmse: 7.90121\nFold 5 rmse: 7.89455\nFold 6 rmse: 7.89928\n\nTotal rmse: 7.90012\n\n# Model:ExtraTrees\n\nFold 0 rmse: 7.93239\nFold 1 rmse: 7.93247\nFold 2 rmse: 7.92993\nFold 3 rmse: 7.93121\nFold 4 rmse: 7.93129\nFold 5 rmse: 7.93247\nFold 6 rmse: 7.93364\n\nTotal rmse: 7.93191\n\n# Model:Poisson\n\nFold 0 rmse: 7.89597\nFold 1 rmse: 7.90240\nFold 2 rmse: 7.90233\nFold 3 rmse: 7.89682\nFold 4 rmse: 7.89873\nFold 5 rmse: 7.89241\nFold 6 rmse: 7.89701\n\nTotal rmse: 7.89795\n\n# ALL Mean ensemble rmse: 7.88061\n\n\n\nLet‚Äôs a look at the correlation heatmap.\n\n\noof_cols = [m_name + '_oof' for m_name in models.keys()]\n\noofs = train[oof_cols]\n\noof_diffs = oofs.copy()\nfor c in oof_cols:\n    oof_diffs[c] = oofs[c]-train[target]\n    oof_diffs[c] = oof_diffs[c]#**2\n\nsns.heatmap(oof_diffs.corr())\n\n<AxesSubplot:>\n\n\n\n\n\nXGB and KNN are most diverse, so I export a 50:50 ensemble. I‚Äôll also export an equally weighted ensemble of all models and HGB only because it is the best single model.\n\n\nCV: ALL equaly weighted: 7.880605075536334\nCV: XGB only: 7.916746570344035\nCV: HGB only: 7.865625158180185\nCV: XGB and LinReg (50:50): 7.872064005057903\nCV: XGB and KNN (50:50): 7.893210466099108\n\n\nNext we inspect the variance and mean of the residuals. Means are close to 0, as expected.\n\noof_diffs.var(), oof_diffs.mean()\n\n(LinReg_oof        62.373163\n HGB_oof           61.868296\n XGB_oof           62.675125\n KNN_oof           63.678431\n BayesRidge_oof    62.412188\n ExtraTrees_oof    62.915511\n Poisson_oof       62.377910\n dtype: float64,\n LinReg_oof       -0.000055\n HGB_oof          -0.003314\n XGB_oof           0.001392\n KNN_oof          -0.005395\n BayesRidge_oof   -0.000024\n ExtraTrees_oof   -0.000136\n Poisson_oof      -0.000084\n dtype: float64)\n\n\nThese are the histograms of the residuals:\n\n\narray([[<AxesSubplot:title={'center':'LinReg_oof'}>,\n        <AxesSubplot:title={'center':'HGB_oof'}>,\n        <AxesSubplot:title={'center':'XGB_oof'}>],\n       [<AxesSubplot:title={'center':'KNN_oof'}>,\n        <AxesSubplot:title={'center':'BayesRidge_oof'}>,\n        <AxesSubplot:title={'center':'ExtraTrees_oof'}>],\n       [<AxesSubplot:title={'center':'Poisson_oof'}>, <AxesSubplot:>,\n        <AxesSubplot:>]], dtype=object)\n\n\n\n\n\nFinally, we apply the two techniques to calculate the ensembling weights\n\nR = oof_diffs.mean().values\nCM = oof_diffs.cov().values\n\nq=0\n\n# Var technique\nfun_ex1 = lambda w: (train[target]-np.matmul(oofs.values, w)).var()\n# Cov technique\nfun_ex2 = lambda w: np.matmul(np.matmul(w.T,CM),w) - q * np.matmul(R,w)\n\ncons = ({'type': 'eq', 'fun': lambda x: x.sum()-1})\n\nbnds = ((0,None),\n        (0,None),\n        (0,None),\n        (0,None),\n        (0,None))\n\n\n# Example 1\n\nw_init = np.ones((len(models)))/len(models)\n\nres = scipy.optimize.minimize(fun_ex1, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\nw_calc = res.x\n\n\n\nCV: Ex1 calc weights: 7.85594426240217\n\n\n\n# Example 2\n\nw_init = np.ones((len(models)))/len(models)\n\nres = scipy.optimize.minimize(fun_ex2, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\nw_calc = res.x\n\n\n\nCV: Ex2 calc weights: 7.855944262936231\n\n\n\n\n5. Results\nThe competition metric is root mean squared error (RMSE). These are the scores of the different ensembles:\n\n\n\nEnsemble\nCV\npublic LB\n\n\n\n\nHGB only\n7.86563\n7.90117\n\n\nAll weights eq.\n7.88061\n7.92183\n\n\nXGB and KNN (50:50)\n7.89321\n7.91603\n\n\nEx1 (Var)\n7.85594\n7.88876\n\n\nEx2 (Cov)\n7.85594\n7.88876\n\n\n\n\n\nReferences\n\nModern Portfolio Theory: https://en.wikipedia.org/wiki/Modern_portfolio_theory\nTPS August 2021 Competition: https://www.kaggle.com/c/tabular-playground-series-aug-2021/overview\n\n\n\nRessources\n\nOriginal notebook: https://www.kaggle.com/joatom/model-allocation\nTPS data: https://www.kaggle.com/c/tabular-playground-series-aug-2021/data"
  },
  {
    "objectID": "posts/2022-11-13-repay-vs-invest/index.html",
    "href": "posts/2022-11-13-repay-vs-invest/index.html",
    "title": "Pay off Mortgage or Invest",
    "section": "",
    "text": "üá©üá™ üá∫üá∏"
  },
  {
    "objectID": "posts/2022-11-13-repay-vs-invest/index.html#interactive-notebook",
    "href": "posts/2022-11-13-repay-vs-invest/index.html#interactive-notebook",
    "title": "Pay off Mortgage or Invest",
    "section": "Interactive notebook",
    "text": "Interactive notebook\n\nPlay with the numbers: \nDownloading the notebook:"
  },
  {
    "objectID": "posts/2022-11-13-repay-vs-invest/index.html#snippets",
    "href": "posts/2022-11-13-repay-vs-invest/index.html#snippets",
    "title": "Pay off Mortgage or Invest",
    "section": "Snippets",
    "text": "Snippets\n\nRecursive SQL\nStyles in Pandas"
  },
  {
    "objectID": "posts/2022-02-28-relationship/index.html",
    "href": "posts/2022-02-28-relationship/index.html",
    "title": "Flat to hierarchie",
    "section": "",
    "text": "Tabular data is often transformed in a flattened manner so it can easily be processed by commen ML frameworks. Sometimes it is usefull to understand the relationships and hierarchies between the columns of a flattend datasource. In this notebook hierarchies are extracted from a flattened csv file."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "",
    "text": "üá©üá™ üá∫üá∏ [kaggle version]\nIn this blog post, I want to find out how easy it is to create interactive charts in notebooks. As a framework, I will use Altair, as it can be easily integrated into my blog [FP20].\nIn order to make the contribution interesting, I evaluate the death rates from Germany over the last five years. I construct two periods - the first period before and the second one during Covid19.\nWe will analyze these questions:\nThe first part describes the origin and processing of the data. Then, charts are built around the above questions. Finally, a technical conclusion to the framework follows."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#preprocessing",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#preprocessing",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe evaluation period is limited to March 2016 until February 2021. The period in which Corona was heavily active in Germany is simplified here to March 2020 (when Covid19 triggered the first major social changes in Germany) until February 2021 (orange). The pre-Covid19 period is set to March 2016 until February 2020 (blue). Thus, the Covid19 period covers exactly one year and the pre-Covid19 period exactly four years. Thus, both periods remain comparable without serious seasonal deviations. The split of the periods is illustrated in the diagram below."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#aggregation-of-data",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#aggregation-of-data",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Aggregation of data",
    "text": "Aggregation of data\nIn the calculations, the values are averaged over a period of time. Depending on the evaluation, this happens over the whole period or per month. The first and third quartile of the aggregated data may also be shown as shading in the diagrams. In some charts, the calculated points are interpolated to increase readability.\n\n\n\n\n\n\nTip\n\n\n\nFor some of the charts there are control elements in the upper right corner, such as drop-down boxes. The mouse wheel can be used to zoom and a chart can be reset by double-clicking."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Observation",
    "text": "Observation\nIn the age groups under 55, there was no excess mortality during these periods. In the age groups from 80 years and older, mortality increased massively."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-1",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-1",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Observation",
    "text": "Observation\nNorth Rhine-Westphalia (NW) has the largest number of deaths, since it is the state with highest population. In each state, there are increases in death rates in the Covid19 period. However, only minimal increases in Hesse (HE) and Bavaria (BY) can be seen for the age groups under the age of 65. In the other states, there is no noticeable increase in this age group.\nThere are smaller variations looking at the number of deaths per 100.000 inhabitants in the age group below 65. The increase in deaths in North Rhine-Westphalia (NW) is slightly lower than in Bavaria (BY) and roughly as high as in Baden-W√ºrttemberg (BW). Saxony (SN) and Brandenburg (BB) have the strongest increases.\n\n\n\n\n\n\nNote\n\n\n\nOnly deaths are evaluated. No other aspects (such as demographic aspects) are taken into account."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-2",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-2",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Observation",
    "text": "Observation\nThe city-states Hamburg (HH) and Bremen (HB) show a moderate increase in deaths despite their high population density. Berlin (BE) instead has a high increase in death rate. Schleswig-Holstein (SH) recorded the lowest increase.\nThe correlation coefficient of population density and increase in mortality is:\n\n\n0.0232\n\n\nHence, There is no correlation for this comparison.\n\n\n\n\n\n\nNote\n\n\n\nOnly deaths are evaluated. No other aspects (such as demographic aspects) are taken into account."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#data-sources",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#data-sources",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Data sources",
    "text": "Data sources\nThe data used here are from the ‚ÄúStatistisches Bundesamt‚Äù (Federal Statistical Office) and are subject to the license ‚Äúdl-de/by-2-0‚Äù. The license text can be found at www.govdata.de/dl-de/by-2-0. The data were modified exclusively within this notebook by executing the specified program code for the purpose of analysis.\n\n[SB21] Statistisches Bundesamt (Destatis), 2021 (published 2012/03/30), Sterbef√§lle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesl√§ndern f√ºr Deutschland 2016 - 2021, visited 2021/04/03, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile\n[SB20] Statistisches Bundesamt (Destatis), 2020 (published 2020/09/02), Bundesl√§nder mit Hauptst√§dten nach Fl√§che, Bev√∂lkerung und Bev√∂lkerungsdichte am 31.12.2019, visited 2021/04/03, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile"
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#other-references",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#other-references",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Other references",
    "text": "Other references\nA lot of the coding is derived from various examples of the Altair homepage and from great examples in the coresponding Github Issue tracker answered by https://github.com/jakevdp.\n\n[AA1] https://altair-viz.github.io/gallery/index.html\n[AA2] https://github.com/altair-viz/altair/issues/\n[AG18] A. Gordon, 2018 (published 2018/10/06), Focus: generating an interactive legend in Altair, visited 2021/04/05, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55\n[FP20] fastpages.fast.ai, 2020 (published 2020/02/20), Fastpages Notebook Blog Post, visited 2021/04/05, https://fastpages.fast.ai/jupyter/2020/02/20/test.html"
  },
  {
    "objectID": "posts/2023-01-03-one-dime-at-a-time/index.html",
    "href": "posts/2023-01-03-one-dime-at-a-time/index.html",
    "title": "One dime at a time?",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis blog post is not a financial advice! This is a toy example. This blog post is full of unrealistic assumptions. All numbers are made up. Reach out to a professional financial advisor you trust, if you need financial advise. And most important, take a spreadsheet and do the math yourself with your numbers and your assumptions.\n\n\n\n\n\n\nOne dime at a time?\nOne night I‚Äôve watched some YouTube videos about investment strategies. One example was to follow a saving plan and put the same amount of money into ETFs on a regular basis, say 100 Euro every month. In the interview a question come up: What would happen when the market goes down and the portfolio looses value? The answer was pretty simple: If you invest regularly then you hit the market when it goes up the one day and when it goes down the other day. So the volatility doesn‚Äôt matter that much on the long run. This intuitively made sense to me: The regular investments smooth out the volatility. And since you can‚Äôt predict the stock prices, it‚Äôs hard to time the market (predicting when buying is cheap over a short period of time).\nIs my intuition about the constant investments reasonable? Let‚Äôs find out in this blog post.\nBy the way, this question made me also think about a proper refueling strategy witch I covered in another blog post.\n\n\nA fictional asset\n\n\nWe will simulate some numbers for a fictional assets price time series. The prices will increase on the long run, but will have some volatility. Here is how our simulation looks like for 200 time slots:\n\n\n\n\n\n\n\nThe trend line indicates the general price increase on the long run. The green dot marks the lowest price and the red dot the highest price in the time range.\n\n\nWith an investment of ‚Ç¨ 20,000 at the lowest price at t=20 and selling at the highest price at t=197 the return is 18.96% and yields ‚Ç¨ 3,792.\n\n\n\n\nTwo investment strategies\nSince we don‚Äôt know in advance when the price will be lowest and highest, we consider two investment strategies and see how they play out.\n\n\nIn both strategies we start out with ‚Ç¨ 20,000 cash.\n\nStrategy 1: One-time investment\nThe entire amount of money is investment at once. We will look how this plays out at various points in time.\nStrategy 2: Continuous investment\nThe cash is split equally over the period of time and at every step the same amount of money is invested.\n$ investment_t = 20,000 / 200 = 100.00 $\n\nLet‚Äôs see how many shares we can buy with both strategies:\n\n\n\n\n\n\n\n\n\nBlue dots: \\[\nonetime(t) = \\frac{cash}{price_t}\n\\]\nBlack line: \\[\n\\begin{aligned}\ncont(t) &= \\sum^t_i{ \\frac{cash/t}{price_i}} \\\\\n        &= \\frac{cash}{t} \\sum^t_i{ \\frac{1}{price_i}}\n\\end{aligned}\n\\]\nLight blue line: \\[\ncontmax(t) = cont(n)\n\\]\nThe blue dots mark the number of shares that can be purchased when the entire money is invested at once at the given time t.\nThe black line indicates the number of shares that can be bought when the entire investment is equally spread from t=0 up to a given t.\nThe light blue line highlights the number of shares of the last entry of the black line, hence the cash is equally invested over the entire period of time.\nA green line marks the one-time investment that always reach a higher number of shares compared to the continuous investment, regardless of the holding duration.\nA red lines indicates the one-time investment that can never reach the number of shares of the continuous investment at any time in \\(0<t<=n\\).\nAn orange line marks the one-time investment that has fewer number of shares as the continuous investment at the given time, if the entire cash has been equally distributed to the given t. However, if the continuous investments are spread for a wider period of time, the one-time investment will be superior to the continuous investment at a later point in time.\nWith our dummy data the strategy to put a big investment at once is good if the investment can stay invested untouched for a long period of time. If the money needs to shortly be withdrawn from the investment the continuous strategy is preferable, regarding number of shares. The tipping point between the strategies is close to \\(t/2\\).\nLet‚Äôs see, how the amount of shares result in value at \\(t=n\\),\n\n\n\n\n\n\n\nBlue line: \\[\ncash(t)=cash\n\\]\nBlack line: \\[\n\\begin{aligned}\ncont(t) &= inv_{acc_t}+cash_{remaining_t} \\\\\n        &= p_n*\\frac{cash}{n}\\sum^t_i{\\frac{1}{p_i}} + cash*(1-\\frac{t}{n})\n\\end{aligned}\n\\]\nLight blue line: \\[\ncontmax(t) = cont(n)\n\\]\nDots: \\[\nonetime(t) = p_n*\\frac{cash}{p_t}\n\\]\nThe blue line is our net worth if we don‚Äôt invest and keep cash for the entire period of time.\nThe black line indicates the net worth in a continuous investment strategy. At time t the number of shares that are accumulated sofar are multiplied by the price at the end of the time period. Also, the remaining cash at time t is added to the investment value.\nThe light blue line highlights the net worth of the continuous investment at the end of the period of time.\nThe dots indicate the value of a one-time purchase at the end of the period of time.\nA red dot one-time investment is always worse than the continuous strategy.\nA green dot one-time investment results in a higher net value at the end of the period compared to the continuous investment.\nAn orange dot one-time investment is superior to the continuous investment for a certain period of time. But if the continuous strategy will go on till the end, the orange dot will result in a smaller net worth.\nThe light blue and black line are highly volatile, when we change the duration, since it depends only on the last price. If the period gets shortened the area of the green, orange and red dots will change accordingly. One idea to reduce volatility the selling can be spread over many periods, like the continuous strategy to accumulate shares.\nAgain, the strategy to invest all at once is preferable, if the investments won‚Äôt be sold for a long time. If the investment needs to be sold at any time a continuous strategy is preferable with this dummy data.\n\n\nConclusion\nThis example only describes a fictional scenario where the prices go up on the long run. We also haven‚Äôt thought at what point(s) in time to sell the shares to actually get the best out of the investment.\nFrom a personal view a continuous strategy seams appealing. This also makes sense considering a continuous income stream (such as salary) which would make this strategy applicable.\nMore on this topic can be found under the phrase cost dollar averaging.\nAnd as said in the beginning, that‚Äôs not a financial advise. So please make up your mind with your own calculations."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/index.html",
    "href": "posts/2021-06-19-watchtrain/index.html",
    "title": "Streaming ML training progress to a smart watch",
    "section": "",
    "text": "üèîÔ∏è\nDuring the Covid winter I hardly had any reason to leave the house. It was clear that I actively had to look after my mental and physical well-being. So, I decided to buy a smart watch (Fitbit Versa 3) and take on the 10000 steps per day challenge. Henceforth I spent much more time outside and in the sunlight moving my body.\n‚õÖ\nWhen I registered my new Fitbit I instantly got attracted by the For Developers link. As you might guess my thoughts started spinning like - Aha, they are providing a SDK! I got to try this out at some point and build an app for the watch. - I wanted this app to be related to ML or at least to data somehow. My first thought was - Would it be possible to use the green heart-rate sensor light as an OCR? - Nope, to complicated for a fun project. - Then I went on with the registration of the watch on the webpage.\nüå§Ô∏è\nSome month later, on a usual Sunday, I lay resting on the couch after lunch while the kids and my wife were cleaning the table and kitchen (I did the cooking ;-)). A little bored, I thought of my ML-model that was training on Hotel Room classification for a couple of hours upstairs in the office. I wanted to know how it was preceding. Sneaking upstairs would result in half an hour in front of the computer, followed by some trouble with my wife üòí. - May by I should eventually register at Neptune.ai or wandb.ai, then I could preview my trainings from the couch on my cell phone!? ‚Ä¶ Or may be I now have a new fun project for my new watch üòÑ üí°! -"
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/index.html#api-server",
    "href": "posts/2021-06-19-watchtrain/index.html#api-server",
    "title": "Streaming ML training progress to a smart watch",
    "section": "API-Server",
    "text": "API-Server\nThe requirements led to the architecture shown in the diagram. In the center of the application is an API-Server to coordinate the training and the watch. The Training, Watch and Web are client applications connected to the API-Server‚Äôs Watchtrain-Topic. The Topic contains a connection pool for the Training client (data Producer) and another connection pool for the Web and the Watch clients (data Consumer).\n\n\nThe initial idea was to setup a classical Consumer/Producer (Pub/Sub) pattern. But it ended up a bit different. The Topic holds the data in an object rather than a queue-like state and also does some data processing. The Producer and Consumer can still subscribe at any time, but they are also strongly connected via Websockets. I took the chance to play around with websockets, since it is also available on the watch.\n\nFor each client type there is an Agent that processes the data and messages that are send from the clients. The stats and progress data is saved in the topic. The topic generates the metric chart that is send to the Consumers, since I couldn‚Äôt find a charts library in the watch SDK.\nThe Topic-Consumer-Producer-Agent ‚Äúpattern‚Äù with the connection pool handler is set up in a generic way so it‚Äôs easy to develop other applications in the same manner and run them on the API-Server.\nAs API-Server I used FastApi which is easy to start with as shown on the tutorial site or in this video.\nThe communications between the components is done with JSON. Messages start with an action-field followed by the training_id and a more or less complex payload. Depending of the action value different functionalities are triggered, such as sending the metric image to the client or converting batch information into a progress bar."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/index.html#training",
    "href": "posts/2021-06-19-watchtrain/index.html#training",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Training",
    "text": "Training\n\nFastai\nThe easiest way to implement the train logging is by using the Fastai Callback infrastructure. So I built a WebsocketLogger which gets past to the training like this:\nlearn = cnn_learner(dls, resnet18, pretrained = False, metrics=[accuracy, \n                                                                Recall(average='macro'), \n                                                                Precision(average='macro')])\n\nlearn.unfreeze\nlearn.fit_one_cycle(10, lr_max = 5e-3, cbs=[WebsocketLogger('ws://myapiserver:8555/ws/watchtrain/producer/12345')])\nStarting out by looking at the source code of the fastai build-in CSVLoggers and ProgressCallback I learned how to track train data (metrics, epoch and batch progress). A bit challenging was the integration of the websocket client. I preferred a permanent connection rather than many one time (open-send-close) connections. Otherwise a simple REST call would have been more suitable. It is also very important that training must not break when the websocket connection is lost or the API-Server isn‚Äôt available anymore.\nThat‚Äôs how it is implemented using the websocket-client library:\n\ndef __init__(self, conn, ...):\n    self.conn = conn\n    ...\n    self.heartbeat = False\n    self._ws_connect()\n    ...\n\n...\n# gets called when a websocket is opened\ndef _on_ws_open(self,ws):\n    # ws connection is now ready => unlock\n    self.ws_ready_lock.release()\n    self.heartbeat = True\n    \ndef _ws_connect(self):\n    \n    self.heartbeat = False\n    \n    # aquire lock until websocket is ready to use\n    self.ws_ready_lock = threading.Lock()\n    self.ws_ready_lock.acquire()\n    \n    print('Connecting websocket ...')\n    \n    self.ws = websocket.WebSocketApp(self.conn,\n                                      on_open = self._on_ws_open,\n                                      on_message = self._on_ws_message,\n                                      on_error = self._on_ws_error,\n                                      on_close = self._on_ws_close)\n\n    # run websocket in background\n    thread.start_new_thread(self.ws.run_forever, ())\n    \n    # wait for websocket to be initialized, \n    # if connection is not possible (e.g. APIServer is down) resume after 3 sec, but heartbeat stays FALSE\n    self.ws_ready_lock.acquire(timeout = 3)\n    \n    print('... websocket connected.')\nThe WebSocketApp runs as a local websocket-handler in the background. The Locks are used to make sure the connection gets properly established before the first messages are send. The heartbeat is introduced to keep the training running even if the websocket connection is broken and could not be reconnected via WebSocketApp.\nIf there is no heartbeat anymore _ws_connect() is called again after any epoch. If the API-Server is still not reachable the training continuous after a 3 second waiting time.\n\n\nPytorch\nI skipped the pytorch implementation until I need it. But it is straight forward. Start a WebSocketApp thread in the background. Send the data from inside of the training/validation/inference-loop."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/index.html#watch",
    "href": "posts/2021-06-19-watchtrain/index.html#watch",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Watch",
    "text": "Watch\n\nThe layout is held pretty simple as shown in the picture. There is a progress bar for the epochs and one for the mini batches (train and valid). In the center is the chart of the metrics. And at the bottom are the latest metric values. The cell phone that belongs to the watch establish a websocket connection to the API-Server and puts EventListeners for incoming messages into place. The incoming messages are uploaded to the watch were they can be displayed."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/index.html#fastapi",
    "href": "posts/2021-06-19-watchtrain/index.html#fastapi",
    "title": "Streaming ML training progress to a smart watch",
    "section": "FastAPI",
    "text": "FastAPI\nFastAPI is a well-documented and easy to use framework. In the beginning I set it up with HTTPS. There is a tutorial on how to setup FastAPI with Traefik. But since I wanted to run the server at home I had to invest some evenings to figure out, how to set it up by myself. I used mkcert for SSL creation. A docker file to setup an FastAPI-Server at home can now be found here. At the end when I got it working I decided to not use HTTPS for reasons described below, ü§∑‚Äç‚ôÇÔ∏è."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/index.html#websockets",
    "href": "posts/2021-06-19-watchtrain/index.html#websockets",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Websockets",
    "text": "Websockets\nThe different components communicate instantly. The data is pushed to the watch, which is the preferred behavior on the receiving site. With the websocket on the training site it is a bit more complicated to be fail safe and pickup communication when the connection is broken for a longer period of time. I might switch this part to a simple REST-post in a later version. But this way it was a fun exersice nevertheless."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/index.html#fitbit-sdk",
    "href": "posts/2021-06-19-watchtrain/index.html#fitbit-sdk",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Fitbit SDK",
    "text": "Fitbit SDK\nThe Fitbit SDK is nice. They provide an online IDE which can easily be connected to your devices. The SDK is documented with a few examples. They also host helpful forum.\nI had a bit of a hard time when I tried to load and display the Metrics chart image to the watch. I had to figure out that there are two types of jpeg, progressive and basic. And only one worked. It also was hard to figure out that the the image needs to have a certain size to be displayed. But that‚Äôs part of the normal learning path with a new technology.\nAnd than, there was this one thing that really upset me (But as fare as I read in forums it is not the Fitbit SDKs fault!). Android doesn‚Äôt allow regular HTTP connection through apps. That‚Äôs why I setup the API-Server with HTTPS. But since I generated the certificate on my own, it wasn‚Äôt a trusted source and therefore Android didn‚Äôt accept it. Then I found some post that showed how to access HTTP from a local net, but only for IP range 192.168.0.x. That meant either building a Reverse Proxy or changing the Subnet of my network. And then finally I needed to deal with the docker net-addresse where the API-Server is running. As suspected, one evening I freaked out - ?#@!, I just want to send a JSON to my cell phone! 30 years of web-development and all we ended up is JavaScript and SSL-certs @!# - That was a good time to go to bed, put the project aside for a few days and celebrate that most of the time I‚Äôm into data instead of GUI üòÅ.\nBesides that I really enjoyed it to build a nice app for my Fitbit."
  },
  {
    "objectID": "posts/2020-01-05-bgml-geotab/index.html",
    "href": "posts/2020-01-05-bgml-geotab/index.html",
    "title": "BigQuery-Geotab Intersection Congestion",
    "section": "",
    "text": "This blog post contains some of my codings for the 2019 kaggle BigQuery-Geotab competition.\nMy submission scored 1st Place in the categorie BigQuery ML Models built in SQL.\nThe challange was to predict six measures for cars approaching intersections in four US cities.\nMy objective in the competition was to tryout BigQuery (BQ) including the basic ML features. Therefore the notebooks rely as much as possible on BQ. All features are generated in BQ with varying SQL-techniques. The prediction model is also build in BQ.\nAdditional resources can be found in my github repo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Interactive Precision-Recall Curve\n\n\n0 min\n\n\n\nMetric\n\n\nBasics\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFill‚Äôer up!\n\n\n5 min\n\n\n\nLife hack\n\n\nFinance\n\n\n\n\nJohannes Tomasoni\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne dime at a time?\n\n\n4 min\n\n\n\nFinance\n\n\n\n\nJohannes Tomasoni\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIngredients for successful projects\n\n\n14 min\n\n\n\nProject Management\n\n\nBusiness\n\n\n\n\nJohannes Tomasoni\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPay off Mortgage or Invest\n\n\n3 min\n\n\n\nFinance\n\n\n\n\nJohannes Tomasoni\n\n\nNov 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt and Diffusion\n\n\n7 min\n\n\n\nArt\n\n\nML\n\n\nSociety\n\n\n\n\nJohannes Tomasoni\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTackling Projects like a ML Challenge\n\n\n9 min\n\n\n\nProject Management\n\n\nML\n\n\nBusiness\n\n\n\n\nJohannes Tomasoni\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlat to hierarchie\n\n\n0 min\n\n\n\nTabular\n\n\nEDA\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning along\n\n\n1 min\n\n\n\nTabular\n\n\nEDA\n\n\nBasics\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nJan 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes about Sequence Modelling\n\n\n3 min\n\n\n\nTime Series\n\n\nML\n\n\nCompetition\n\n\n\n\nJohannes Tomasoni\n\n\nNov 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel allocation\n\n\n6 min\n\n\n\nBasics\n\n\nML\n\n\nEconomics\n\n\n\n\nJohannes Tomasoni\n\n\nAug 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStreaming ML training progress to a smart watch\n\n\n10 min\n\n\n\nAPI\n\n\nML\n\n\nSide Project\n\n\n\n\nJohannes Tomasoni\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifying Hotels\n\n\n1 min\n\n\n\nVision\n\n\nML\n\n\nCompetition\n\n\n\n\nJohannes Tomasoni\n\n\nMay 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBending the space with nonlinearity\n\n\n0 min\n\n\n\nBasics\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nApr 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeaths by age group and states in Germany from 2016 to 2021\n\n\n5 min\n\n\n\nEDA\n\n\nSociety\n\n\n\n\nJohannes Tomasoni\n\n\nApr 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically translate blog posts\n\n\n2 min\n\n\n\nNLP\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nDec 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA handful of bricks - from SQL to Pandas\n\n\n25 min\n\n\n\nSQL\n\n\nPandas\n\n\nBigQuery\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nDec 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBigQuery-Geotab Intersection Congestion\n\n\nüèÜ Winning solution\n\n\n0 min\n\n\n\nSQL\n\n\nBigQuery\n\n\nML\n\n\nCompetition\n\n\n\n\nJohannes Tomasoni\n\n\nJan 5, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Hi there!\nThis is my hobby blog. It contains personal notes from my ML learning journey, code snippets, personal notes and blog posts of various topics.\nI‚Äôm happy when you leave a comment on articles you like.\nCheers,\nJohannes"
  },
  {
    "objectID": "snippets/2022-11-11-melt/2022-11-11-melt.html",
    "href": "snippets/2022-11-11-melt/2022-11-11-melt.html",
    "title": "Melt & pivot",
    "section": "",
    "text": "date\n      fruit\n      amount\n      quality\n    \n  \n  \n    \n      0\n      2022-09-01\n      apple (red)\n      4\n      3\n    \n    \n      1\n      2022-09-01\n      grape (red)\n      5\n      1\n    \n    \n      2\n      2022-09-01\n      grape (white)\n      6\n      3\n    \n    \n      3\n      2022-09-03\n      grape (red)\n      6\n      2\n    \n    \n      4\n      2022-09-03\n      grape (white)\n      4\n      2\n    \n    \n      5\n      2022-09-04\n      apple (red)\n      2\n      1\n    \n    \n      6\n      2022-09-04\n      grape (white)\n      9\n      1\n    \n    \n      7\n      2022-09-05\n      apple (red)\n      5\n      1\n    \n    \n      8\n      2022-09-05\n      grape (red)\n      5\n      2\n    \n    \n      9\n      2022-09-05\n      grape (white)\n      8\n      3\n    \n  \n\n\n\n\n\nPivot\n\ndata_pv = data.pivot(index='date', columns='fruit', \n                     values=['amount','quality']).reset_index()\ndata_pv\n\n\n\n\n\n  \n    \n      \n      date\n      amount\n      quality\n    \n    \n      fruit\n      \n      apple (red)\n      grape (red)\n      grape (white)\n      apple (red)\n      grape (red)\n      grape (white)\n    \n  \n  \n    \n      0\n      2022-09-01\n      4.0\n      5.0\n      6.0\n      3.0\n      1.0\n      3.0\n    \n    \n      1\n      2022-09-03\n      NaN\n      6.0\n      4.0\n      NaN\n      2.0\n      2.0\n    \n    \n      2\n      2022-09-04\n      2.0\n      NaN\n      9.0\n      1.0\n      NaN\n      1.0\n    \n    \n      3\n      2022-09-05\n      5.0\n      5.0\n      8.0\n      1.0\n      2.0\n      3.0\n    \n  \n\n\n\n\n\n\nMelt (Unpivot)\n\ndata_pv.columns = ['date', 'apple (red)_amt', 'grape (red)_amt',\n                   'grape (white)_amt','apple (red)_qlt',\n                   'grape (red)_qlt','grape (white)_qlt']\n\n\ndata_pv.melt(id_vars='date', value_vars=['apple (red)_amt', \n                                         'grape (red)_amt',\n                                         'grape (white)_amt'])\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2022-09-01\n      apple (red)_amt\n      4.0\n    \n    \n      1\n      2022-09-03\n      apple (red)_amt\n      NaN\n    \n    \n      2\n      2022-09-04\n      apple (red)_amt\n      2.0\n    \n    \n      3\n      2022-09-05\n      apple (red)_amt\n      5.0\n    \n    \n      4\n      2022-09-01\n      grape (red)_amt\n      5.0\n    \n    \n      5\n      2022-09-03\n      grape (red)_amt\n      6.0\n    \n    \n      6\n      2022-09-04\n      grape (red)_amt\n      NaN\n    \n    \n      7\n      2022-09-05\n      grape (red)_amt\n      5.0\n    \n    \n      8\n      2022-09-01\n      grape (white)_amt\n      6.0\n    \n    \n      9\n      2022-09-03\n      grape (white)_amt\n      4.0\n    \n    \n      10\n      2022-09-04\n      grape (white)_amt\n      9.0\n    \n    \n      11\n      2022-09-05\n      grape (white)_amt\n      8.0"
  },
  {
    "objectID": "snippets/2022-11-07-partitioned-join/index.html",
    "href": "snippets/2022-11-07-partitioned-join/index.html",
    "title": "Partitioned Join",
    "section": "",
    "text": "About\nFilling gaps for undefined value combinations can be tricky. It can either be implemented with a CROSS JOINS or a PARTITIONED JOIN. The advantage of a PARTITIONED JOIN is reduced complexity, due to fewer joins, and enhanced execution plan.\nAs example I explore a harvest of apples and Grapes and fill the list with zero for days where a fruit hasn‚Äôt been collected.\n\n\nImplementation\nThis query contains the harvest of three fruits over five days.\nSELECT f.name AS fruit, \n       h.date, \n       h.amount\n  FROM fruit f,\n       harvest h\n WHERE h.fruit_id = f.fruit_id\n ORDER BY 1, 2;\n\n\n\nFRUIT\nDATE\nAMOUNT\n\n\n\n\nGrape (red)\n2022-09-01\n5\n\n\nGrape (red)\n2022-09-03\n6\n\n\nGrape (red)\n2022-09-05\n5\n\n\nGrape (white)\n2022-09-01\n6\n\n\nGrape (white)\n2022-09-03\n4\n\n\nGrape (white)\n2022-09-04\n9\n\n\nGrape (white)\n2022-09-05\n8\n\n\nApple (red)\n2022-09-01\n4\n\n\nApple (red)\n2022-09-04\n2\n\n\nApple (red)\n2022-09-05\n5\n\n\n\nA PARTITIONED OUTER JOIN can be used to includes the dates without harvest for each fruit:\nWITH baskets as (\n  SELECT f.name AS fruit, \n         h.date, \n         h.amount\n    FROM fruit f,\n         harvest h\n  WHERE h.fruit_id = f.fruit_id\n)\nSELECT b.fruit,\n       t.date,\n       nvl(b.amount, 0) amount\n  FROM baskets b\nPARTITION BY (b.fruit)\n RIGHT OUTER JOIN time t\n    ON (t.date = b.date)\nORDER BY 1, 2;\n\n\n\nFRUIT\nDATE\nAMOUNT\n\n\n\n\nGrape (red)\n2022-09-01\n5\n\n\nGrape (red)\n2022-09-02\n0\n\n\nGrape (red)\n2022-09-03\n6\n\n\nGrape (red)\n2022-09-04\n0\n\n\nGrape (red)\n2022-09-05\n5\n\n\nGrape (white)\n2022-09-01\n6\n\n\nGrape (white)\n2022-09-02\n0\n\n\nGrape (white)\n2022-09-03\n4\n\n\nGrape (white)\n2022-09-04\n9\n\n\nGrape (white)\n2022-09-05\n8\n\n\nApple (red)\n2022-09-01\n4\n\n\nApple (red)\n2022-09-02\n0\n\n\nApple (red)\n2022-09-03\n0\n\n\nApple (red)\n2022-09-04\n2\n\n\nApple (red)\n2022-09-05\n5"
  },
  {
    "objectID": "snippets/2022-11-07-udaf/index.html",
    "href": "snippets/2022-11-07-udaf/index.html",
    "title": "User defined aggregation function",
    "section": "",
    "text": "About\nThis article describes the steps to create a customized aggregation function in Oracle.\n\n\n\\[Recall = \\frac{TP}{TP+FN}\\]\nAs example Recall is implemented.\n\n\nImplementation\nDefine a type for the input values, if it isn‚Äôt a native Oracle type (NUMBER, VARCHAR2, ‚Ä¶) or if the aggragation function will take more than one input value.\nCREATE OR REPLACE TYPE y_pair FORCE AS OBJECT (\n  y_true NUMBER, \n  y_pred NUMBER, \n  threshold NUMBER DEFAULT 0.5\n);\n/\nImplement logic of user defined function in a oracle object type.\n-- Create type header\nCREATE OR REPLACE TYPE RecallImpl AS OBJECT(\n  -- define object variables\n  tp NUMBER, -- number of true positives\n  fn NUMBER, -- number of false negatives\n\n  -- Interface for Oracle userdefined functions. \n  -- Init, iterate and terminate are mandatory\n  STATIC FUNCTION ODCIAggregateInitialize(sctx IN OUT RecallImpl) RETURN NUMBER,\n  MEMBER FUNCTION ODCIAggregateIterate(self IN OUT RecallImpl, value IN y_pair) RETURN NUMBER,\n  MEMBER FUNCTION ODCIAggregateDelete(self IN OUT RecallImpl, value IN y_pair) RETURN NUMBER,\n  MEMBER FUNCTION ODCIAggregateTerminate(self IN RecallImpl, recall OUT NUMBER, flags IN NUMBER) RETURN NUMBER,\n  MEMBER FUNCTION ODCIAggregateMerge(self IN OUT RecallImpl, ctx2 IN RecallImpl) RETURN NUMBER\n);\n/\n-- Create type body\nCREATE OR REPLACE TYPE BODY RecallImpl IS \n  \n\n  STATIC FUNCTION ODCIAggregateInitialize(sctx IN OUT RecallImpl) \n                                                        RETURN NUMBER IS \n  BEGIN\n\n    -- initialize tp and fn with 0\n    sctx := RecallImpl(0, 0);\n  \n    RETURN ODCIConst.Success;\n  END;\n\n\n  -- Increase tp and fn count is condition is satisfied\n  MEMBER FUNCTION ODCIAggregateIterate(self IN OUT RecallImpl, \n                                       value IN y_pair) RETURN NUMBER IS\n  BEGIN\n\n    IF value.y_true = 1 THEN\n      -- increase true positive count\n      IF value.y_pred >= value.y_true - value.threshold THEN\n        self.tp := self.tp + 1;\n      END IF;\n      \n      -- increase false negative count\n      IF value.y_pred < value.y_true - value.threshold THEN\n        self.fn := self.fn + 1;\n      END IF;\n    END IF;\n    \n    RETURN ODCIConst.Success;\n  END;\n\n\n  -- In case of aggregation in window function RECALL_CAF(...)OVER(ORDER BY...) \n  -- the entry leaving the window needs to decrease tp and fn count \n  -- if condition is satisfied\n  MEMBER FUNCTION ODCIAggregateDelete(self IN OUT RecallImpl, \n                                      value IN y_pair) RETURN NUMBER IS\n  BEGIN\n\n    IF value.y_true = 1 THEN\n      -- decrease true positive count\n      IF value.y_pred >= value.y_true - value.threshold THEN\n        self.tp := self.tp - 1;\n      END IF;\n      \n      -- decrease false negative count\n      IF value.y_pred < value.y_true - value.threshold THEN\n        self.fn := self.fn - 1;\n      END IF;\n    END IF;\n\n    RETURN ODCIConst.Success;\n  END;\n\n\n  -- Calculate *Recall* from tp and fn count\n  MEMBER FUNCTION ODCIAggregateTerminate(self IN RecallImpl, \n                                         recall OUT NUMBER, \n                                         flags IN NUMBER) RETURN NUMBER IS\n    v_recall number;\n  BEGIN\n\n    recall := self.tp / nullif(self.tp + self.fn, 0);\n\n    RETURN ODCIConst.Success;\n  BEGIN;\n\n\n  -- In case of parallel execution combine tp and fn counts \n  -- from parallel threads\n  MEMBER FUNCTION ODCIAggregateMerge(self IN OUT RecallImpl, \n                                     ctx2 IN RecallImpl) RETURN NUMBER IS\n  BEGIN\n\n    self.tp := self.tp + ctx2.tp;\n    self.fn := self.fn + ctx2.fn;\n  \n    RETURN ODCIConst.Success;\n  END;\n\n\nEND;\n/\n\n\nParallel aggregation of data:\n\n\n\n\nflowchart TB\n  A[Init] --> B1[Iter /\\nDel]\n  B1 --> B1\n  A --> B2[Iter /\\nDel]\n  B2 --> B2\n  B1 --> C[Merge]\n  B2 --> C[Merge]\n  C --> D[Terminate]\n\n\n\n\n\n\n\n\nDefine function that uses the implemented logic.\nCREATE OR REPLACE FUNCTION recall_caf(value y_pair) \n                    RETURN NUMBER PARALLEL_ENABLED AGGREGATION USING RecallImpl;\n/\nThe function can now be used as a aggregation function including analytical and window features.\nExample: Recall per kfold and two thresholds.\nSELECT \n  r.kfold,\n  recall_caf(y_pair(y_true => r.y_true, \n                    y_pred => r.y_oof)) recall_0_5,\n  recall_caf(y_pair(y_true => r.y_true, \n                    y_pred => r.y_oof, \n                    threshold => 0.7)) recall_0_7\n  FROM oof_results r\nGROUP BY r.kfold;\n\n\n\nKFOLD\nRECALL_0_5\nRECALL_0_7\n\n\n\n\n0\n0.72\n0.81\n\n\n1\n0.781\n0.841\n\n\n2\n0.754\n0.81\n\n\n3\n0.79\n0.838\n\n\n4\n0.71\n0.72\n\n\n\nor in detail:\nSELECT \n  r.kfold,\n  r.y_true,\n  r.y_oof,\n  recall_caf(y_pair(y_true => r.y_true, \n                    y_pred => r.y_oof)) OVER (PARTITION BY r.kfold) recall_0_5,\n  recall_caf(y_pair(y_true => r.y_true, \n                    y_pred => r.y_oof, \n                    threshold => 0.7)) OVER (PARTITION BY r.kfold) recall_0_7\nFROM oof_results r;\n\n\n\nKFOLD\nY_TRUE\nY_OOF\nRECALL_0_5\nRECALL_0_7\n\n\n\n\n0\n1\n0.9768\n0.72\n0.81\n\n\n0\n0\n0.56\n0.72\n0.81\n\n\n0\n0\n0.121\n0.72\n0.81\n\n\n0\n1\n0.433\n0.72\n0.81\n\n\n0\n1\n0.6768\n0.72\n0.81\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2\n0\n0.0068\n0.754\n0.87\n\n\n2\n1\n0.877\n0.754\n0.87\n\n\n2\n0\n0.1021\n0.754\n0.87\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\nResources\nMore details can be found in the Oracle documentation:\n\nhttps://docs.oracle.com/cd/B10501_01/appdev.920/a96595/dci11agg.htm\nhttps://docs.oracle.com/cd/B12037_01/appdev.101/b10800/dciaggref.htm"
  },
  {
    "objectID": "snippets/2023-02-02-num_dataset/index.html",
    "href": "snippets/2023-02-02-num_dataset/index.html",
    "title": "Numeric Dataset",
    "section": "",
    "text": "About\nA basic Dataset for numeric features from a Pandas dataframe. If there is enough memory the dataframe can be converted into a FloatTensor to speed up data access.\nDataframe consists of feature columns and target columns and a kfold column.\nclass NumDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, df, features = [], targets: list = None, kfolds=[0]):\n        \n        if targets:\n            self.targets = torch.FloatTensor(df[df['kfold'].isin(kfolds)][targets].values.reshape(-1,len(targets)))\n        else:\n            self.targets = torch.zeros(len(targets))\n        \n        # Convert Dataframe into FloatTensor for speed up\n        self.df = torch.FloatTensor(df[features].values.reshape(-1,len(features)))\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, item):\n        \n        return self.df[item], self.targets[item]\n        \nFor dict usage in Module.forward define dataset like this:\n....    \n    def __getitem__(self, item):\n        \n        return {'data': self.df[item], 'targets': self.targets[item]}\n....\n\nclass MyModel(nn.Module):\n  ....\n  def forward(self, data, targets):\n    ....\nDataLoader in Fastai:\ntrain_ds = NumDataset(df = train, features = FEATURES, targets=TARGETS, kfolds = [1,2,3,4])\nvalid_ds = NumDataset(df = train, features = FEATURES, targets=TARGETS, kfolds = [0])\n\ntrain_dl = DataLoader(train_ds, batch_size= BS_TRAIN, shuffle = True, num_workers = WORKERS) \nvalid_dl = DataLoader(valid_ds, batch_size= BS_TEST, shuffle = False, num_workers = WORKERS)\n\ndata = DataLoaders(train_dl, valid_dl).cuda() \nTraining in Fastai:\nlearn = Learner(data, MYMODEL, loss_func=MYLOSS, metrics=MYMETRICS)\n\nlearn.fit_one_cycle(1,1e-3)"
  },
  {
    "objectID": "snippets/2022-11-20-pandas-styles/index.html",
    "href": "snippets/2022-11-20-pandas-styles/index.html",
    "title": "Pandas styles",
    "section": "",
    "text": "A styling example from the blog post Pay off Mortgage or Invest. Includes MultiIndexed columns, bar charts, html table layout, et.\n\ndf.columns = pd.MultiIndex.from_tuples([('Mortgage', 'Interest'),\n                                        ('Mortgage', 'Principal'),\n                                        ('Mortgage', 'Extra Payment'),\n                                        ('Mortgage', 'Loan Balance'),\n                                        ('Investment', 'Payment'),\n                                        ('Investment', 'Balance'),\n                                        ('Appreciation', 'Balance'),\n                                        ('Net Worth', 'w/o appr.'),\n                                        ('Net Worth', 'with appr.')], names=['','Year'])\n                                        \nvmin = -interest_rate*principal\nvmax = m_payment-interest_rate*principal\n\n(df.style\n    .format('{:.0f}', na_rep=\"\")\n    .bar(subset=pd.IndexSlice[df[df[('Mortgage', 'Interest')]<=0].index,('Mortgage', 'Interest')],\n         align='right', vmin=vmin, vmax=0, cmap=\"autumn\", \n         height=80, width=80, props=\"width: 100px; border-right: 1px solid gray;\"\n        )\n    .bar(subset=pd.IndexSlice[df[df[('Mortgage', 'Principal')]>=0].index,('Mortgage', 'Principal')],\n         align='left', vmin=0, vmax=m_payment, cmap=\"summer_r\", \n         height=80, width=80, props=\"width: 100px; border-right: 1px solid gray;\"\n        )\n    .bar(subset=[('Mortgage', 'Extra Payment')], align='left', vmin=0, vmax=lump_sum, cmap=\"summer_r\", \n         height=80, width=80, props=\"width: 80px; border-right: 1px solid gray; border-left: 1px solid gray;\"\n        )\n    .bar(subset=[('Investment', 'Payment')], align='left', vmin=0, vmax=max(lump_sum, m_payment), cmap=\"summer_r\", \n         height=80, width=80, props=\"width: 80px; border-right: 1px solid gray; border-left: 1px solid gray;\"\n        )\n    \n    .bar(subset=[('Mortgage', 'Loan Balance')], align='right', vmin=-2.5*principal, vmax=0, cmap=\"PuRd_r\", \n         height=80, width=100, props=\"width: 80px; border-right: 1px solid gray; border-left: 1px solid gray;\"\n        )\n    .bar(subset=[('Appreciation', 'Balance')], align='left', vmin=0, vmax=2.5*principal, cmap=\"Blues\", \n         height=80, width=100, props=\"width: 80px; border-right: 1px solid gray;\"\n        )\n    .bar(subset=[('Investment', 'Balance')], align='left', vmin=0, vmax=2.5*principal, cmap=\"Blues\", \n         height=80, width=100, props=\"width: 80px; border-right: 1px solid gray; border-left: 1px solid gray;\"\n        )\n    .bar(subset=[('Net Worth', 'w/o appr.')], align='left', vmin=0, vmax=2.5*principal, cmap=\"Blues\", \n         height=80, width=100, props=\"width: 80px; border-right: 1px solid gray;\"\n        )\n    .bar(subset=[('Net Worth', 'with appr.')], align='left', vmin=0, vmax=2.5*principal, cmap=\"Blues\", \n         height=80, width=100, props=\"width: 80px; border-right: 1px solid gray;\"\n        )\n    .set_table_styles({\n       ('Total:'): [{'selector': 'th', 'props': 'border-top: 1px solid gray; border-bottom: 1px solid gray'},\n                  {'selector': 'td', 'props': 'border-top: 1px solid gray; border-bottom: 1px solid gray'}]\n       }, overwrite=False, axis=1)\n    .set_table_styles([\n        {'selector': 'table', 'props': 'border-spacing: 2px'},\n        {'selector': 'thead', 'props': 'border: 1px solid gray'}, \n        {'selector': 'th', 'props': 'text-align: center; padding: 4.5px;'},\n        {'selector': 'th.col_heading', 'props': 'border: 1px solid gray'},\n        {'selector': 'tbody', 'props': 'border: 1px solid gray'},\n        {'selector': 'td', 'props': 'text-align: center; border-left: 1px solid gray; border-right: 1px solid gray;'}\n       ], overwrite=False)\n    .set_table_styles({\n            ('Investment', 'Balance'): [{'selector': 'th', 'props': 'border-right: 1px solid gray'},\n                       {'selector': 'td', 'props': 'border-right: 1px solid gray'}]\n            }, overwrite=False, axis=0)\n)\n\n\n\n\n  \n    \n      \n      Mortgage\n      Investment\n      Appreciation\n      Net Worth\n    \n    \n      Year\n      Interest\n      Principal\n      Extra Payment\n      Loan Balance\n      Payment\n      Balance\n      Balance\n      w/o appr.\n      with appr.\n    \n  \n  \n    \n      0\n      \n      \n      \n      -100000\n      \n      0\n      \n      0\n      \n    \n    \n      1\n      -2500\n      2500\n      0\n      -97500\n      0\n      0\n      50\n      2500\n      2550\n    \n    \n      2\n      -2438\n      2562\n      0\n      -94938\n      15000\n      15000\n      205\n      20062\n      20267\n    \n    \n      3\n      -2373\n      2627\n      0\n      -92311\n      0\n      15420\n      471\n      23109\n      23580\n    \n    \n      4\n      -2308\n      2692\n      0\n      -89619\n      0\n      15852\n      856\n      26233\n      27089\n    \n    \n      5\n      -2240\n      2760\n      0\n      -86859\n      0\n      16296\n      1368\n      29436\n      30804\n    \n    \n      6\n      -2171\n      2829\n      0\n      -84031\n      0\n      16752\n      2015\n      32721\n      34736\n    \n    \n      7\n      -2101\n      2899\n      0\n      -81131\n      0\n      17221\n      2805\n      36090\n      38895\n    \n    \n      8\n      -2028\n      2972\n      0\n      -78160\n      0\n      17703\n      3749\n      39543\n      43293\n    \n    \n      9\n      -1954\n      3046\n      0\n      -75114\n      0\n      18199\n      4855\n      43085\n      47940\n    \n    \n      10\n      -1878\n      3122\n      0\n      -71992\n      0\n      18708\n      6134\n      46717\n      52851\n    \n    \n      11\n      -1800\n      3200\n      0\n      -68791\n      0\n      19232\n      7595\n      50441\n      58036\n    \n    \n      12\n      -1720\n      3280\n      0\n      -65511\n      0\n      19771\n      9251\n      54260\n      63511\n    \n    \n      13\n      -1638\n      3362\n      0\n      -62149\n      0\n      20324\n      11113\n      58175\n      69289\n    \n    \n      14\n      -1554\n      3446\n      0\n      -58703\n      0\n      20893\n      13194\n      62191\n      75384\n    \n    \n      15\n      -1468\n      3532\n      0\n      -55170\n      0\n      21478\n      15505\n      66308\n      81813\n    \n    \n      16\n      -1379\n      3621\n      0\n      -51549\n      0\n      22080\n      18062\n      70530\n      88592\n    \n    \n      17\n      -1289\n      3711\n      0\n      -47838\n      0\n      22698\n      20877\n      74860\n      95737\n    \n    \n      18\n      -1196\n      3804\n      0\n      -44034\n      0\n      23334\n      23967\n      79299\n      103267\n    \n    \n      19\n      -1101\n      3899\n      0\n      -40135\n      0\n      23987\n      27347\n      83852\n      111199\n    \n    \n      20\n      -1003\n      3997\n      0\n      -36138\n      0\n      24659\n      31033\n      88520\n      119554\n    \n    \n      21\n      -903\n      4097\n      0\n      -32042\n      0\n      25349\n      35044\n      93307\n      128351\n    \n    \n      22\n      -801\n      4199\n      0\n      -27843\n      0\n      26059\n      39396\n      98216\n      137612\n    \n    \n      23\n      -696\n      4304\n      0\n      -23539\n      0\n      26788\n      44110\n      103249\n      147360\n    \n    \n      24\n      -588\n      4412\n      0\n      -19127\n      0\n      27538\n      49206\n      108411\n      157617\n    \n    \n      25\n      -478\n      4522\n      0\n      -14606\n      0\n      28310\n      54704\n      113704\n      168408\n    \n    \n      26\n      -365\n      4635\n      0\n      -9971\n      0\n      29102\n      60627\n      119131\n      179759\n    \n    \n      27\n      -249\n      4751\n      0\n      -5220\n      0\n      29917\n      66999\n      124697\n      191696\n    \n    \n      28\n      -130\n      4870\n      0\n      -350\n      0\n      30755\n      73843\n      130404\n      204247\n    \n    \n      29\n      -9\n      350\n      0\n      0\n      4641\n      36257\n      77584\n      136257\n      213841\n    \n    \n      Total:\n      -40359\n      100000\n      0\n      0\n      19641\n      36257\n      77584\n      136257\n      213841"
  },
  {
    "objectID": "snippets/2022-11-20-pandas-styles/index.html#interactive-notebook",
    "href": "snippets/2022-11-20-pandas-styles/index.html#interactive-notebook",
    "title": "Pandas styles",
    "section": "Interactive notebook",
    "text": "Interactive notebook\n\nPlay with the numbers: \nDownloading the notebook: )"
  },
  {
    "objectID": "snippets/2022-11-20-pandas-styles/index.html#related-blog",
    "href": "snippets/2022-11-20-pandas-styles/index.html#related-blog",
    "title": "Pandas styles",
    "section": "Related Blog",
    "text": "Related Blog\n\nPay off Mortgage or Invest"
  },
  {
    "objectID": "snippets/2022-11-09-unpivot/index.html",
    "href": "snippets/2022-11-09-unpivot/index.html",
    "title": "Un/pivot",
    "section": "",
    "text": "HARVEST AMOUNT\n\n\nDATE\nFRUIT\nAMOUNT\n\n\n\n\n2022-09-01\nGrape_red\n4\n\n\n2022-09-01\nGrape_red\n1\n\n\n2022-09-01\nGrape_white\n6\n\n\n2022-09-01\nApple_red\n4\n\n\n2022-09-03\nGrape_red\n6\n\n\n2022-09-03\nGrape_white\n4\n\n\n2022-09-04\nGrape_white\n9\n\n\n2022-09-04\nApple_red\n2\n\n\n2022-09-05\nGrape_red\n5\n\n\n2022-09-05\nGrape_white\n8\n\n\n2022-09-05\nApple_red\n5\n\n\n\n\nPIVOT clauseAGG-CASE block\n\n\nSELECT *\n  FROM harvest_amount\n PIVOT(SUM(amount) AS amt \n       FOR fruit IN ('Grape_red' AS grape_red, \n                     'Grape_white' AS grape_white, \n                     'Apple_red' AS apple_red);\n);\n\n\nSELECT h.date,\n       SUM(CASE WHEN h.fruit = 'Grape_red' THEN \n                    h.amount \n                ELSE \n                    0 \n            END) AS grape_red_amt,\n       SUM(CASE WHEN h.fruit = 'Grape_white' THEN \n                    h.amount \n                ELSE \n                    0 \n            END) AS grape_white_amt,\n       SUM(CASE WHEN h.fruit = 'Apple_red' THEN \n                    h.amount \n                ELSE \n                    0 \n            END) AS apple_red_amt,\n  FROM harvest_amount h\n GROUP BY h.date;\n\n\n\n\nPivoted table\n\n\nDATE\nGRAPE_RED_AMT\nGRAPE_WHITE_AMT\nAPPLE_RED_AMT\n\n\n\n\n2022-09-01\n5\n6\n4\n\n\n2022-09-03\n6\n4\n\n\n\n2022-09-04\n\n9\n2\n\n\n2022-09-04\n5\n8\n5\n\n\n\n\n\nSELECT *\n  FROM harvest_amount\n PIVOT(sum(amount) as amt, \n       max(amount) as max_amt \n       FOR fruit IN ('Grape_red', 'Grape_white', 'Apple_red');\n);"
  },
  {
    "objectID": "snippets/2022-11-09-unpivot/index.html#multiple-columns-1",
    "href": "snippets/2022-11-09-unpivot/index.html#multiple-columns-1",
    "title": "Un/pivot",
    "section": "Multiple columns",
    "text": "Multiple columns\nSELECT *\n  FROM harvest_amt_qlt\nUNPIVOT((amount, quality) \n        FOR fruit IN ((grape_red_amt, grape_red_qlt) AS 'Grape (red)',\n                      (grape_white_amt, grape_white_qlt) AS 'Grape (red)'\n                      (apple_red_amt, apple_red_qlt) AS 'Grape (red)');\n);"
  },
  {
    "objectID": "snippets/2022-11-20-recursive-sql/index.html",
    "href": "snippets/2022-11-20-recursive-sql/index.html",
    "title": "Recursive SQL",
    "section": "",
    "text": "!sqlite3 fin.db .databases .quit\n\nmain: /home/git_repos/blog/snippets/2022-11-20-recursive-sql/fin.db r/w"
  },
  {
    "objectID": "snippets/2022-11-20-recursive-sql/index.html#interactive-notebook",
    "href": "snippets/2022-11-20-recursive-sql/index.html#interactive-notebook",
    "title": "Recursive SQL",
    "section": "Interactive notebook",
    "text": "Interactive notebook\n\nPlay with the numbers: \nDownloading the notebook:"
  },
  {
    "objectID": "snippets/2022-11-20-recursive-sql/index.html#related-blog",
    "href": "snippets/2022-11-20-recursive-sql/index.html#related-blog",
    "title": "Recursive SQL",
    "section": "Related Blog",
    "text": "Related Blog\n\nPay off Mortgage or Invest"
  }
]