{"title":"A handful of bricks - from SQL to Pandas","markdown":{"yaml":{"toc":true,"layout":"post","comments":{"utterances":{"repo":"joatom/blog"}},"date":"2020-12-12","description":"A step by step guide to learn Pandas from a SQL perspective.","categories":["SQL","Pandas","BigQuery","ML"],"image":"logo_sql2pandas.jpg","title":"A handful of bricks - from SQL to Pandas","author":"Johannes Tomasoni"},"headingText":"SQL ~~vs.~~ and Pandas","containsRefs":false,"markdown":"\n\nOriginal article published at [datamuni.com](https://datamuni.com/@joatom/a-handful-of-bricks-from-sql-to-pandas).\n\n\nI love SQL. It's been around for decades to arrange and analyse data. Data is kept in tables which are stored in a relational structure. Consistancy and data integraty is kept in mind when designing a relational data model. However, when it comes to machine learning other data structures such as matrices and tensors become important to feat the underlying algorithms and make data processing more efficient. That's where Pandas steps in. From a SQL developer perspective it is the library to close the gap between your data storage and the ml frameworks.\n\nThis blog post shows how to translate some common and some advanced techniques from SQL to Pandas step-by-step. I didn't just want to write a plain cheat sheet (actually Pandas has a good one to get started: [Comparison SQL](https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html) (Ref. 1)). Rather I want to unwind some concepts that might be helpful for a SQL developer who now and then deals with Pandas.\n\nThe coding examples are built upon a [Lego Dataset](https://www.kaggle.com/rtatman/lego-database) (Ref. 2), that contains a couple of tables with data about various lego sets. \n> To follow along I've provided a [notebook](https://www.kaggle.com/joatom/a-handful-of-bricks-from-sql-to-pandas) (Res. 1) on kaggle, where you can play with the blog examples either using SQLite or Bigquery. You can also checkout a [docker container](https://github.com/joatom/blog-resources/tree/main/handful_bricks) (Res. 2) to play on your home machine.\n\n# Missing bricks\n\nFirst listen to this imaginary dialogue that guides us throug the coding:\n\n![](chick.png): *I miss all red bricks of the Lego Pizzeria. I definetly need a new one.*\n\n![](penguin.png): *Don't worry. We can try to solve this with data. That will be fun. :-)*\n\n![](chick.png): *(!@#%&) You're kidding, right?*\n\nNow that we have a mission we are ready to code and figuere out how to deal with missing bricks.\nFirst we inspect the tables. They are organized as shown in the relational diagram (Fig. 1).\n\n![](schema.png)\n\nFig. 1: Data model ([source: Lego dataset](https://www.kaggle.com/rtatman/lego-database) (Ref. 2))\n\nThere are colors, parts, sets and inventories. We should start by searching for the *Pizzeria* in the `sets` table using the set number (*41311*).\n\n![](piz.png)\n\nFig. 2: Lego Box with set number\n\n# A simple Filter *(The behaviour of brackets.)*\nA simple `like`-filter on the `sets` table will return the set info.\n\n```sql\nSELECT *\n  FROM sets s\n WHERE s.set_num like '41311%'\n```\n\n\nThere are several ways to apply a filter in Pandas. The most SQL-like code utilizes the `query`-function which basicaly substitutes the `where` clause.\n\n\n```python\ndf_sets.query(\"set_num.str.contains('41311')\", engine='python')\n```\n\n\n\n\n||set_num|name|year|theme_id|num_parts|\n|-|-|-|-|-|-|\n|**3582**|41311-1|Heartlake Pizzeria|2017|494|287|\n\n\n\n\nSince the `query` function expects a String as input parameter we loose syntax highlighting and syntax checking for our filter expression.\n\nTherefore a more commonly used expression consists of the **bracket notation** (The behaviour of the bracket notation of a class in python is implementated in the class function `__getitem__`)\n\nSee how we can apply an equals filter using brackets.\n\n\n```python\ndf_sets.query(\"set_num == '41311-1'\")\n\n# or\n\ndf_sets[df_sets.set_num == '41311-1']\n\n# or\n\ndf_sets[df_sets['set_num'] == '41311-1']\n```\n\n\n\n\n\n||set_num|name|year|theme_id|num_parts|\n|-|-|-|-|-|-|\n|**3582**|41311-1|Heartlake Pizzeria|2017|494|287|\n\n\n\nThere is a lot going on in this expression.\n\nLet's take it apart.\n\n`df_sets['set_num']` returns a single column (a *Pandas.Series* object). A Pandas DataFrame is basically a collection of Series. Additionaly there is a row index (often just called *index*) and a column index (*columnnames*). Think of a column store database.\n\n![](df.png)\n\nFig. 3: Elements of a DataFrame\n\nApplying a boolean condition (`== '41311-1'`) to a Series of the DataFrame (`df_sets['set_num']`) will result in a boolean collection of the size of the column.\n\n\n```python\nbool_coll = df_sets['set_num'] == '41311-1'\n\n# only look at the position 3580 - 3590 of the collection\nbool_coll[3580:3590]\n```\n\n\n\n```bash\n    3580    False\n    3581    False\n    3582     True\n    3583    False\n    3584    False\n    3585    False\n    3586    False\n    3587    False\n    3588    False\n    3589    False\n    Name: set_num, dtype: bool\n```\n\n\nThe boolean collection now gets past to the DataFrame and filters the rows:\n```python\ndf_sets[bool_coll]\n# or \ndf_sets[df_sets['set_num'] == '41311-1']\n```\nDepending on what type of object we pass to the square brackets the outcome result in very different behaviors. \n\nWe have already seen in the example above that if we pass a **boolean collection** with the size of number of rows, the collection is been handled as a **row filter**. \n\nBut if we pass a column name or a **list of column** names to the brackets instead, the given columns are selected like in the `SELECT` clause of an SQL statement. \n```sql\nSELECT name,\n       year\n  FROM lego.sets;\n```\n=> \n```python\ndf_sets[['name', 'year']]\n```\n\nRow filter and column selection can be combined like this:\n```sql\nSELECT s.name,\n       s.year\n  FROM lego.sets s\n WHERE s.set_num = '41311-1';\n```\n=>\n\n\n```python\ndf_temp = df_sets[df_sets['set_num'] == '41311-1']\ndf_temp[['name','year']]\n\n# or simply:\n\ndf_sets[df_sets['set_num'] == '41311-1'][['name','year']]\n```\n\n\n\n\n||name|year|\n|-|-|-|\n|3582|Heartlake Pizzeria|2017|\n\n\n\n# Indexing *(What actually is an index?)*\nAnother way to access a row in Pandas is by using the row index. With the `loc` function (and brackets) we select the *Pizzeria* and another arbitrary set. We use the row numbers to filter the rows.\n\n\n```python\ndf_sets.loc[[236, 3582]]\n```\n\n\n\n\n||set_num|name|year|theme_id|num_parts|\n|-|-|-|-|-|-|\n|**236**|10255-1|Assembly Square|2017|155|4009|\n|**3582**|41311-1|Heartlake Pizzeria|2017|494|287|\n\n\n\nIf we inspect the DataFrame closely we realize that it doesn't realy look like a simple table but rather like a **cross table**. \n\nThe first column on the left is a row index and the table header is the column index. In the center the values of the columns are displayed (see Fig. 3). \n\nIf we think of the values as a matrix the rows are dimension 0 and columns are dimension 1. The dimension is often used in DataFrame functions as `axis` parameter. E.g. dropping columns can be done using dimensional information:\n\n```sql\n-- EXCEPT in SELECT clause only works with BIGQUERY\nSELECT s.* EXCEPT s.year\n  FROM lego.sets s;\n```\n\n==> \n\n\n```python\ndf_sets.drop(['year'], axis = 1).head(5)\n```\n\n\n\n\n||set_num|name|theme_id|num_parts|\n|-|-|-|-|-|\n|**0**|00-1|Weetabix Castle|414|471|\n|**1**|0011-2|Town Mini-Figures|84|12|\n|**2**|0011-3|Castle 2 for 1 Bonus Offer|199|2|\n|**3**|0012-1|Space Mini-Figures|143|12|\n|**4**|0013-1|Space Mini-Figures|143|12|\n\n\n\nThe indexes can be accessed with the `index` and `columns` variable. `axes` contains both.\n\n\n```python\ndf_sets.index # row index\ndf_sets.columns # column index\n\ndf_sets.axes # both\n```\n\n\n\n```bash\n    [RangeIndex(start=0, stop=11673, step=1),\n     Index(['set_num', 'name', 'year', 'theme_id', 'num_parts'], dtype='object')]\n```\n\n\nThe row index doesn't necessarely be the row number. We can also convert a column into a row index.\n\n\n```python\ndf_sets.set_index('set_num').head()\n```\n\n\n\n\n||name|year|theme_id|num_parts|\n|-|-|-|-|-|\n|**set_num**|||||\n|**00-1**|Weetabix Castle|1970|414|471|\n|**0011-2**|Town Mini-Figures|1978|84|12|\n|**0011-3**|Castle 2 for 1 Bonus Offer|1987|199|2|\n|**0012-1**|Space Mini-Figures|1979|143|12|\n|**0013-1**|Space Mini-Figures|1979|143|12|\n\n\n\nIt is also possible to define hierarchicle indicies for multi dimensional representation. \n\n\n```python\ndf_sets.set_index(['year', 'set_num']).sort_index(axis=0).head() # axis = 0 => row index\n```\n\n\n\n\n|||name|theme_id|num_parts|\n|-|-|-|-|-|\n|**year**|**set_num**||||\n|**150**|**700.1.1-1**|Individual 2 x 4 Bricks|371|10|\n||**700.1.2-1**|Individual 2 x 2 Bricks|371|9|\n||**700.A-1**|Automatic Binding Bricks Small Brick Set (Lego...|366|24|\n||**700.B.1-1**|Individual 1 x 4 x 2 Window (without glass)|371|7|\n||**700.B.2-1**|Individual 1 x 2 x 3 Window (without glass)|371|7|\n\n\n\nSometimes it is usefull to reset the index, hence reset the row numbers.\n\n\n```python\ndf_sets.loc[[236, 3582]].reset_index(drop = True) # set drop = False to keep the old index as new column\n```\n\n\n\n\n||set_num|name|year|theme_id|num_parts|\n|-|-|-|-|-|-|\n|**0**|10255-1|Assembly Square|2017|155|4009|\n|**1**|41311-1|Heartlake Pizzeria|2017|494|287|\n\n\n\nNow we get a sence what is meant by an index in Pandas in contrast to SQL.\n\n**Indices in SQL** are hidden data structures (in form of e.g. b-trees or hash-tables). They are built to **access data more quickly**, to avoid full table scans when appropriate or to mantain consistancies when used with constraints.\n\nAn **index in Pandas** can rather be seen as a **dimensional access** to the data values. They can be distingueshed between row and column indices.\n\n# Joins *(Why merge doesn't mean upsert.)*\n\n![](chick.png): *What are we gonna do now about my missing parts?*\n\n![](penguin.png): *We don't have all the information we need, yet. We need to join the other tables.*\n\nThough there is a function called `join` to join DataFrames I always use the `merge` function. This can be a bit confusing, when you are used to Oracle where *merge* means upsert/updelete rather then combining two tables.\n\nWhen combining two DataFrames with the `merge` function in Pandas we have to define the relationship more explicitly. If you are used to SQL thats what you want. \n\nIn contrast the `join` function implicitly combines the DataFrames by their index or column names. It also enables multiply DataFrame joins in one statement as long as the join columns are matchable by name.\n```sql\nSELECT * \n  FROM sets s\n INNER JOIN\n       inventories i\n    ON s.set_num = i.set_num -- USING (set_num)\nLIMIT 5\n```\n\n\n|set_num|name|year|theme_id|num_parts|id|version|set_num_1|\n|-|-|-|-|-|-|-|-|\n|00-1|Weetabix Castle|1970|414|471|5574|1|00-1|\n|0011-2|Town Mini-Figures|1978|84|12|5087|1|0011-2|\n|0011-3|Castle 2 for 1 Bonus Offer|1987|199|2|2216|1|0011-3|\n|0012-1|Space Mini-Figures|1979|143|12|1414|1|0012-1|\n|0013-1|Space Mini-Figures|1979|143|12|4609|1|0013-1|\n\n\n\nThese Pandas statements all do the same:\n\n\n```python\ndf_sets.merge(df_inventories, how = 'inner', on = 'set_num').head(5)\n\n#or if columns are matching\n\ndf_sets.merge(df_inventories, on = 'set_num').head(5)\n\n#or explicitly defined columns\n\ndf_sets.merge(df_inventories, how = 'inner', left_on = 'set_num', right_on = 'set_num').head(5)\n```\n\n\n\n\n||set_num|name|year|theme_id|num_parts|id|version|\n|-|-|-|-|-|-|-|-|\n|**0**|00-1|Weetabix Castle|1970|414|471|5574|1|\n|**1**|0011-2|Town Mini-Figures|1978|84|12|5087|1|\n|**2**|0011-3|Castle 2 for 1 Bonus Offer|1987|199|2|2216|1|\n|**3**|0012-1|Space Mini-Figures|1979|143|12|1414|1|\n|**4**|0013-1|Space Mini-Figures|1979|143|12|4609|1|\n\n\n\nTo see witch parts are needed for the *Pizzeria* we combine some tables. We look for the inventory of the set and gather all parts. Then we get color and part category information.\n\nWe end up with an inventory list:\n```sql\nSELECT s.set_num, \n       s.name set_name, \n       p.part_num, \n       p.name part_name, \n       ip.quantity,\n       c.name color,\n       pc.name part_cat\n  FROM sets s,\n       inventories i,\n       inventory_parts ip,\n       parts p,\n       colors c,\n       part_categories pc\n WHERE s.set_num = i.set_num\n   AND i.id = ip.inventory_id\n   AND ip.part_num = p.part_num\n   AND ip.color_id = c.id\n   AND p.part_cat_id = pc.id \n   AND s.set_num in ('41311-1')\n   AND i.version = 1\n   AND ip.is_spare = 'f'\n ORDER BY p.name, s.set_num, c.name\nLIMIT 10\n```\n\n\n|set_num|set_name|part_num|part_name|quantity|color|part_cat|\n|-|-|-|-|-|-|-|\n|41311-1|Heartlake Pizzeria|25269pr03|1/4 CIRCLE TILE 1X1 with Pizza Print|4|Tan|Tiles Printed|\n|41311-1|Heartlake Pizzeria|32807|BRICK 1X1X1 1/3, W/ ARCH|4|Red|Other|\n|41311-1|Heartlake Pizzeria|6190|Bar 1 x 3 (Radio Handle, Phone Handset)|1|Red|Bars, Ladders and Fences|\n|41311-1|Heartlake Pizzeria|30374|Bar 4L (Lightsaber Blade / Wand)|1|Light Bluish Gray|Bars, Ladders and Fences|\n|41311-1|Heartlake Pizzeria|99207|Bracket 1 x 2 - 2 x 2 Inverted|1|Black|Plates Special|\n|41311-1|Heartlake Pizzeria|2453b|Brick 1 x 1 x 5 with Solid Stud|4|Tan|Bricks|\n|41311-1|Heartlake Pizzeria|3004|Brick 1 x 2|4|Light Bluish Gray|Bricks|\n|41311-1|Heartlake Pizzeria|3004|Brick 1 x 2|3|Tan|Bricks|\n|41311-1|Heartlake Pizzeria|3004|Brick 1 x 2|1|White|Bricks|\n|41311-1|Heartlake Pizzeria|3245b|Brick 1 x 2 x 2 with Inside Axle Holder|2|White|Bricks|\n\n\n\nLets create a general `inventory_list` as view and make the statement more readable and comparable with ANSI-syntax (separating filters/predicates from join conditions).\n```sql\nDROP VIEW IF EXISTS inventory_list;\nCREATE VIEW inventory_list AS\nSELECT s.set_num, \n       s.name set_name, \n       s.theme_id,\n       s.num_parts,\n       p.part_num, \n       ip.quantity,\n       p.name part_name, \n       c.name color,\n       pc.name part_cat\n  FROM sets s\n INNER JOIN\n       inventories i\n USING (set_num)\n INNER JOIN\n       inventory_parts ip\n    ON (i.id = ip.inventory_id)\n INNER JOIN\n       parts p\n USING (part_num)\n INNER JOIN\n       colors c\n    ON (ip.color_id = c.id)\n INNER JOIN\n       part_categories pc\n    ON (p.part_cat_id = pc.id)\n WHERE i.version = 1\n   AND ip.is_spare = 'f';\n```\n\n\nNow we translate the view to Pandas. We see how the structure relates to sql. The parameter `how` defines the type of join (here: `inner`), `on` represents the `USING` clause whereas `left_on` and `right_on` stand for the SQL `ON` condition.\n\nIn SQL usually an optimizer defines based in rules or statistics the execution plan (the order in which the tables are accessed, combined and filtered). I'm not sure if Pandas follows a similar approache. To be safe, I assume the order and early column dropping might matter for performance and memory management.\n\n\n```python\ndf_inventory_list = df_sets[['set_num', 'name', 'theme_id', 'num_parts']] \\\n                    .merge(\n                    df_inventories[df_inventories['version'] == 1][['id', 'set_num']],\n                        how = 'inner',\n                        on = 'set_num'\n                    ) \\\n                    .merge(\n                    df_inventory_parts[df_inventory_parts['is_spare'] == 'f'][['inventory_id', 'part_num', 'color_id', 'quantity']],\n                        how = 'inner',\n                        left_on = 'id',\n                        right_on = 'inventory_id'\n                    ) \\\n                    .merge(\n                    df_parts[['part_num', 'name', 'part_cat_id']],\n                        how = 'inner',\n                        on = 'part_num'\n                    ) \\\n                    .merge(\n                    df_colors[['id', 'name']],\n                        how = 'inner',\n                        left_on = 'color_id',\n                        right_on = 'id'\n                    ) \\\n                    .merge(\n                    df_part_categories[['id', 'name']],\n                        how = 'inner',\n                        left_on = 'part_cat_id',\n                        right_on = 'id'\n                    )\n\n# remove some columns and use index as row number (reset_index)\ndf_inventory_list = df_inventory_list.drop(['id_x', 'inventory_id', 'color_id', 'part_cat_id', 'id_y', 'id'], axis = 1).reset_index(drop = True)\n\n# rename columns\ndf_inventory_list.columns = ['set_num', 'set_name', 'theme_id', 'num_parts', 'part_num', 'quantity', 'part_name', 'color', 'part_cat']\n```\n\nLots of code here. So we better check if our Pandas code matches the results of our SQL code.\n\nSelect the inventory list for our example, write it to a DataFrame (`df_test_from_sql`) and compare the results.\n```sql\ndf_test_from_sql <<\nSELECT il.* \n  FROM inventory_list il\n WHERE il.set_num in ('41311-1')\n ORDER BY \n       il.part_name, \n       il.set_num, \n       il.color\nLIMIT 10;\n```\n\n\n|set_num|set_name|theme_id|num_parts|part_num|quantity|part_name|color|part_cat|\n|-|-|-|-|-|-|-|-|-|\n|41311-1|Heartlake Pizzeria|494|287|25269pr03|4|1/4 CIRCLE TILE 1X1 with Pizza Print|Tan|Tiles Printed|\n|41311-1|Heartlake Pizzeria|494|287|32807|4|BRICK 1X1X1 1/3, W/ ARCH|Red|Other|\n|41311-1|Heartlake Pizzeria|494|287|6190|1|Bar 1 x 3 (Radio Handle, Phone Handset)|Red|Bars, Ladders and Fences|\n|41311-1|Heartlake Pizzeria|494|287|30374|1|Bar 4L (Lightsaber Blade / Wand)|Light Bluish Gray|Bars, Ladders and Fences|\n|41311-1|Heartlake Pizzeria|494|287|99207|1|Bracket 1 x 2 - 2 x 2 Inverted|Black|Plates Special|\n|41311-1|Heartlake Pizzeria|494|287|2453b|4|Brick 1 x 1 x 5 with Solid Stud|Tan|Bricks|\n|41311-1|Heartlake Pizzeria|494|287|3004|4|Brick 1 x 2|Light Bluish Gray|Bricks|\n|41311-1|Heartlake Pizzeria|494|287|3004|3|Brick 1 x 2|Tan|Bricks|\n|41311-1|Heartlake Pizzeria|494|287|3004|1|Brick 1 x 2|White|Bricks|\n|41311-1|Heartlake Pizzeria|494|287|3245b|2|Brick 1 x 2 x 2 with Inside Axle Holder|White|Bricks|\n\n\n\n\n```python\n# test\ndf_test_from_df = df_inventory_list[df_inventory_list['set_num'].isin(['41311-1'])].sort_values(by=['part_name', 'set_num', 'color']).head(10)\n\ndf_test_from_df\n```\n\n\n\n\n||set_num|set_name|theme_id|num_parts|part_num|quantity|part_name|color|part_cat|\n|-|-|-|-|-|-|-|-|-|-|\n|**507161**|41311-1|Heartlake Pizzeria|494|287|25269pr03|4|1/4 CIRCLE TILE 1X1 with Pizza Print|Tan|Tiles Printed|\n|**543292**|41311-1|Heartlake Pizzeria|494|287|32807|4|BRICK 1X1X1 1/3, W/ ARCH|Red|Other|\n|**266022**|41311-1|Heartlake Pizzeria|494|287|6190|1|Bar 1 x 3 (Radio Handle, Phone Handset)|Red|Bars, Ladders and Fences|\n|**273113**|41311-1|Heartlake Pizzeria|494|287|30374|1|Bar 4L (Lightsaber Blade / Wand)|Light Bluish Gray|Bars, Ladders and Fences|\n|**306863**|41311-1|Heartlake Pizzeria|494|287|99207|1|Bracket 1 x 2 - 2 x 2 Inverted|Black|Plates Special|\n|**47206**|41311-1|Heartlake Pizzeria|494|287|2453b|4|Brick 1 x 1 x 5 with Solid Stud|Tan|Bricks|\n|**50211**|41311-1|Heartlake Pizzeria|494|287|3004|4|Brick 1 x 2|Light Bluish Gray|Bricks|\n|**45716**|41311-1|Heartlake Pizzeria|494|287|3004|3|Brick 1 x 2|Tan|Bricks|\n|**16485**|41311-1|Heartlake Pizzeria|494|287|3004|1|Brick 1 x 2|White|Bricks|\n|**22890**|41311-1|Heartlake Pizzeria|494|287|3245b|2|Brick 1 x 2 x 2 with Inside Axle Holder|White|Bricks|\n\n\n\n\n\n\n```python\n# assert equals\nif not USE_BIGQUERY:\n    df_test_from_sql = df_test_from_sql.DataFrame()\n    \npd._testing.assert_frame_equal(df_test_from_sql, df_test_from_df.reset_index(drop = True))\n```\n\nThe results are equal as expected.\n\nSince we are only interested in the red bricks we create a list of those missing parts.\n```sql\nSELECT *\n  FROM inventory_list il\n WHERE il.set_num = '41311-1'\n   AND part_cat like '%Brick%'\n   AND color = 'Red'\n ORDER BY \n       il.color, \n       il.part_name, \n       il.set_num;\n```\n\n\n|set_num|set_name|theme_id|num_parts|part_num|quantity|part_name|color|part_cat|\n|-|-|-|-|-|-|-|-|-|\n|41311-1|Heartlake Pizzeria|494|287|3039|1|Slope 45° 2 x 2|Red|Bricks Sloped|\n|41311-1|Heartlake Pizzeria|494|287|3045|2|Slope 45° 2 x 2 Double Convex|Red|Bricks Sloped|\n\n\n\nWe need to watchout for the brackets when combining filters in DataFrames.\n\n\n```python\ndf_missing_parts = df_inventory_list[(df_inventory_list['set_num'] == '41311-1') & \n                                     (df_inventory_list['part_cat'].str.contains(\"Brick\")) &\n                                     (df_inventory_list['color'] == 'Red')\n                                    ].sort_values(by=['color', 'part_name', 'set_num']).reset_index(drop = True)\n\ndf_missing_parts\n```\n\n\n\n\n||set_num|set_name|theme_id|num_parts|part_num|quantity|part_name|color|part_cat|\n|-|-|-|-|-|-|-|-|-|-|\n|**0**|41311-1|Heartlake Pizzeria|494|287|3039|1|Slope 45° 2 x 2|Red|Bricks Sloped|\n|**1**|41311-1|Heartlake Pizzeria|494|287|3045|2|Slope 45° 2 x 2 Double Convex|Red|Bricks Sloped|\n\n\n\n![](penguin.png): *There we go, we are missing one 2x2 brick and tw0 2x2 double convex.*\n\n![](chick.png): *Yup, that's the roof of the fireplace. I knew that before.*\n\n# Conditional Joins and Aggregation *(Almost done!)*\n\nNext we search for sets that contain the missing parts. The quantity of the parts in the found sets must be greater or equal the quantity of the missing parts.\n\nIn SQL it is done with an **conditional join** `il.quantity >= mp.quantity`.\n\n```sql\nsets_with_missing_parts << \n\n-- A list of missing parts\n\nWITH missing_parts AS (\n  SELECT il.set_name,\n         il.part_num, \n         il.part_name,\n         il.color,\n         il.quantity\n    FROM inventory_list il\n   WHERE il.set_num = '41311-1'\n     AND il.part_cat like '%Brick%'\n     AND il.color = 'Red'\n)\n\n-- Looking for set that contains as much of the missing parts as needed\n\nSELECT mp.set_name as searching_for_set,\n       il.set_num, \n       il.set_name, \n       il.part_name,\n       -- total number of parts per set\n       il.num_parts,\n       -- how many of the missing parts were found in the set \n       COUNT(*) OVER (PARTITION BY il.set_num) AS matches_per_set\n  FROM inventory_list il\n INNER JOIN\n       missing_parts mp\n    ON (il.part_num = mp.part_num\n        AND il.color = mp.color\n        -- searching for a set that contains at least as much parts as there are missing\n        AND il.quantity >= mp.quantity\n       )\n -- don't search in the Pizzeria set\n WHERE il.set_num <> '41311-1'\n -- prioritize sets with all the missing parts and as few parts as possible\n ORDER BY \n       matches_per_set DESC, \n       il.num_parts, \n       il.set_num,\n       il.part_name\nLIMIT 16\n```\n\n\n## Conditional Join\n\nThere is no intuitive way to do a conditional join on DataFrames. The easiest I've [seen](https://stackoverflow.com/questions/23508351/how-to-do-workaround-a-conditional-join-in-python-pandas) so far is a two step solution.\nAs substitution for the SQL `WITH`-clause we can reuse `df_missing_parts`.\n\n\n```python\n# 1. merge on the equal conditions\ndf_sets_with_missing_parts = df_inventory_list.merge(df_missing_parts, how = 'inner', on = ['part_num', 'color'], suffixes = ('_found', '_missing'))\n# 2. apply filter for the qreater equals condition\ndf_sets_with_missing_parts = df_sets_with_missing_parts[df_sets_with_missing_parts['quantity_found'] >= df_sets_with_missing_parts['quantity_missing']]\n\n# select columns\ncols = ['set_num', 'set_name', 'part_name', 'num_parts']\ndf_sets_with_missing_parts = df_sets_with_missing_parts[['set_name_missing'] + [c + '_found' for c in cols]]\ndf_sets_with_missing_parts.columns = ['searching_for_set'] + cols\n```\n\n## Aggregation\n\nIn the next step the aggregation of the analytic function \n```sql\nCOUNT(*) OVER (PARTITION BY il.set_num) matches_per_set\n```\nneeds to be calculated. Hence the number of not-NaN values will be counted per `SET_NUM` group and assigned to each row in a new column (`matches_per_set`).\n\nBut before translating the analytic function, let's have a look at a regular aggregation, first. Say, we simply want to count the entries per `set_num` on group level (without assigning the results back to the original group entries) and also sum up all parts of a group. Then the SQL would look something like this:\n```sql\nSELECT s.set_num,\n       COUNT(*) AS matches_per_set\n       SUM(s.num_parts) AS total_num_parts\n  FROM ...\n WHERE ...\n GROUP BY \n       s.set_num;\n```\nAll selected columns must either be aggregated by a function (`COUNT`, `SUM`) or defined as a group (`GROUP BY`).\nThe result is a two column list with the group `set_num` and the aggregations `matches_per_set` and `total_num_part`.\n\nNow see how the counting is done with Pandas.\n\n\n```python\ndf_sets_with_missing_parts.groupby(['set_num']).count()  .sort_values('set_num', ascending = False)\n\n# for sum and count:\n# df_sets_with_missing_parts.groupby(['set_num']).agg(['count', 'sum']) \n```\n\n\n\n\n||searching_for_set|set_name|part_name|num_parts|\n|-|-|-|-|-|\n|**set_num**|||||\n|**llca8-1**|1|1|1|1|\n|**llca21-1**|1|1|1|1|\n|**fruit1-1**|1|1|1|1|\n|**MMMB026-1**|1|1|1|1|\n|**MMMB003-1**|1|1|1|1|\n|**...**|...|...|...|...|\n|**10021-1**|1|1|1|1|\n|**088-1**|1|1|1|1|\n|**080-1**|2|2|2|2|\n|**066-1**|1|1|1|1|\n|**00-4**|1|1|1|1|\n\n\nWow, that's different! The aggregation function is applied to every column independently and the group is set as row index. \nBut it is also possible to define the aggregation function for each column explicitly like in SQL:\n\n\n```python\ndf_sets_with_missing_parts.groupby(['set_num'], as_index = False) \\\n    .agg(matches_per_set = pd.NamedAgg(column = \"set_num\", aggfunc = \"count\"), \n         total_num_parts = pd.NamedAgg(column = \"num_parts\", aggfunc = \"sum\"))\n```\n\n\n\n\n||set_num|matches_per_set|total_num_parts|\n|-|-|-|-|\n|**0**|00-4|1|126|\n|**1**|066-1|1|407|\n|**2**|080-1|2|1420|\n|**3**|088-1|1|615|\n|**4**|10021-1|1|974|\n|**...**|...|...|...|\n|**463**|MMMB003-1|1|15|\n|**464**|MMMB026-1|1|43|\n|**465**|fruit1-1|1|8|\n|**466**|llca21-1|1|42|\n|**467**|llca8-1|1|58|\n\n\n\n\nThis looks more familiar. With the `as_index` argument the group becomes a column (rather than a row index).\n\nSo, now we return to our initial task translating the `COUNT(*) OVER(PARTITION BY)` clause. One approach could be to join the results of the above aggregated DataFrame with the origanal DataFrame, like\n```python\ndf_sets_with_missing_parts.merge(my_agg_df, on = 'set_num')\n```\nA more common why is to use the `transform()` function:\n\n\n```python\n# add aggregatiom\ndf_sets_with_missing_parts['matches_per_set'] = df_sets_with_missing_parts.groupby(['set_num'])['part_name'].transform('count')\n\ndf_sets_with_missing_parts.head(5)\n```\n\n\n\n\n||searching_for_set|set_num|set_name|part_name|num_parts|matches_per_set|\n|-|-|-|-|-|-|-|\n|**0**|Heartlake Pizzeria|00-4|Weetabix Promotional Windmill|Slope 45° 2 x 2|126|1|\n|**1**|Heartlake Pizzeria|066-1|Basic Building Set|Slope 45° 2 x 2|407|1|\n|**2**|Heartlake Pizzeria|080-1|Basic Building Set with Train|Slope 45° 2 x 2|710|2|\n|**3**|Heartlake Pizzeria|088-1|Super Set|Slope 45° 2 x 2|615|1|\n|**4**|Heartlake Pizzeria|10021-1|U.S.S. Constellation|Slope 45° 2 x 2|974|1|\n\n\n\nLet's elaborate the magic that's happening.\n```python\ndf_sets_with_missing_parts.groupby(['set_num'])['part_name']\n```\nreturns a `GroupByDataFrame` which contains the group names (from `set_num`) and all row/column indicies and values related to the groups. Here only one column `['part_name']` is selected. In the next step [`transform` applies](https://github.com/pandas-dev/pandas/blob/v1.1.4/pandas/core/groupby/generic.py#L514) the given function (`count`) to each column individually but only with the values in the current group. Finaly the results are assigned to each row in the group as shown in Fig. 4.\n\n\n![](trnsf.png)\n\nFig. 4: Aggregation with transform\n\nNow that we have gathered all the data we arange the results so that they can be compared to the SQL data:\n\n\n```python\n# sort and pick top 16\ndf_sets_with_missing_parts = df_sets_with_missing_parts.sort_values(['matches_per_set', 'num_parts', 'set_num', 'part_name'], ascending = [False, True, True, True]).reset_index(drop = True).head(16)\n\ndf_sets_with_missing_parts\n```\n\n\n\n\n||searching_for_set|set_num|set_name|part_name|num_parts|matches_per_set|\n|-|-|-|-|-|-|-|\n|**0**|Heartlake Pizzeria|199-1|Scooter|Slope 45° 2 x 2|41|2|\n|**1**|Heartlake Pizzeria|199-1|Scooter|Slope 45° 2 x 2 Double Convex|41|2|\n|**2**|Heartlake Pizzeria|212-2|Scooter|Slope 45° 2 x 2|41|2|\n|**3**|Heartlake Pizzeria|212-2|Scooter|Slope 45° 2 x 2 Double Convex|41|2|\n|**4**|Heartlake Pizzeria|838-1|Red Roof Bricks Parts Pack, 45 Degree|Slope 45° 2 x 2|58|2|\n|**5**|Heartlake Pizzeria|838-1|Red Roof Bricks Parts Pack, 45 Degree|Slope 45° 2 x 2 Double Convex|58|2|\n|**6**|Heartlake Pizzeria|5151-1|Roof Bricks, Red, 45 Degrees|Slope 45° 2 x 2|59|2|\n|**7**|Heartlake Pizzeria|5151-1|Roof Bricks, Red, 45 Degrees|Slope 45° 2 x 2 Double Convex|59|2|\n|**8**|Heartlake Pizzeria|811-1|Red Roof Bricks, Steep Pitch|Slope 45° 2 x 2|59|2|\n|**9**|Heartlake Pizzeria|811-1|Red Roof Bricks, Steep Pitch|Slope 45° 2 x 2 Double Convex|59|2|\n|**10**|Heartlake Pizzeria|663-1|Hovercraft|Slope 45° 2 x 2|60|2|\n|**11**|Heartlake Pizzeria|663-1|Hovercraft|Slope 45° 2 x 2 Double Convex|60|2|\n|**12**|Heartlake Pizzeria|336-1|Fire Engine|Slope 45° 2 x 2|76|2|\n|**13**|Heartlake Pizzeria|336-1|Fire Engine|Slope 45° 2 x 2 Double Convex|76|2|\n|**14**|Heartlake Pizzeria|6896-1|Celestial Forager|Slope 45° 2 x 2|92|2|\n|**15**|Heartlake Pizzeria|6896-1|Celestial Forager|Slope 45° 2 x 2 Double Convex|92|2|\n\n\n\n\n```python\n# assert equals\nif not USE_BIGQUERY:\n    sets_with_missing_parts = sets_with_missing_parts.DataFrame()\n    \npd._testing.assert_frame_equal(sets_with_missing_parts, df_sets_with_missing_parts)\n```\n\nThe results are matching!\n\n![](penguin.png): We got it. We can buy the small Fire Engine to fix the roof of the fireplace. Now need for a new Pizzeria. :-)\n\n![](chick.png): (#@§?!*#) Are you sure your data is usefull for anything?\n\n# Recursion *(Lost in trees?)*\nWe solved the red brick problem. But since we have the data already open, let's have a closer look at the *Fire Engine*, set number *336-1*.\n```sql\nSELECT s.name AS set_name,\n       s.year,\n       t.id,\n       t.name AS theme_name,\n       t.parent_id\n  FROM sets s,\n       themes t\n WHERE t.id = s.theme_id\n   AND s.set_num = '336-1';\n```    \n\n\n|set_name|year|id|theme_name|parent_id|\n|-|-|-|-|-|\n|Fire Engine|1968|376|Fire|373.0|\n\n\n\nThe *fire engine* is quiet old (from 1968) and it belongs to the theme *Fire*. \nThe `themes` table also includes as column called `parent_id`. This suggests that `themes` is a hierarchical structure.\nWe can check this with an recursive `WITH`-clause in SQL. (*BQ: recursive WITH is not implemented in BIGQUERY.\n```sql\nWITH RECURSIVE hier(name, parent_id, level) AS ( \n    \n    -- init recursion\n    SELECT t.name, \n           t.parent_id, \n           0 AS level \n      FROM themes t \n     WHERE id = 376\n    \n    UNION ALL\n    \n    -- recursive call\n    SELECT t.name, \n           t.parent_id, \n           h.level +1 \n      FROM themes t, \n           hier h \n     WHERE t.id = h.parent_id\n    \n)\nSELECT COUNT(1) OVER() - level AS level, \n       name as theme, \n       GROUP_CONCAT(name,' --> ') over(order by level desc) path\n  FROM hier \n ORDER BY level;\n```\n\n\n|level|theme|path|\n|-|-|-|\n|1|Classic|Classic|\n|2|Vehicle|Classic --&gt; Vehicle|\n|3|Fire|Classic --&gt; Vehicle --&gt; Fire|\n\n\n\nOK, that looks like a reasonable hierarchie. The `path` column includes the parents and grant parents of a theme.\nWhat if we want to reverse the order of the path. Unfortunately `GROUP_CONCAT` in SQLite doesn't allow us to specify a sort order in the aggregation.\nIt's possible to add custom aggregation function in some databases. In SQLite we can compile [application defined function](https://www.sqlite.org/appfunc.html) or in Oracle we can define [customized aggregation function](https://docs.oracle.com/cd/B28359_01/appdev.111/b28425/aggr_functions.htm) even at runtime as types.\n\nQuiet some steps need to be taken to make the database use costumized aggregation efficently, so we can use them like regulare aggregation and windowing function. In Oracle for instance we have to define:\n\n1. initial values: \n    ```plsql\n    total := 0; \n    n := 0;\n    ```\n2. calculation per iteration step: \n    ```plsql\n    total := total + this_step_value; \n    n := n + 1;\n    ```\n3. deletion per iteration for windowing: \n    ```plsql\n    total := total - removed_step_value; \n    n := n - 1;\n    ```\n4. merging for parallel execution:\n    ```\n    total := total_worker1 + total_worker2; \n    n := n_worker1 + n_worker2; \n    ```\n5. termination: \n    ```plsql\n    my_avg := total / nullif(n-1, 0)\n    ```\n\nNow, how are we gonna do this in Pandas? We start by traversing the hierarchie.\n\n\n```python\nfire_engine_info = df_themes[df_themes['id'] == 376].copy()\nfire_engine_info['level'] = 0\n\nparent_id = fire_engine_info.parent_id.values[0]\n\n\nlvl = 0\nwhile not np.isnan(parent_id) and lvl < 10:\n    lvl+= 1\n    new_info = df_themes[df_themes['id'] == parent_id].copy()\n    new_info['level'] = lvl\n    parent_id = new_info.parent_id.values[0]\n    fire_engine_info = fire_engine_info.append(new_info)\n\nfire_engine_info['grp']=0\nfire_engine_info\n```\n\n\n\n\n||id|name|parent_id|level|grp|\n|-|-|-|-|-|-|\n|**375**|376|Fire|373.0|0|0|\n|**372**|373|Vehicle|365.0|1|0|\n|**364**|365|Classic|NaN|2|0|\n\n\n\nOn the one hand this is pretty need, since we can do what ever we want in a manually coded loop. On the other hand I doubt that it is very efficent when we have to deal with lots of data. But to be fair, Recursive `WITH` isn't that fast either in SQL.\n\nFinaly we consider how to do customized aggregation. We could do it in the loop above or we can rather use the library's `transform` or `apply` functions.\n\nWe define a custom aggregation function `cat_sorted` and then use the `apply` function like this:\n\n\n```python\ndef cat_sorted(ser, df, val_col = None, sort_col = None):\n    u=' --> '.join(df[df.id.isin(ser)].sort_values(sort_col)[val_col].values)\n    return [u]\n```\n\n\n```python\nfire_engine_info.apply(lambda x: cat_sorted(x, fire_engine_info, 'name', 'level'))\n```\n\n\n\n\n||id|name|parent_id|level|grp|\n|-|-|-|-|-|-|\n|0|Fire --&gt; Vehicle --&gt; Classic||Vehicle --&gt; Classic|||\n\n\n\nWe can also apply rolling or windowing behaviour.\n> Note that a rolling representation or windowing on string values is not possible because Pandas only allows numeric values for those action.\n\n\n```python\nfire_engine_info.rolling(10,min_periods=1)['level'].apply(lambda x: sum(10**x), raw = False)\n```\n\n\n\n```bash\n    375      1.0\n    372     11.0\n    364    111.0\n    Name: level, dtype: float64\n```\n\n\nNow, we not only understand the numbers on the lego package but also have a better understandig of Pandas.\n\n# Summary *(Got it!)*\n\nSQL stays my favourite language to access structured data arranged over many tables. Pandas shines when data already is gathered together and easily accessable (e.g. as csv file).\nThere are alternatives to Pandas to build ml pipelines, such as [Dask](https://docs.dask.org/en/latest/) or [CUDF](https://docs.rapids.ai/api/cudf/stable/). But learning Pandas is a good foundation to learn more of them.\n\n# Resources \nTo play with the examples:\n- *Res. 1* Kaggle notebook: https://www.kaggle.com/joatom/a-handful-of-bricks-from-sql-to-pandas\n- *Res. 2* Docker container: https://github.com/joatom/blog-resources/tree/main/handful_bricks\n\n# References\n- *Ref. 1* Pandas SQL comparison: https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html\n- *Ref. 2* The Lego dataset: https://www.kaggle.com/rtatman/lego-database\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.256","bibliography":["../../references.bib"],"theme":{"dark":"darkly","light":"flatly"},"title-block-banner":true,"sidebar":false,"layout":"post","comments":{"utterances":{"repo":"joatom/blog"}},"date":"2020-12-12","description":"A step by step guide to learn Pandas from a SQL perspective.","categories":["SQL","Pandas","BigQuery","ML"],"image":"logo_sql2pandas.jpg","title":"A handful of bricks - from SQL to Pandas","author":"Johannes Tomasoni"},"extensions":{"book":{"multiFile":true}}}}}