{"title":"Notes about Sequence Modelling","markdown":{"yaml":{"title":"Notes about Sequence Modelling","description":"Predicting Ventilator Pressure Time Series","toc":true,"badges":false,"author":"Johannes Tomasoni","image":"logo_vpp_seq.png","categories":["Time Series","ML","Competition"],"date":"2021-11-22","from":"markdown+emoji","keep-ipynb":true,"format":{"html":{"page-layout":"full"}}},"headingText":"Notes about Sequence Modelling","containsRefs":false,"markdown":"\n\n\nI lately participated in the [Google Brain - Ventilator Pressure Prediction](https://www.kaggle.com/c/ventilator-pressure-prediction) competition. I didn't score decent, but I still learned a lot. And some of it is worth to sum up, so I can easily look it up later.\n\nThe goal of the competition was to predict airway pressure of lungs that are ventilated in a clinician-intensive procedure. Given values of the input pressure (`u_in`) we had to predict the output `pressure` for a time frame of a few seconds.\n\n\nSince all `u_in` values for a time frame were given we can build a bidirectional sequence model. Unless in a typical time-series problem where the future points are unknown at a certain time step, we know the future and past input values. Therefore I decided not to *mask* the sequences while training.\n\nA good model choice for sequencing tasks are LSTMs and Transformers. I built a model that combines both architectures. I also tried XGBoost with a lot of features (especially windowing, rolling, lead, lag features) engineering, But neural nets (NN) performed better, here. Though I kept some of the engineered features as embeddings for the NN model.\n\nThe competition metric was mean average error (MAE). Only those pressures were evaluated, that appear while filling the lungs with oxygen.\n\n\n## Feature engineering\n\nBesides the given features, *u_in*, *u_out*, *R*, *C* and *time_step* I defined several features. They can by categorized as:\n\n- area (accumulation of u_in over time) from [this notebook](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153)\n- one hot encoding of ventilator parameters R and C\n- statistical (mean, max, skewness, quartiles, rolling mean, ...)\n- shifted input pressure\n- input pressure performance over window\n- inverse features\n\nTo reduce memory consumption I used a function from [this notebook](https://www.kaggle.com/konradb/tabnet-end-to-end-starter).\n\n### Scaler\nThe data was transformed with *scikit's RobustScaler* to reduce influence of outliers.\n\n## Folds\nI didn't do cross validation here, but instead trained the final model on the entire dataset. Nevertheless it's helpful to build kfolds for model evaluation. I build GroupKFold over `breath_id` to keep the entire time frame in the same fold.\n\n## Dataloader\n\nSince the data is quite small (ca. 800 MB after memory reduction) I decided to load the entire train set in the `Dataset` object during construction (calling `__init__()`). In a first attempt I loaded the data as *Pandas Dataframe*. Then I figured out (from [this notebook](https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization)) that converting the Dataframe into an numpy array speeds up training significantly. The Dataframe is converted to an numpy array by the scaler.\n\nSince the competition metric only evaluates the pressures where `u_out==0` I also provide a mask tensor, which can later on be used feeding the loss and metric functions.\n\n## Model\n\nMy model combines a multi layered LSTM and a Transformer Encoder. Additionally I build an AutoEncoder by placing a Transformer Decoder on top of the Transformer encoder. The AutoEncoder predictions are used as auxiliary variables.\n\n![](logo_vpp_seq.png)\n\nSome further considerations:\n\n- I did not use drop out. The reason why it performence worse is discussed [here](https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/276719).\n- LayerNorm can be used in sequential models but didn't improve my score.\n\nThe model is influenced by these notebooks:\n\n- [Transformer part](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n- [LSTM part](https://www.kaggle.com/theoviel/deep-learning-starter-simple-lstm)\n- [Parameter initialization](https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization)\n\n## Metric and Loss Function\n\n### Masked MAE metric\nThe competition metric was Mean Absolute Error (MAE), but only for the time-steps where air flows into the lunge (approx. half of the timesteps).\nHence, I masked the predictions (using the flag introduced in the Dataset) ignoring the unnecessary time-steps. The flags are passed through the model (`val[1]`) and is an output along with the predictions.\n\nThee values produced by the AutoGenerater are additionally measured by the `vppGenMetric`. It uses MAE to evaluate how good the reconstruction of the input features values evolves.\n\n## Combined Loss function\n\nThe loss function is a combination of L1-derived-Loss (`vppAutoLoss`) for the predictions and the AutoEncoder-predictions.\n\nDue to [this discussion](https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/277690) I did some experiments with variations of Huber and SmoothL1Loss. The later (`vppAutoSmoothL1Loss`) performed better.\n\n# Training\n\nThe training was done in mixed precision mode (`to_fp16()`) to speed up training. As optimizer I used *QHAdam*.\nThe best single score I achieved with a 100 epoch `fit_one_cycle` (*CosineAnnealing* with warmup). I also tried more epochs with restart schedules `fit_sgdr` and changing loss functions. But the didn't do better.\n\nTh CV score after 100 epochs is 0.227013 for vppMetric and 0.255794 for vppGenMetric. Leaderboard scores for this singele model training in the entire train-dataset were 0.2090 private LB and 0.2091 public LB.\n\n# References\n- Competition Home Page: https://www.kaggle.com/c/ventilator-pressure-prediction\n- Area features: https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153\n- Memory reduction: https://www.kaggle.com/konradb/tabnet-end-to-end-starter\n- Dataloader from numpy: https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization\n- Dropout discussion: https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/276719\n- Transformer model: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n- LSTM model: https://www.kaggle.com/theoviel/deep-learning-starter-simple-lstm\n- LSTM parameter initialization: https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization\n- SmoothL1Loss discussion: https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/277690\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":true,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"from":"markdown+emoji","output-file":"2021-11-22-vpp-seq.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.256","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":true,"title":"Notes about Sequence Modelling","description":"Predicting Ventilator Pressure Time Series","badges":false,"author":"Johannes Tomasoni","image":"logo_vpp_seq.png","categories":["Time Series","ML","Competition"],"date":"2021-11-22","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}