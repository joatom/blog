[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there!\nI‚Äôm Johannes and this is my hobby blog. The content is limited to topics I‚Äôm curious in my spare time. Don‚Äôt expect to find here anything related to my job.\nI‚Äôm happy when you leave a comment on blog posts you like.\nCheers,\nJohannes"
  },
  {
    "objectID": "posts/2022-01-31-learn_along/index.html",
    "href": "posts/2022-01-31-learn_along/index.html",
    "title": "Learning along",
    "section": "",
    "text": "It‚Äôs said, that the most effective way to learn on kaggle is to participate in a competition and afterwards wrangling the top solutions. At the current state (as of January 2022) of my ML skills this approach doesn‚Äôt work well for me. 1. Top notebook solutions are often complicated, contain implementations that are difficult to grasp or are made out of tons of ensembled models. Solutions in the discussion section contain broad overviews that are inspiring, yet hard to rebuild. 2. The solutions are often different from my own solution. It is difficult to compare them with my own code and to derive how to improve my own code specifically.\nThe second best way to learn on kaggle is to follow the discussions and read the notebooks during a competition. This approach suites me better, since there are often small insights to be discovered that are shared by others and that I can easily integrated in my solution. Filtering the valuable information can be hard, because the notebook and discussion sections are often flooded with similar content. You can easily get distracted by too many different techniques and ideas.\nI often enjoy the Playground competitions to focus on a few skills to improve. They are also less challenging and it is easier to get a baseline implementation up running because the data is already prepared to quickly get started.\nIn January 2022 there is a community competition hosted by Abhishek Thakur, that provides the opportunity for yet another learning approach. During the competition there are sessions being recoreded on YouTube, with two Grandmasters covering the topics EDA and Imputation. So we get high quality guidance for two important topics while working on the competition. I took the opportunity to learn along.\nI wrote down my experience with this learning approach and shared it with my solution in the following notebook."
  },
  {
    "objectID": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html",
    "href": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html",
    "title": "A handful of bricks - from SQL to Pandas",
    "section": "",
    "text": "Original article published at datamuni.com."
  },
  {
    "objectID": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#conditional-join",
    "href": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#conditional-join",
    "title": "A handful of bricks - from SQL to Pandas",
    "section": "Conditional Join",
    "text": "Conditional Join\nThere is no intuitive way to do a conditional join on DataFrames. The easiest I‚Äôve seen so far is a two step solution. As substitution for the SQL WITH-clause we can reuse df_missing_parts.\n# 1. merge on the equal conditions\ndf_sets_with_missing_parts = df_inventory_list.merge(df_missing_parts, how = 'inner', on = ['part_num', 'color'], suffixes = ('_found', '_missing'))\n# 2. apply filter for the qreater equals condition\ndf_sets_with_missing_parts = df_sets_with_missing_parts[df_sets_with_missing_parts['quantity_found'] >= df_sets_with_missing_parts['quantity_missing']]\n\n# select columns\ncols = ['set_num', 'set_name', 'part_name', 'num_parts']\ndf_sets_with_missing_parts = df_sets_with_missing_parts[['set_name_missing'] + [c + '_found' for c in cols]]\ndf_sets_with_missing_parts.columns = ['searching_for_set'] + cols"
  },
  {
    "objectID": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#aggregation",
    "href": "posts/2020-12-12-a-handful-of-bricks-from-sql-to-pandas/index.html#aggregation",
    "title": "A handful of bricks - from SQL to Pandas",
    "section": "Aggregation",
    "text": "Aggregation\nIn the next step the aggregation of the analytic function\nCOUNT(*) OVER (PARTITION BY il.set_num) matches_per_set\nneeds to be calculated. Hence the number of not-NaN values will be counted per SET_NUM group and assigned to each row in a new column (matches_per_set).\nBut before translating the analytic function, let‚Äôs have a look at a regular aggregation, first. Say, we simply want to count the entries per set_num on group level (without assigning the results back to the original group entries) and also sum up all parts of a group. Then the SQL would look something like this:\nSELECT s.set_num,\n       COUNT(*) AS matches_per_set\n       SUM(s.num_parts) AS total_num_parts\n  FROM ...\n WHERE ...\n GROUP BY \n       s.set_num;\nAll selected columns must either be aggregated by a function (COUNT, SUM) or defined as a group (GROUP BY). The result is a two column list with the group set_num and the aggregations matches_per_set and total_num_part.\nNow see how the counting is done with Pandas.\ndf_sets_with_missing_parts.groupby(['set_num']).count()  .sort_values('set_num', ascending = False)\n\n# for sum and count:\n# df_sets_with_missing_parts.groupby(['set_num']).agg(['count', 'sum']) \n\n\n\n\nsearching_for_set\nset_name\npart_name\nnum_parts\n\n\n\n\nset_num\n\n\n\n\n\n\nllca8-1\n1\n1\n1\n1\n\n\nllca21-1\n1\n1\n1\n1\n\n\nfruit1-1\n1\n1\n1\n1\n\n\nMMMB026-1\n1\n1\n1\n1\n\n\nMMMB003-1\n1\n1\n1\n1\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n10021-1\n1\n1\n1\n1\n\n\n088-1\n1\n1\n1\n1\n\n\n080-1\n2\n2\n2\n2\n\n\n066-1\n1\n1\n1\n1\n\n\n00-4\n1\n1\n1\n1\n\n\n\nWow, that‚Äôs different! The aggregation function is applied to every column independently and the group is set as row index. But it is also possible to define the aggregation function for each column explicitly like in SQL:\ndf_sets_with_missing_parts.groupby(['set_num'], as_index = False) \\\n    .agg(matches_per_set = pd.NamedAgg(column = \"set_num\", aggfunc = \"count\"), \n         total_num_parts = pd.NamedAgg(column = \"num_parts\", aggfunc = \"sum\"))\n\n\n\n\nset_num\nmatches_per_set\ntotal_num_parts\n\n\n\n\n0\n00-4\n1\n126\n\n\n1\n066-1\n1\n407\n\n\n2\n080-1\n2\n1420\n\n\n3\n088-1\n1\n615\n\n\n4\n10021-1\n1\n974\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n463\nMMMB003-1\n1\n15\n\n\n464\nMMMB026-1\n1\n43\n\n\n465\nfruit1-1\n1\n8\n\n\n466\nllca21-1\n1\n42\n\n\n467\nllca8-1\n1\n58\n\n\n\nThis looks more familiar. With the as_index argument the group becomes a column (rather than a row index).\nSo, now we return to our initial task translating the COUNT(*) OVER(PARTITION BY) clause. One approach could be to join the results of the above aggregated DataFrame with the origanal DataFrame, like\ndf_sets_with_missing_parts.merge(my_agg_df, on = 'set_num')\nA more common why is to use the transform() function:\n# add aggregatiom\ndf_sets_with_missing_parts['matches_per_set'] = df_sets_with_missing_parts.groupby(['set_num'])['part_name'].transform('count')\n\ndf_sets_with_missing_parts.head(5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nsearching_for_set\nset_num\nset_name\npart_name\nnum_parts\nmatches_per_set\n\n\n\n\n0\nHeartlake Pizzeria\n00-4\nWeetabix Promotional Windmill\nSlope 45¬∞ 2 x 2\n126\n1\n\n\n1\nHeartlake Pizzeria\n066-1\nBasic Building Set\nSlope 45¬∞ 2 x 2\n407\n1\n\n\n2\nHeartlake Pizzeria\n080-1\nBasic Building Set with Train\nSlope 45¬∞ 2 x 2\n710\n2\n\n\n3\nHeartlake Pizzeria\n088-1\nSuper Set\nSlope 45¬∞ 2 x 2\n615\n1\n\n\n4\nHeartlake Pizzeria\n10021-1\nU.S.S. Constellation\nSlope 45¬∞ 2 x 2\n974\n1\n\n\n\nLet‚Äôs elaborate the magic that‚Äôs happening.\ndf_sets_with_missing_parts.groupby(['set_num'])['part_name']\nreturns a GroupByDataFrame which contains the group names (from set_num) and all row/column indicies and values related to the groups. Here only one column ['part_name'] is selected. In the next step transform applies the given function (count) to each column individually but only with the values in the current group. Finaly the results are assigned to each row in the group as shown in Fig. 4.\n\nFig. 4: Aggregation with transform\nNow that we have gathered all the data we arange the results so that they can be compared to the SQL data:\n# sort and pick top 16\ndf_sets_with_missing_parts = df_sets_with_missing_parts.sort_values(['matches_per_set', 'num_parts', 'set_num', 'part_name'], ascending = [False, True, True, True]).reset_index(drop = True).head(16)\n\ndf_sets_with_missing_parts\n\n\n\n\n\n\n\n\n\n\n\n\n\nsearching_for_set\nset_num\nset_name\npart_name\nnum_parts\nmatches_per_set\n\n\n\n\n0\nHeartlake Pizzeria\n199-1\nScooter\nSlope 45¬∞ 2 x 2\n41\n2\n\n\n1\nHeartlake Pizzeria\n199-1\nScooter\nSlope 45¬∞ 2 x 2 Double Convex\n41\n2\n\n\n2\nHeartlake Pizzeria\n212-2\nScooter\nSlope 45¬∞ 2 x 2\n41\n2\n\n\n3\nHeartlake Pizzeria\n212-2\nScooter\nSlope 45¬∞ 2 x 2 Double Convex\n41\n2\n\n\n4\nHeartlake Pizzeria\n838-1\nRed Roof Bricks Parts Pack, 45 Degree\nSlope 45¬∞ 2 x 2\n58\n2\n\n\n5\nHeartlake Pizzeria\n838-1\nRed Roof Bricks Parts Pack, 45 Degree\nSlope 45¬∞ 2 x 2 Double Convex\n58\n2\n\n\n6\nHeartlake Pizzeria\n5151-1\nRoof Bricks, Red, 45 Degrees\nSlope 45¬∞ 2 x 2\n59\n2\n\n\n7\nHeartlake Pizzeria\n5151-1\nRoof Bricks, Red, 45 Degrees\nSlope 45¬∞ 2 x 2 Double Convex\n59\n2\n\n\n8\nHeartlake Pizzeria\n811-1\nRed Roof Bricks, Steep Pitch\nSlope 45¬∞ 2 x 2\n59\n2\n\n\n9\nHeartlake Pizzeria\n811-1\nRed Roof Bricks, Steep Pitch\nSlope 45¬∞ 2 x 2 Double Convex\n59\n2\n\n\n10\nHeartlake Pizzeria\n663-1\nHovercraft\nSlope 45¬∞ 2 x 2\n60\n2\n\n\n11\nHeartlake Pizzeria\n663-1\nHovercraft\nSlope 45¬∞ 2 x 2 Double Convex\n60\n2\n\n\n12\nHeartlake Pizzeria\n336-1\nFire Engine\nSlope 45¬∞ 2 x 2\n76\n2\n\n\n13\nHeartlake Pizzeria\n336-1\nFire Engine\nSlope 45¬∞ 2 x 2 Double Convex\n76\n2\n\n\n14\nHeartlake Pizzeria\n6896-1\nCelestial Forager\nSlope 45¬∞ 2 x 2\n92\n2\n\n\n15\nHeartlake Pizzeria\n6896-1\nCelestial Forager\nSlope 45¬∞ 2 x 2 Double Convex\n92\n2\n\n\n\n# assert equals\nif not USE_BIGQUERY:\n    sets_with_missing_parts = sets_with_missing_parts.DataFrame()\n    \npd._testing.assert_frame_equal(sets_with_missing_parts, df_sets_with_missing_parts)\nThe results are matching!\n: We got it. We can buy the small Fire Engine to fix the roof of the fireplace. Now need for a new Pizzeria. :-)\n: (#@¬ß?!*#) Are you sure your data is usefull for anything?"
  },
  {
    "objectID": "posts/2022-08-30-art_diffusion/index.html",
    "href": "posts/2022-08-30-art_diffusion/index.html",
    "title": "Art and Diffusion",
    "section": "",
    "text": "Human-machine interaction\nIn summer 2016 I spent a day with my brother at his studio in the Monastery of Bentlage to try out fine art printing. The monastery is in the countryside and is a very calming place to escape from everyday life. As preparation for the workshop my brother asked me to prepare a drawing. I‚Äôm not a good drawer, my hands are shaky and whatever comes out of the pencil never looks like what I intended to draw. Luckily I was experimenting with edge-detecting algorithms at that time. The algorithm I was implementing was suitable to create an image that looked like a drawing. I took a picture of my son where he was playing with the vacuum robot and run my program to extract the contours of my son and the vacuum robot. The setting where I generate a drawing with an algorithm and the theme of my son playing with a robot was very appealing. In the workshop we used the generated drawing as template for my artwork. The printing technique we applied to create the artwork is called etching. The classical look and feel of this centuries old technique was a nice contrast to the rather modern theme - a kid playing with a smart robot. Yet, it made the interaction between humans and robots look pretty normal. And I was proud of my amateurish accomplishments.\n\nA few weeks later I was attending a TEDx event in M√ºnster. One of the talks was given by the artist Roman Lipski and the art collective YQP. Surprisingly, they introduced their concept of art where the artist interacts with an Artificial Muse. The Muse (an ML algorithm) learned to imitate Lipski‚Äôs style of painting by ‚Äúanalysing‚Äù his artworks. The painter than used the generated images as inspiration (Muse) to paint new pictures on canvas. This was in 2016 shortly after google shared their Deep Dream accomplishments and Style Transfer was a hot topic in the tech community. Lipski and his team envisioned the natural interaction between AI and humans to create art. In 2016 I was not practicing ML yet, nor was I heavily involved into art, apart from seeing my brother from time to time. But after this TEDx talk it was obvious that computer generated images will have a high impact on arts in the future. But will it be interruptive in the sense that artists become obsolete?\n\n\nWhat is art? Or, why is art?\nThere are countless definitions and strong opinions about What is Art? and what not. Let‚Äôs ignore this question for now and rather focus on another question. Why does Art exists? As I said, I‚Äôm not an art professional, but a few answers are obvious to me.\nArt exists because of,\n\nthe human nature:\n\npeople love to be creative,\npeople love beauty,\npeople use different forms to express feelings,\npeople love stories and imagination\n\npolitical and sociological activities:\n\npeople, who are suppressed, can hide critics in art,\npeople use art to defame opponents,\npeople can visualize opinions ‚Ä¶\n\n\nThere certainly are many more categories and reasons why art exists. However, a few things stand out.\n\nArt is a highly influential medium to form opinions using the imaginary power of the viewer. To the point where the viewer‚Äôs imagination can go way beyond the obvious object. For example, if you look at a picture with a house in a landscape and the rich and shiny oil colors, your imagination can take you from the museum right to Tuscany.\nPeople also practice art for their own sake.\n\nSo why do we get confused about the question What is art? Some of the techniques that are used in art are also used for other purpose. Let‚Äôs take drawing as an example. An architect uses drawing technics to plot the layout of a house. The plot may including information about wall measures, connection for water and electricity, etc. Usually you wouldn‚Äôt consider that kind of plot as a piece of art, though it still requires a bit of imagination from the viewer. The technical drawing must be easy accessible for the interpreter and thus provides the obvious information (e.g.¬†measures of the wall in centimeters). However, sometimes a technical drawing can diffuses into art. although it still serves the original purpose. A technical drawing can be of such of an elegance that it my raise strong feelings in the viewer - or the drawer. Than it becomes art. Think of Christo‚Äôs drawings. Don‚Äôt you feel joy or fascination when you are looking at them?\nBack to the question, will advancements in AI/ML disrupt and takeover Art? No! Every argument I used to describe the purpose of art involves humans at some point. There is no perception of beauty or expression of feelings without humans nor is there any political or social implications without humans. Will AI/ML heavily influence the arts and creativity? Yes! We will gain an impressive tool set and new techniques. And we can see that Computers are already being creative in certain environments (e.g.¬†generative ML-models).\n\n\nDiffusion\nComputers are magical and support you being creative. I remember in the 90‚Äôs when we used to create flyers for our parties. The visual effects of the graphic programs were so fascinating that we had to use almost all of them. The flyers always ended up looking like invitations to psychedelic 70‚Äôs parties rather then to underground techno parties. Some 25 years later the algorithms are getting very sophisticated. Images can be created out of noise or text. Existing images can be transformed to adapt the style of another painter or to look totally different. At the time of writing this article Diffusion Models are trending. Their strength lies in modification of existing images by adding noise to the image and then transforming the image while reducing the added noise again. The models can be combined with text encoders. You can basically tell the algorithm how you want a given image to change and how you want it to look like.\nThis summer my brother called me because someone dropped out of a workshop he was offering. And if I wanted to join spontaneously. I signed up for the two day offside and because I missed the first day of the workshop I needed to come up with a good template quickly. I decided to use one of the publicly available Diffusion Models to get started. I picked Disco Diffusion, where a handy tutorial was available. I added one of my favorite photos of the kids to the model and as text input I asked the model to change the picture into something like ‚ÄúThree children waiting for a bus in a photorealistic cyberpunk town. Red, blue, black, dark cyberpunk scheme.‚Äù. I did a bit of parameter tuning and after two hours I had a few templates to work with on the workshop.\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal photo (censored)\n\n\nCombination of two generated images\n\n\n\n\nOn this workshop we applied screen printing technique and due to time restrictions we were limited to two (color) layers. I used the original photo and the generated images as inspiration for the print. I wouldn‚Äôt call it Muse in my case because it used the images as template with a few abstractions. But if I‚Äôd be a professional artist I would definitely use the algorithms somehow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor me as an amateur the fulfillment lays rather in the relaxing hours, escaping daily routines, interacting with people, while handcrafting some little artwork to use as Christmas card or to hang on my walls at home.\n\n\n\nResources\n\nHomepage Maximilian Tomasoni: http://www.maximilian-tomasoni.com/\nHomepage Monastery of Bentlage: https://www.kloster-bentlage.de/en/kunst-kultur-en/bentlage-print-society\nEdge-detecting algo for workshop in 2016: https://github.com/joatom/ART-playing-with-rob\nHow Art Meets Artificial Intelligence, YQP and Roman Lipski, TEDxM√ºnster, 2016. https://www.youtube.com/watch?v=oVE5rRJa0D8.\n‚ÄúInceptionism: Going Deeper into Neural Networks.‚Äù Accessed August 29, 2022. http://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html.\nGatys, Leon A., Alexander S. Ecker, and Matthias Bethge. ‚ÄúA Neural Algorithm of Artistic Style.‚Äù arXiv, September 2, 2015. https://doi.org/10.48550/arXiv.1508.06576.\nHo, Jonathan, Ajay Jain, and Pieter Abbeel. ‚ÄúDenoising Diffusion Probabilistic Models.‚Äù arXiv, December 16, 2020. https://doi.org/10.48550/arXiv.2006.11239.\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. ‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models.‚Äù arXiv, April 13, 2022. https://doi.org/10.48550/arXiv.2112.10752.\nQuick Start on Using AI to Render Images Using Disco Diffusion, 2022. https://www.youtube.com/watch?v=wIw59kAU6u8.\nhttps://github.com/alembics/disco-diffusion, and Katherine Crowson. ‚ÄúDisco Diffusion v5.61 - Now with Portrait_generator_v001,‚Äù n.d. https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb."
  },
  {
    "objectID": "posts/2021-04-14-nonlinearity/index.html",
    "href": "posts/2021-04-14-nonlinearity/index.html",
    "title": "Bending the space with nonlinearity",
    "section": "",
    "text": "I‚Äôm trying out a new kaggle feature which allows us to include kaggle notebooks in our personal blogs. This articel was initialy published as kaggle notebook in November 2020."
  },
  {
    "objectID": "posts/2021-05-28-hotel/index.html",
    "href": "posts/2021-05-28-hotel/index.html",
    "title": "Identifying Hotels",
    "section": "",
    "text": "‚ÄúRecognizing a hotel from an image of a hotel room is important for human trafficking investigations. Images directly link victims to places and can help verify where victims have been trafficked.‚Äù (Stylianou et al., 2019).\n\nAs part of the Eight Workshop on Fine-Grained Visual Categorization a kaggle competition was launched to support investigations by advancing models to identify hotels from images.\nThis post contains some parts of my contribution to the Hotel-ID to Combat Human Trafficking 2021 - FGVC8 kaggle competition.\n\nThe challenge\nThe competition contained 97000+ images of hotel rooms from 7700! different hotels around the world. The objective was to identify the hotels of 13000 images from the hidden test set. The metric of the competition was Mean Average Precision of the top 5 picks (MAP@5). My solution scored 14th place out of 92 teams with a 0.6164 MAP@5 on the private leaderboard.\nMy solution contained six CNN models with various configurations. More technical details and why I ended up with rather simple models is described in a kaggle discussion topic.\nHere I post the training and inference of one of the six models as well as the ensemble inference code.\n\n\nTraining\nThe final training was done on the entire train dataset. I didn‚Äôt choose a cross validation strategy to safe training time. To keep variance low nonetheless I relied on the usual regularization strategies, such as dropout and augmentation and in particular on test time augmentation during inference.\nTo refine the model a validation set can be created by setting the debug flag as described in the notebook. \n\n\nInference\n\n\n\n\nInference ensemble\n\n\n\n\nReferences\nStylianou, Abby and Xuan, Hong and Shende, Maya and Brandt, Jonathan and Souvenir, Richard and Pless, Robert (2019). Hotels-50K: A Global Hotel Recognition Dataset. The AAAI Conference on Artificial Intelligence (AAAI)"
  },
  {
    "objectID": "posts/2020-12-26-blog-translator/index.html",
    "href": "posts/2020-12-26-blog-translator/index.html",
    "title": "Automatically translate blog posts",
    "section": "",
    "text": "üá©üá™ üá∫üá∏\n\nAttention! This text has been automatically translated!\n\nSince I made so many mistakes in my first Blog post, I write this post in German and have it automatically translated.\nFor translation I use the popular NLP framework of huggingface.co. On their website is a simple example to implement a translation application and I will use it.\nAs expected, the Markdown syntax does not immediately work correctly when translating. So I had to make some adjustments at the beginning and afterwards.\nThe code (including pre- and post-processing) I used for the translation of the markdown files can be found here. But since it‚Äôs just a few lines of code, we can also look at it here:\nfrom transformers import MarianMTModel, MarianTokenizer \n \n# load pretrained model and tokenizer \nmodel_name = 'Helsinki-NLP/opus-mt-de-en' \ntokenizer = MarianTokenizer.from_pretrained(model_name) \nmodel = MarianMTModel.from_pretrained(model_name) \n \n# load german block post \nf_in = open(\"blog_translator_de.md\", \"r\") \nsrc_text = f_in.readlines() \nf_in.close() \n \n# preprocessing \n## line break (\\n) results to \"I don't know.\"  We make it more specific: \nsrc_text = [s.replace('\\n',' ') for s in src_text] \n \n## remove code block \ncode = [] \ninside_code_block = False \nfor i, line in enumerate(src_text): \n    if line.startswith('```') and not inside_code_block: \n        # entering codeblock \n        inside_code_block = True \n        code += [line] \n        src_text[i] = '<<code_block>>' \n    elif inside_code_block and not line.startswith('```'): \n        code += [line] \n        src_text[i] = '<<code_block>>' \n    elif inside_code_block and line.startswith('```'): \n        # leaving code block \n        code += [line] \n        src_text[i] = '<<code_block>>' \n        inside_code_block = False \n \n# translate \ntranslated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\")) \ntgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] \n \n# postprocessing \n## replace code_blog tags with code \nfor i, line in enumerate(tgt_text): \n    if line == '<<code_block>>': \n        tgt_text[i] = code.pop(0) \n \n## remove the eol (but keep empty list entries / lines) \ntgt_text = [s.replace('', '',) for s in tgt_text] \n## remove space between ]( to get the md link syntax right \ntgt_text = [s.replace('](', '](',) for s in tgt_text] \n \n# write english blog post \nwith open('2020-12-26-blog-translator.md', 'w') as f_out: \n    for line in tgt_text: \n        f_out.write(\"%s\\n\" % line) \nf_out.close() \nSince this is my first NLP application, I left it with this Hello World code. Surely there are clever ways to map the markdown syntax in tokenizer. Maybe I‚Äôll write a follow up when I find out.\nBy the way, the translation just made me adapt my German writing style. For example, sarcasm doesn‚Äôt work so well after translation, so I avoided it. Also, it often depends on the correct choice of words (e.g.¬†there is no markdown command, but there is markdown syntax). <\nBest regards\nJohannes & the Robot"
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#who-are-we",
    "href": "posts/2022-04-28-ml-pm/index.html#who-are-we",
    "title": "Tackling Projects like a ML Challenge",
    "section": "Who are we?",
    "text": "Who are we?\nThe dynamics of a team can be described in Tuckman‚Äôs [BT65] four stages,\n\nforming, where group members try ‚Äúto identify the boundaries of both interpersonal and task behaviors‚Äù,\nstorming, where interpersonal conflicts arise, hence everyone searches for her/his role in the team,\nnorming, where everyone settles for her/his role in the team and new group ‚Äùstandards evolve‚Äù,\nperforming, where finally ‚Äùthe group energy is channeled into the task‚Äù and the group works efficiently.\n\nBecause everyone in our team was short in time we tried to skip the Storming and Norming stage and go straight to Performing. But Tuckman‚Äôs stages seem the work like physical laws, a group can‚Äôt circumvent certain stages. Even if formal hierarchies already exist, humans need to build trust first and find their role in the team. This is only possible through human interaction, which takes time. Though, the Storming and Norming phase can be very short if you take on a very standardized role to execute very standardized work. Which wasn‚Äôt the case in our small creative (non standardized) project."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#who-is-it-for",
    "href": "posts/2022-04-28-ml-pm/index.html#who-is-it-for",
    "title": "Tackling Projects like a ML Challenge",
    "section": "Who is it for?",
    "text": "Who is it for?\nWe all had a common sense of the technical topic we were supposed to cover. But the field of the technical topic is huge, with all levels of difficulties. At this point there also was only a vague definition of the audience of the curriculum."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#how-are-we-gonna-make-it",
    "href": "posts/2022-04-28-ml-pm/index.html#how-are-we-gonna-make-it",
    "title": "Tackling Projects like a ML Challenge",
    "section": "How are we gonna make it?",
    "text": "How are we gonna make it?\nWe also didn‚Äôt talk in advance what steps we would take to reach our goal. Hence, we didn‚Äôt set a framework.\nThe overall situation could basically be summarized as: > A bunch of ambitious strangers from different cultural background were short in time and wanted to solve a vaguely defined task without a framework."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#obstacle-1-being-short-on-time",
    "href": "posts/2022-04-28-ml-pm/index.html#obstacle-1-being-short-on-time",
    "title": "Tackling Projects like a ML Challenge",
    "section": "Obstacle 1: being short on time",
    "text": "Obstacle 1: being short on time\nThe obvious adjustment would have been to take more time for the task at hand. Unfortunately this was not the preferred option in this case, because we volunteered for a side project that shouldn‚Äôt badly effect the regular work we had to do. But we could save some time by avoiding ineffective meetings, hence being well prepared when attending a meeting. ## Obstacle 2: working in a new team The time restriction was hard, so we sacrificed team building. That‚Äôs always a huge risk and relies on the hope that everyone acts professionally and has the same intrinsic motivation. Luckily, it worked out well. ## Obstacle 3: vague goals We had to define the goals more precisely. ## Obstacle 4: missing framework We had to define a structure on how to approach the task. Given the time limit and the interpersonal constraints this was the part with the highest chance to make a positive impact on the outcome of our project. And that‚Äôs what the rest of this blog will focus on."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#state-your-overall-goal",
    "href": "posts/2022-04-28-ml-pm/index.html#state-your-overall-goal",
    "title": "Tackling Projects like a ML Challenge",
    "section": "1. State your overall goal üñºÔ∏è",
    "text": "1. State your overall goal üñºÔ∏è\nWrite the foremost goal down in huge letters and pin it on the wall. A classifier to predict flowers. Every decision you make from now on must follow this goal. If you lose yourself in details during the process you can always refocus by looking at the wall reading the huge letters stating your overall goal. ## 2. Re/define the objectives üñåÔ∏è Figure out what is important and what not. Define the objectives precisely (e.g.¬†through User Stories). ‚ÄùAs a cook I want the classifier to recognize all herbs in my garden, so that I don‚Äôt use the poisoning ones in my meals.‚Äù ## 3. Look at the data üîé Check the quality and structure of the data. Do exploratory data analysis (EDA). ‚ÄùSome of the pictures of the flowers and herbs are zoomed in, some are in panorama view and show many flowers. Classes are balanced.‚Äù ## 4. Define metrics üéØ Decide what your algorithm should optimize for. The cook will zoom in closely the take the picture. And since the training targets are well balanced, we choose Accuracy as metric. ## 5. Define test strategy üìã With the knowledge from EDA we decide on a test strategy and identify what is missing to implement the test strategy. ‚ÄúA stratified-KFold-split makes sense, but the labels still need some cleanup.‚Äù ## 6. Define architecture üèóÔ∏è We always want to start with a simple baseline in the first iteration. Later-on the architecture can get more complex, e.g.¬†through ensembling. Let‚Äôs start with a Resnet18. ## 7. Prepare data üë∑ Clean and enrich the data. There is enough data. Remove the panorama shots. Clean the labels. Define the train/test/validation data sets. ## 8. Execute ‚ñ∂Ô∏è Build a model and train. ## 9. Evaluate üìä Evaluate the model against the metrics. Accuracy is only 80%. Let‚Äôs look at the most miss classified data and try to change the architecture in the next iteration.\nI follow this procedure when I‚Äôm participating in Kaggle competition almost every time and I‚Äôm sure others follow similar approaches. Depending on how well prepared the data already is, some steps may be executed in slightly different order, e.g.¬†architecture choice can also happen after data preparation. But for sure you want to have a clear picture from the very beginning for what to optimize for and how you evaluate it."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-the-overall-goal",
    "href": "posts/2022-04-28-ml-pm/index.html#define-the-overall-goal",
    "title": "Tackling Projects like a ML Challenge",
    "section": "1. Define the overall goal üñºÔ∏è",
    "text": "1. Define the overall goal üñºÔ∏è\nWe want to collect learning material so that people with different skill-sets and backgrounds can learn about the technical topic to use on in their jobs."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#redefine-the-objectives",
    "href": "posts/2022-04-28-ml-pm/index.html#redefine-the-objectives",
    "title": "Tackling Projects like a ML Challenge",
    "section": "2. Re/define the objectives üñåÔ∏è",
    "text": "2. Re/define the objectives üñåÔ∏è\nThe audience consists of highly educated professionals, that need the technical knowledge for different reasons. Some need a deep technical understanding to use it as a Software Developer, some need a broad overview for their role as Project Manager and some just want to learn about the topic out of curiosity. Because of the heterogeneity of the audience I found it suitable to define different Personas. E.g. Andrea, Project Manager, doesn‚Äôt like to be fooled by the nerds during the meetings when they talk in their jargon. or Peter, Java evangelist, curious about that hot topic everybody is talking about. ‚ÄúCan‚Äôt be that hard, to just learn another language.‚Äù. With the personas in place we created categories where we could instantly assign the resources we already had collected. The human characteristics of the personas also lead to an imagination where the collected resources eventually fit to the the stories of the Personas we had in our minds.\nIn a later iteration we refined the objectives and added two other kind of categories. The one kind of category was concerning the knowledge depth (beginner, intermediate and advanced). The other kind of category distinguished between a business role and a tech role."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#look-at-the-data",
    "href": "posts/2022-04-28-ml-pm/index.html#look-at-the-data",
    "title": "Tackling Projects like a ML Challenge",
    "section": "3. Look at the data üîé",
    "text": "3. Look at the data üîé\nSince we were limited on time we couldn‚Äôt do a deep analysis of every resource. If resources where known in advance by one of the team members we had to rely on her/his judgment. If resources were unknown we briefly scanned them or relied on public recommendations. It‚Äôs fare from optimal, but it must have been sufficient for a first iteration."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-metrics",
    "href": "posts/2022-04-28-ml-pm/index.html#define-metrics",
    "title": "Tackling Projects like a ML Challenge",
    "section": "4. Define Metrics üéØ",
    "text": "4. Define Metrics üéØ\nWe defined soft metrics implicitly.\n\nDo resources fit a Persona?\nAre the curriculum suitable for a certain category (e.g.¬†beginner)?\nAre the resources accessible?\nAre the resources of high quality?\nIs a resource complementing other resources or can it replace other resources in the curriculum?"
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-test-strategy",
    "href": "posts/2022-04-28-ml-pm/index.html#define-test-strategy",
    "title": "Tackling Projects like a ML Challenge",
    "section": "5. Define Test strategy üìã",
    "text": "5. Define Test strategy üìã\nWe had chosen feedback as our test strategy. Feedback was gatherd from outsiders that were not involved in the creation process."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#define-architecture",
    "href": "posts/2022-04-28-ml-pm/index.html#define-architecture",
    "title": "Tackling Projects like a ML Challenge",
    "section": "6. Define architecture üèóÔ∏è",
    "text": "6. Define architecture üèóÔ∏è\nThe resources were collected in a wiki style manner that was easily accessible. The curriculum and its resources were structured by the categories defined in step 1."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#prepare-data",
    "href": "posts/2022-04-28-ml-pm/index.html#prepare-data",
    "title": "Tackling Projects like a ML Challenge",
    "section": "7. Prepare data üë∑",
    "text": "7. Prepare data üë∑\nCollect the resources. Scan the resources. Save references. Check accessibility."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#execute",
    "href": "posts/2022-04-28-ml-pm/index.html#execute",
    "title": "Tackling Projects like a ML Challenge",
    "section": "8. Execute ‚ñ∂Ô∏è",
    "text": "8. Execute ‚ñ∂Ô∏è\nBuild a baseline model by choosing the relevant resources, sort them by category and write them down in the wiki. Make the page look nice and welcoming."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#evaluate",
    "href": "posts/2022-04-28-ml-pm/index.html#evaluate",
    "title": "Tackling Projects like a ML Challenge",
    "section": "9. Evaluate üìä",
    "text": "9. Evaluate üìä\nGet feedback from stakeholders that are not part of the creation process."
  },
  {
    "objectID": "posts/2022-04-28-ml-pm/index.html#iterate",
    "href": "posts/2022-04-28-ml-pm/index.html#iterate",
    "title": "Tackling Projects like a ML Challenge",
    "section": "10. Iterate üîÅ",
    "text": "10. Iterate üîÅ\nAfter a few iterations we had a nice curriculum. Additionally we encourage the audience to extend the wiki page."
  },
  {
    "objectID": "posts/2021-11-22-vpp-seq/2021-11-22-vpp-seq.html",
    "href": "posts/2021-11-22-vpp-seq/2021-11-22-vpp-seq.html",
    "title": "Notes about Sequence Modelling",
    "section": "",
    "text": "I lately participated in the Google Brain - Ventilator Pressure Prediction competition. I didn‚Äôt score decent, but I still learned a lot. And some of it is worth to sum up, so I can easily look it up later.\nThe goal of the competition was to predict airway pressure of lungs that are ventilated in a clinician-intensive procedure. Given values of the input pressure (u_in) we had to predict the output pressure for a time frame of a few seconds.\n\n\n<AxesSubplot:xlabel='time_step', ylabel='Pressure'>\n\n\n\n\n\nSince all u_in values for a time frame were given we can build a bidirectional sequence model. Unless in a typical time-series problem where the future points are unknown at a certain time step, we know the future and past input values. Therefore I decided not to mask the sequences while training.\nA good model choice for sequencing tasks are LSTMs and Transformers. I built a model that combines both architectures. I also tried XGBoost with a lot of features (especially windowing, rolling, lead, lag features) engineering, But neural nets (NN) performed better, here. Though I kept some of the engineered features as embeddings for the NN model.\nThe competition metric was mean average error (MAE). Only those pressures were evaluated, that appear while filling the lungs with oxygen.\n\n\nBesides the given features, u_in, u_out, R, C and time_step I defined several features. They can by categorized as:\n\narea (accumulation of u_in over time) from this notebook\none hot encoding of ventilator parameters R and C\nstatistical (mean, max, skewness, quartiles, rolling mean, ‚Ä¶)\nshifted input pressure\ninput pressure performance over window\ninverse features\n\nTo reduce memory consumption I used a function from this notebook.\n\ndef gen_features(df, norm=False):\n    \n    # area feature from https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153\n    df['area'] = df['time_step'] * df['u_in']\n    df['area_crv'] = (1.5-df['time_step']) * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['area_crv'] = df.groupby('breath_id')['area_crv'].cumsum()\n    df['area_inv'] = df.groupby('breath_id')['area'].transform('max') - df['area']\n    \n    df['ts'] = df.groupby('breath_id')['id'].rank().astype('int')\n    \n    df['R4'] = 1/df['R']**4\n    df['R'] = df['R'].astype('str')\n    df['C'] = df['C'].astype('str')\n    \n    df = pd.get_dummies(df)\n    \n    for in_out in [0,1]: #,1\n        for qs in [0.2, 0.25, 0.5, 0.9, 0.95]:\n            df.loc[:, f'u_in_{in_out}_q{str(qs*100)}'] = 0\n            df.loc[df.u_out==in_out, f'u_in_{in_out}_q{str(qs*100)}'] = df[df.u_out==in_out].groupby('breath_id')['u_in'].transform('quantile', q=0.2)\n    \n        for agg_type in ['count', 'std', 'skew','mean', 'min', 'max', 'median', 'last', 'first']:\n            df.loc[:,f'u_out_{in_out}_{agg_type}'] = 0\n            df.loc[df.u_out==in_out, f'u_out_{in_out}_{agg_type}'] = df[df.u_out==in_out].groupby('breath_id')['u_in'].transform(agg_type)\n    \n        if norm:\n            df.loc[:,f'u_in'] = (df.u_in - df[f'u_out_{in_out}_mean']) / (df[f'u_out_{in_out}_std']+1e-6)\n    \n    \n    for s in range(1,8):\n        df.loc[:,f'shift_u_in_{s}'] = 0\n        df.loc[:,f'shift_u_in_{s}'] = df.groupby('breath_id')['u_in'].shift(s)\n        df.loc[:,f'shift_u_in_m{s}'] = 0\n        df.loc[:,f'shift_u_in_m{s}'] = df.groupby('breath_id')['u_in'].shift(-s)\n    \n    df.loc[:,'perf1'] = (df.u_in / df.shift_u_in_1).clip(-2,2)\n    df.loc[:,'perf3'] = (df.u_in / df.shift_u_in_3).clip(-2,2)\n    df.loc[:,'perf5'] = (df.u_in / df.shift_u_in_5).clip(-2,2)\n    df.loc[:,'perf7'] = (df.u_in / df.shift_u_in_7).clip(-2,2)\n    \n    df.loc[:,'perf1'] = df.perf1-1\n    df.loc[:,'perf3'] = df.perf3-1\n    df.loc[:,'perf5'] = df.perf5-1\n    df.loc[:,'perf7'] = df.perf7-1\n    \n    df.loc[:,'perf1inv'] = (df.u_in / df.shift_u_in_m1).clip(-2,2)\n    df.loc[:,'perf3inv'] = (df.u_in / df.shift_u_in_m3).clip(-2,2)\n    df.loc[:,'perf5inv'] = (df.u_in / df.shift_u_in_m5).clip(-2,2)\n    df.loc[:,'perf7inv'] = (df.u_in / df.shift_u_in_m7).clip(-2,2)\n    \n    df.loc[:,'perf1inv'] = df.perf1inv-1\n    df.loc[:,'perf3inv'] = df.perf3inv-1\n    df.loc[:,'perf5inv'] = df.perf5inv-1\n    df.loc[:,'perf7inv'] = df.perf7inv-1\n    \n    df.loc[:,'rol_mean5'] = df.u_in.rolling(5).mean()\n    \n    return df\n\n\n\nThe data was transformed with scikit‚Äôs RobustScaler to reduce influence of outliers.\n\nfeatures =  list(set(train.columns)-set(['id','breath_id','pressure','kfold_2021','kfold']))\nfeatures.sort()\n\nrs = RobustScaler().fit(train[features]) \n\n\n\n\n\nI didn‚Äôt do cross validation here, but instead trained the final model on the entire dataset. Nevertheless it‚Äôs helpful to build kfolds for model evaluation. I build GroupKFold over breath_id to keep the entire time frame in the same fold.\n\n\n\nSince the data is quite small (ca. 800 MB after memory reduction) I decided to load the entire train set in the Dataset object during construction (calling __init__()). In a first attempt I loaded the data as Pandas Dataframe. Then I figured out (from this notebook) that converting the Dataframe into an numpy array speeds up training significantly. The Dataframe is converted to an numpy array by the scaler.\nSince the competition metric only evaluates the pressures where u_out==0 I also provide a mask tensor, which can later on be used feeding the loss and metric functions.\n\nclass VPPDataset(torch.utils.data.Dataset):\n    \n    def __init__(self,df, scaler, is_train = True, kfolds = [0], features = ['R','C', 'time_step', 'u_in', 'u_out']):\n        if is_train:\n            # build a mask for metric and loss function\n            self.mask = torch.FloatTensor(1 - df[df['kfold'].isin(kfolds)].u_out.values.reshape(-1,80))\n            self.target = torch.FloatTensor(df[df['kfold'].isin(kfolds)].pressure.values.reshape(-1,80))\n            \n            # calling scaler also converts the dataframe in an numpy array, which results in speed up while training\n            feature_values = scaler.transform(df[df['kfold'].isin(kfolds)][features]) \n            \n            self.df = torch.FloatTensor(feature_values.reshape(-1,80,len(features)))\n            \n        else:\n            self.mask = torch.FloatTensor(1 - df.u_out.values.reshape(-1,80))\n            \n            feature_values = scaler.transform(df[features]) \n            self.df = torch.FloatTensor(feature_values.reshape(-1,80,len(features)))\n            \n            self.target = None\n        \n        self.features = features\n        self.is_train = is_train\n        \n    def __len__(self):\n        return self.df.shape[0]\n        \n    def __getitem__(self, item):\n        sample = self.df[item]\n        mask = self.mask[item]\n        if self.is_train:\n            targets = self.target[item]\n        else:\n            targets = torch.zeros((1))\n        \n        return torch.cat([sample, mask.view(80,1)],dim=1), targets #.float()\n\n\n\n\nMy model combines a multi layered LSTM and a Transformer Encoder. Additionally I build an AutoEncoder by placing a Transformer Decoder on top of the Transformer encoder. The AutoEncoder predictions are used as auxiliary variables.\n\nSome further considerations:\n\nI did not use drop out. The reason why it performence worse is discussed here.\nLayerNorm can be used in sequential models but didn‚Äôt improve my score.\n\nThe model is influenced by these notebooks:\n\nTransformer part\nLSTM part\nParameter initialization\n\n\n# Influenced by: \n# Transformer: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n# LSTM: https://www.kaggle.com/theoviel/deep-learning-starter-simple-lstm\n# Parameter init from: https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization \n\nclass VPPEncoder(nn.Module):\n\n    def __init__(self, fin = 5, nhead = 8, nhid = 2048, nlayers = 6, seq_len=80, use_decoder = True):\n        super(VPPEncoder, self).__init__()\n                 \n        self.seq_len = seq_len\n        self.use_decoder = use_decoder\n        \n        # number of input features\n        self.fin = fin\n                      \n        #self.tail = nn.Sequential(\n        #    nn.Linear(self.fin, nhid),\n        #    #nn.LayerNorm(nhid),\n        #    nn.SELU(),\n        #    nn.Linear(nhid, fin),\n        #    #nn.LayerNorm(nhid),\n        #    nn.SELU(),\n        #    #nn.Dropout(0.05),\n        #)                \n            \n        encoder_layers = nn.TransformerEncoderLayer(self.fin, nhead, nhid , activation= 'gelu')\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n        \n        decoder_layers = nn.TransformerDecoderLayer(self.fin, nhead, nhid, activation= 'gelu')\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, nlayers)\n        \n        self.lstm_layer = nn.LSTM(fin, nhid, num_layers=3, bidirectional=True)\n        \n        \n        # Head\n        self.linear1 = nn.Linear(nhid*2+fin , seq_len*2)\n        self.linear3 = nn.Linear(seq_len*2, 1)\n       \n        \n        self._reinitialize()\n        \n\n    # from https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization    \n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow/Keras-like initialization\n        \"\"\"\n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    # Set forget-gate bias to 1\n                    n = p.size(0)\n                    p.data[(n // 4):(n // 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n            elif 'fc' in name:\n                if 'weight' in name:\n                    nn.init.xavier_uniform_(p.data,gain=3/4)\n                elif 'bias' in name:\n                    p.data.fill_(0)\n  \n\n    def forward(self, x):\n        out = x[:,:,:-1]\n        \n        out = out.permute(1,0,2)\n        \n        out = self.transformer_encoder( out)\n        out_l,_ = self.lstm_layer(out)\n        \n        if self.use_decoder:\n            out = self.transformer_decoder(out, out) \n            out_dec_diff = (out - x[:,:,:-1].permute(1,0,2)).abs().mean(dim=2)\n        else:\n            out_dec_diff = out*0\n        \n        out = torch.cat([out, out_l], dim=2)\n        \n        # Head\n        out = F.gelu(self.linear1(out.permute(1,0,2)))\n        out = self.linear3(out)\n\n        return out.view(-1, self.seq_len) , x[:,:,-1], out_dec_diff.view(-1, self.seq_len)  \n\n\n\n\n\n\nThe competition metric was Mean Absolute Error (MAE), but only for the time-steps where air flows into the lunge (approx. half of the timesteps). Hence, I masked the predictions (using the flag introduced in the Dataset) ignoring the unnecessary time-steps. The flags are passed through the model (val[1]) and is an output along with the predictions.\n\n\ndef vppMetric(val, target):\n    flag = val[1]\n    \n    preds = val[0]\n    \n    loss = (preds*flag-target*flag).abs()\n    loss= loss.sum()/flag.sum()\n    \n    return loss\n\nThee values produced by the AutoGenerater are additionally measured by the vppGenMetric. It uses MAE to evaluate how good the reconstruction of the input features values evolves.\n\n\ndef vppGenMetric(val, target):\n    gen =val[2]\n    \n    flag = val[1]\n    \n    loss = (gen*flag).abs()\n    loss= loss.sum()/flag.sum()\n    \n    return loss\n\n\n\n\n\nThe loss function is a combination of L1-derived-Loss (vppAutoLoss) for the predictions and the AutoEncoder-predictions.\nDue to this discussion I did some experiments with variations of Huber and SmoothL1Loss. The later (vppAutoSmoothL1Loss) performed better.\n\n\ndef vppAutoLoss(val, target):\n    gen =val[2]\n    \n    flag = val[1]\n    \n    preds = val[0]\n    \n    loss = (preds*flag-target*flag).abs() + (gen*flag).abs()*0.2 #\n    loss= loss.sum()/flag.sum()\n    \n    return loss\n    \n\n# Adapting https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss\ndef vppAutoSmoothL1Loss(val, target):\n    \n    beta = 2\n    fct = 0.5\n    \n    gen =val[2]\n    \n\n    flag = val[1] \n    \n    preds = val[0]\n    \n    loss = (preds*flag-target*flag).abs() + (gen*flag).abs()*0.2\n    \n    loss = torch.where(loss < beta, (fct*(loss**2))/beta, loss)#-fct*beta)\n    \n    # reduction mean**0.5\n    loss = loss.sum()/flag.sum() #()**0.5\n    \n    return loss"
  },
  {
    "objectID": "posts/2021-08-15-model-allocation/2021-08-15-model-allocation.html",
    "href": "posts/2021-08-15-model-allocation/2021-08-15-model-allocation.html",
    "title": "Model allocation",
    "section": "",
    "text": "This notebook was originally published on https://www.kaggle.com/joatom/model-allocation.\n\nIn this notebook I experiment with two ensembling strategies.\nThere are many ways to combine different models to improve predictions. A common technique for regression tasks is taking a weighted average of the model predictions (y_pred = (m1(x)*w1 + ... + mn(x)*wn) / n). Another common technique is building a meta model, that is trained on the models‚Äô outputs.\nThe first chapter starts with a simple linear combination of two models. And we explore with an simple example, why ensembling actually works. These insights will lead, in the second chapter, to the first technique on how to choose weights for a linear ensemble by using residual variance. In the third chapter an alternative for the weight selection is examined. This second technique is inspired by portfolio theory (a theory to combine financial assets). In the fourth chapter the two techniques are applied and compared on the Tabular Playground Series (TPS) - Aug 2021 competition. Finaly cross validation (CV) and leaderboard (LB) Scores are listed in the fith chapter.\n\n\n\n\n\n\nNote\n\n\n\nFor the ease of explanation we make some simplifying assumptions, such as equal distribution of the data, same distribution on unseen data, ‚Ä¶ (just think of a vanilla world).\n\n\n\n1. Why ensembling works\nSuppose there are two fitted regression models and they predict values like shown in the first chart.\n\n\n\n\n\nTo get a better intuition on how good the two models fit the ground truth, we plot the residuals y_true(x)-m(x).\n\n\n\n\n\nIf we had to choose one of the models, which one would we prefer? Model 2 does better on the first data point and perfect on the third, but it contains an outlier the 5th data point.\nLet‚Äôs look at the mean and the variance of the residuals.\n\n\nModel #1. mean:  0.0714, var:  1.6020\nModel #2. mean:  0.0000, var:  2.0000\n\n\nOn the long run Model2 has an average residual of 0. Model 1 carries along a residual of 0.0714. So on average Model 2 seams to do better.\nBut Model 2 also has a higher variance. That implies we have a great chance to do a great prediction (e.g.¬†x=3) but we also have high risk to screw the prediction (e.g.¬†x=5).\nNow we build a simple linear ensemble of the two models like ens = 0.5 * m1 + 0.5 m2.\n\n\n\n\n\nThe ensemble line is closer to the true values. It also looks smoother then m1 and m2.\n\n\n\n\n\nIn the residual chart we can see that the ensemble does a bit worse for x=3 compared to Model 2. But it also decreases the residuals for the outliers (points 1, 5, 7).\nLet‚Äôs check the stats:\n\n\nEnsemble. mean:  0.0357, var:  0.2219\n\n\nWe dramatically reduced the variance, hence reduced the risk/chance. The mean value is now in between Model 1 and Model 2.\nFinally let‚Äôs play around with the model weights in the ensemble and check how mean and variance change.\n\n# generate weights for w1\nweight_m1 = np.linspace(0, 1, 30)\n\nens_mean = np.zeros(30)\nens_var = np.zeros(30)\n\nfor i, w1 in enumerate(weight_m1):\n    # build ensemble for different weights\n    ens = m1*w1 + m2*(1-w1)\n    ens_res = y_true - ens\n    \n    # keep track of mean and var of the differently weighted ensembles\n    ens_mean[i] = ens_res.mean()\n    ens_var[i] = ens_res.var()\n\n\n\n\n\n\nWith the previous 50:50 split the variance seems almost at the lowest point. So we only get a reduction of the mean below 0.0357 if we allow the ensemble to have more variance, hence take more risk.\n\n\n2. Weights by residual variance\nSince the Model 1 and Model 2 are well fitted, their average residuals are pretty close to 0. So let‚Äôs focus on reducing our variance to avoid surprises on later later predictions.\nWe now solve for the optimal weights that minimizes the variance of the residual of our ensemble with this function:\n\nfun = lambda w: (y_true-np.matmul(w, preds)).var()\n\nWe also define a constraint so that the w.sum() == 0:\n\n# w.sum() = 1  <=> 0 = w.sum()-1\ncons = ({'type': 'eq', 'fun': lambda w: w.sum()-1})\n\nIf you want, you can also set bounds, so that the weights want be negative.\nI don‚Äôt. I like the idea of going short with a model. And negative weights really increase the results of TPS predictions in chapter 4.\n\nbnds = ((0,None),\n        (0,None),\n        (0,None),\n        (0,None),\n        (0,None))\n\nNow, we are all set to retrieve the optimal weights.\n\n# predictions of Model1 and Model 2\npreds = np.array([m1, m2])\n\n# init weights\nw_init = np.ones(preds.shape[0])/preds.shape[0]\n\n# run optimization\nres = scipy.optimize.minimize(fun, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\n# get optimal weights\nw_calc = res.x\n\n\nprint(f'Calculated weights: {w_calc}')\n\nCalculated weights: [0.53150242 0.46849758]\n\n\nLet‚Äôs see how the calculated weights perform.\n\nens_ex1 = np.matmul(w_calc, preds)\nens_ex1_res=y_true-ens_ex1\n\nprint(f'Ensemble Ex1. mean: {ens_ex1_res.mean(): .4f}, var: {ens_ex1_res.var(): .4f}')\n\nEnsemble Ex1. mean:  0.0380, var:  0.2157\n\n\nWe con compare the results with the first ensemble 50:50 split. With the calculated weights we could further reduce the variance of the model (0.2219 -> 0.2157). But unfortunately the mean increased a bit (0.0357 -> 0.0380).\nWe see the trade off between mean and variance and have to decide if we prefer a more stable model or take some risk for better results.\n\n\n3. Portfolio theory for ensembling\nIn finance different assets are often combined in a portfolio. There are many criteria for the asset selection/allocation. One of them is by choosing a risk strategy. In 1952 the economist Harry Markowitz defined a Portfolio Selection strategy which built the foundation of many portfolio strategies to come. There is a great summary on Wikipidia, but the original paper can also be found with a google search.\nSo, what it is all about. Let‚Äôs assume we are living in an easy, plain vanilla world. We want to build a portfolio that yields high return with low risk. That‚Äôs not easy. If we only buy stocks of our favorite fruit grower, a rainy summer would result in a low return. Wouldn‚Äôt it be smart to also buy stocks of a raincoat producer, just in case. But what if the summer was sunny, then we would have rather invested the entire money in fruits instead of raincoats. It‚Äôs clearly a trade off. Either we lower the risk of loosing money in a rainy summer and invest in both (fruits and raincoats). Or we take the risk investing all money in fruits to maybe gain more money. And if we lower the risk, in which raincoat producer should we invest? The one with the bumpy stock price or the one with a steady, but slowly growing stock price.\nNow, we already see the first similarities between our ensemble example above and the Portfolio Theory. Risk can be measured through variance and a good return of our ensemble is results in a low expected residual.\nBut there is even more in Portfolio Theory. It also takes dependencies between assets into account. If the summer is sunny the fruit price goes up and the raincoat price goes down, they are somewhat negative correlated.\nSince we expect the average residual of our fitted models to be close to 0 and we build a linear model, we can expect our ensemble average residual also to be close to 0. Therefore, we focus on optimizing the portfolio variance, which can be boiled down to Var_p = w'*Cov*w. The covariance measures the dependency between combined models and also considers the variance.\n\nWhat data can we actually use? In the financial example returns are the increase or decrease of an asset price (p/p_t-1), hence we are looking on returns for a certain period of time. In ML we can take our out-of-fold (oof) predictions and calculate the residuals from the train targets to build a dataset.\n\n\nCan we do this despite we are looking at a time-series in the financial example? Yes, in this basic portfolio theory we don‚Äôt take time dependencies into account. But it‚Äôs important to keep the same order for the different asset returns for correlation/covariance calculation. We want to compare the residual of model 1 and 2 for always the same data item.\n\nThe optimization function for the second ensemble technique is:\n\n## following https://en.wikipedia.org/wiki/Modern_portfolio_theory\n\n# Predictions of Model 1 and Model 2\npreds = np.array([m1,m2])\n# Residuals of Model 1 and Model 2\npreds_res = np.array([m1_res, m2_res])\n\n# handle residuals like asset returns\nR = np.array(preds_res.mean(axis=1))\n# factor by which R is considered during optimization. turned off for our example\nq = 0 #-1\n\n# covariance matrix of model residuals\nCM = np.cov(preds_res)\n\n# optimization function\nfun = lambda w: np.matmul(np.matmul(w.T,CM),w) - q * np.matmul(R,w)\n\n# constraint: weights must sum up to 1.0\ncons = ({'type': 'eq', 'fun': lambda x: x.sum()-1})\n\nRun the optimization.\n\n# init weights\nw_init = np.ones(preds.shape[0])/preds.shape[0]\n\n# run optimization\nres = scipy.optimize.minimize(fun, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\n# get optimal weights\nw_calc = res.x\n\nprint(f'Calculated weights: {w_calc}')\n\nCalculated weights: [0.53150242 0.46849758]\n\n\nThe weights are the same as in the first technique. That really surprised me. And I run a couple of examples with different models. But the weights were only slightly different between the two techniques.\n\n\n4. Ensembling TPS Aug 2021\nNow that we have to techniques to ensemble, let‚Äôs try them on the TPS August 2021 data.\nWe do a 7 kfold split and calculate the residuals on the out-of-fold-predictions, that are used for validation. We train 7 regression models with different architecture so we get some diversity.\n\nN_SPLITS = 7\nSEED = 2021\n\nPATH_INPUT = '/home/kaggle/TPS-AUG-2021/input/'\n\n\n# load and shuffle\ntest = pd.read_csv(PATH_INPUT + 'test.csv')\n\ntrain = pd.read_csv(PATH_INPUT + 'train.csv').sample(frac=1.0, random_state = SEED).reset_index(drop=True)\n\ntrain['fold_crit'] = train.loss\ntrain.loc[train.loss>=39, 'fold_crit']=39\n\n\ntarget = 'loss'\nfold_crit = 'fold_crit'\nfeatures = list(set(train.columns)-set(['id','kfold','loss','fold_crit']+[target]))\n\n\n# apply abhisheks splitting technique\nskf = StratifiedKFold(n_splits = N_SPLITS, random_state = None, shuffle = False)\n\ntrain.kfold = -1\n\nfor f, (train_idx, valid_idx) in enumerate(skf.split(X = train, y = train[fold_crit].values)):\n    \n    train.loc[valid_idx,'kfold'] = f\n\ntrain.groupby('kfold')[target].count()\n\nkfold\n0.0    35715\n1.0    35715\n2.0    35714\n3.0    35714\n4.0    35714\n5.0    35714\n6.0    35714\nName: loss, dtype: int64\n\n\n\n# define models\nmodels = {\n    'LinReg': LinearRegression(n_jobs=-1),\n    'HGB': HistGradientBoostingRegressor(),\n    'XGB': XGBRegressor(tree_method = 'gpu_hist', reg_lambda= 6, reg_alpha= 10, n_jobs=-1),\n    'KNN': KNeighborsRegressor(100, n_jobs=-1),\n    'BayesRidge': BayesianRidge(),\n    'ExtraTrees': ExtraTreesRegressor(max_depth=2, n_jobs=-1),\n    'Poisson': Pipeline(steps=[('scale', StandardScaler()),\n                ('pois', PoissonRegressor(max_iter=100))])    \n}\n\n\nFit models and save oof predictions.\n\nfor (m_name, m) in models.items():\n    print(f'# Model:{m_name}\\n')\n    train[m_name + '_oof'] = 0\n    test[m_name] = 0\n    \n    y_oof = np.zeros(train.shape[0])\n    \n    for f in range(N_SPLITS):\n\n        train_df = train[train['kfold'] != f]\n        valid_df = train[train['kfold'] == f]\n        \n        m.fit(train_df[features], train_df[target])\n        \n        oof_preds = m.predict(valid_df[features])\n        y_oof[valid_df.index] = oof_preds\n        print(f'Fold {f} rmse: {mean_squared_error(valid_df[target], oof_preds, squared = False):0.5f}')\n        \n        test[m_name] += m.predict(test[features]) / N_SPLITS\n    \n    train[m_name + '_oof'] = y_oof\n    \n    print(f\"\\nTotal rmse: {mean_squared_error(train[target], train[m_name + '_oof'], squared = False):0.5f}\\n\")\n\n\noof_cols = [m_name + '_oof' for m_name in models.keys()]\n\nprint(f\"# ALL Mean ensemble rmse: {mean_squared_error(train[target], train[oof_cols].mean(axis=1), squared = False):0.5f}\\n\")        \n\n# Model:LinReg\n\nFold 0 rmse: 7.89515\nFold 1 rmse: 7.90212\nFold 2 rmse: 7.90260\nFold 3 rmse: 7.89748\nFold 4 rmse: 7.89844\nFold 5 rmse: 7.89134\nFold 6 rmse: 7.89643\n\nTotal rmse: 7.89765\n\n# Model:HGB\n\nFold 0 rmse: 7.86447\nFold 1 rmse: 7.87374\nFold 2 rmse: 7.86688\nFold 3 rmse: 7.86255\nFold 4 rmse: 7.86822\nFold 5 rmse: 7.85785\nFold 6 rmse: 7.86566\n\nTotal rmse: 7.86563\n\n# Model:XGB\n\nFold 0 rmse: 7.91179\nFold 1 rmse: 7.92748\nFold 2 rmse: 7.92141\nFold 3 rmse: 7.91901\nFold 4 rmse: 7.91125\nFold 5 rmse: 7.90286\nFold 6 rmse: 7.92340\n\nTotal rmse: 7.91675\n\n# Model:KNN\n\nFold 0 rmse: 7.97845\nFold 1 rmse: 7.97709\nFold 2 rmse: 7.98165\nFold 3 rmse: 7.97895\nFold 4 rmse: 7.97781\nFold 5 rmse: 7.97798\nFold 6 rmse: 7.98711\n\nTotal rmse: 7.97986\n\n# Model:BayesRidge\n\nFold 0 rmse: 7.89649\nFold 1 rmse: 7.90576\nFold 2 rmse: 7.90349\nFold 3 rmse: 7.90007\nFold 4 rmse: 7.90121\nFold 5 rmse: 7.89455\nFold 6 rmse: 7.89928\n\nTotal rmse: 7.90012\n\n# Model:ExtraTrees\n\nFold 0 rmse: 7.93239\nFold 1 rmse: 7.93247\nFold 2 rmse: 7.92993\nFold 3 rmse: 7.93121\nFold 4 rmse: 7.93129\nFold 5 rmse: 7.93247\nFold 6 rmse: 7.93364\n\nTotal rmse: 7.93191\n\n# Model:Poisson\n\nFold 0 rmse: 7.89597\nFold 1 rmse: 7.90240\nFold 2 rmse: 7.90233\nFold 3 rmse: 7.89682\nFold 4 rmse: 7.89873\nFold 5 rmse: 7.89241\nFold 6 rmse: 7.89701\n\nTotal rmse: 7.89795\n\n# ALL Mean ensemble rmse: 7.88061\n\n\n\nLet‚Äôs a look at the correlation heatmap.\n\n\noof_cols = [m_name + '_oof' for m_name in models.keys()]\n\noofs = train[oof_cols]\n\noof_diffs = oofs.copy()\nfor c in oof_cols:\n    oof_diffs[c] = oofs[c]-train[target]\n    oof_diffs[c] = oof_diffs[c]#**2\n\nsns.heatmap(oof_diffs.corr())\n\n<AxesSubplot:>\n\n\n\n\n\nXGB and KNN are most diverse, so I export a 50:50 ensemble. I‚Äôll also export an equally weighted ensemble of all models and HGB only because it is the best single model.\n\n\nCV: ALL equaly weighted: 7.880605075536334\nCV: XGB only: 7.916746570344035\nCV: HGB only: 7.865625158180185\nCV: XGB and LinReg (50:50): 7.872064005057903\nCV: XGB and KNN (50:50): 7.893210466099108\n\n\nNext we inspect the variance and mean of the residuals. Means are close to 0, as expected.\n\noof_diffs.var(), oof_diffs.mean()\n\n(LinReg_oof        62.373163\n HGB_oof           61.868296\n XGB_oof           62.675125\n KNN_oof           63.678431\n BayesRidge_oof    62.412188\n ExtraTrees_oof    62.915511\n Poisson_oof       62.377910\n dtype: float64,\n LinReg_oof       -0.000055\n HGB_oof          -0.003314\n XGB_oof           0.001392\n KNN_oof          -0.005395\n BayesRidge_oof   -0.000024\n ExtraTrees_oof   -0.000136\n Poisson_oof      -0.000084\n dtype: float64)\n\n\nThese are the histograms of the residuals:\n\n\narray([[<AxesSubplot:title={'center':'LinReg_oof'}>,\n        <AxesSubplot:title={'center':'HGB_oof'}>,\n        <AxesSubplot:title={'center':'XGB_oof'}>],\n       [<AxesSubplot:title={'center':'KNN_oof'}>,\n        <AxesSubplot:title={'center':'BayesRidge_oof'}>,\n        <AxesSubplot:title={'center':'ExtraTrees_oof'}>],\n       [<AxesSubplot:title={'center':'Poisson_oof'}>, <AxesSubplot:>,\n        <AxesSubplot:>]], dtype=object)\n\n\n\n\n\nFinally, we apply the two techniques to calculate the ensembling weights\n\nR = oof_diffs.mean().values\nCM = oof_diffs.cov().values\n\nq=0\n\n# Var technique\nfun_ex1 = lambda w: (train[target]-np.matmul(oofs.values, w)).var()\n# Cov technique\nfun_ex2 = lambda w: np.matmul(np.matmul(w.T,CM),w) - q * np.matmul(R,w)\n\ncons = ({'type': 'eq', 'fun': lambda x: x.sum()-1})\n\nbnds = ((0,None),\n        (0,None),\n        (0,None),\n        (0,None),\n        (0,None))\n\n\n# Example 1\n\nw_init = np.ones((len(models)))/len(models)\n\nres = scipy.optimize.minimize(fun_ex1, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\nw_calc = res.x\n\n\n\nCV: Ex1 calc weights: 7.85594426240217\n\n\n\n# Example 2\n\nw_init = np.ones((len(models)))/len(models)\n\nres = scipy.optimize.minimize(fun_ex2, w_init, method='SLSQP',  constraints=cons) #,bounds=bnds\n\nw_calc = res.x\n\n\n\nCV: Ex2 calc weights: 7.855944262936231\n\n\n\n\n5. Results\nThe competition metric is root mean squared error (RMSE). These are the scores of the different ensembles:\n\n\n\nEnsemble\nCV\npublic LB\n\n\n\n\nHGB only\n7.86563\n7.90117\n\n\nAll weights eq.\n7.88061\n7.92183\n\n\nXGB and KNN (50:50)\n7.89321\n7.91603\n\n\nEx1 (Var)\n7.85594\n7.88876\n\n\nEx2 (Cov)\n7.85594\n7.88876\n\n\n\n\n\nReferences\n\nModern Portfolio Theory: https://en.wikipedia.org/wiki/Modern_portfolio_theory\nTPS August 2021 Competition: https://www.kaggle.com/c/tabular-playground-series-aug-2021/overview\n\n\n\nRessources\n\nOriginal notebook: https://www.kaggle.com/joatom/model-allocation\nTPS data: https://www.kaggle.com/c/tabular-playground-series-aug-2021/data"
  },
  {
    "objectID": "posts/2022-02-28-relationship/index.html",
    "href": "posts/2022-02-28-relationship/index.html",
    "title": "Flat to hierarchie",
    "section": "",
    "text": "Tabular data is often transformed in a flattened manner so it can easily be processed by commen ML frameworks. Sometimes it is usefull to understand the relationships and hierarchies between the columns of a flattend datasource. In this notebook hierarchies are extracted from a flattened csv file."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "",
    "text": "üá©üá™ üá∫üá∏ [kaggle version]\nIn this blog post, I want to find out how easy it is to create interactive charts in notebooks. As a framework, I will use Altair, as it can be easily integrated into my blog [FP20].\nIn order to make the contribution interesting, I evaluate the death rates from Germany over the last five years. I construct two periods - the first period before and the second one during Covid19.\nWe will analyze these questions:\nThe first part describes the origin and processing of the data. Then, charts are built around the above questions. Finally, a technical conclusion to the framework follows."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#preprocessing",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#preprocessing",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe evaluation period is limited to March 2016 until February 2021. The period in which Corona was heavily active in Germany is simplified here to March 2020 (when Covid19 triggered the first major social changes in Germany) until February 2021 (orange). The pre-Covid19 period is set to March 2016 until February 2020 (blue). Thus, the Covid19 period covers exactly one year and the pre-Covid19 period exactly four years. Thus, both periods remain comparable without serious seasonal deviations. The split of the periods is illustrated in the diagram below."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#aggregation-of-data",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#aggregation-of-data",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Aggregation of data",
    "text": "Aggregation of data\nIn the calculations, the values are averaged over a period of time. Depending on the evaluation, this happens over the whole period or per month. The first and third quartile of the aggregated data may also be shown as shading in the diagrams. In some charts, the calculated points are interpolated to increase readability.\n\n\n\n\n\n\nTip\n\n\n\nFor some of the charts there are control elements in the upper right corner, such as drop-down boxes. The mouse wheel can be used to zoom and a chart can be reset by double-clicking."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Observation",
    "text": "Observation\nIn the age groups under 55, there was no excess mortality during these periods. In the age groups from 80 years and older, mortality increased massively."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-1",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-1",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Observation",
    "text": "Observation\nNorth Rhine-Westphalia (NW) has the largest number of deaths, since it is the state with highest population. In each state, there are increases in death rates in the Covid19 period. However, only minimal increases in Hesse (HE) and Bavaria (BY) can be seen for the age groups under the age of 65. In the other states, there is no noticeable increase in this age group.\nThere are smaller variations looking at the number of deaths per 100.000 inhabitants in the age group below 65. The increase in deaths in North Rhine-Westphalia (NW) is slightly lower than in Bavaria (BY) and roughly as high as in Baden-W√ºrttemberg (BW). Saxony (SN) and Brandenburg (BB) have the strongest increases.\n\n\n\n\n\n\nNote\n\n\n\nOnly deaths are evaluated. No other aspects (such as demographic aspects) are taken into account."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-2",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#observation-2",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Observation",
    "text": "Observation\nThe city-states Hamburg (HH) and Bremen (HB) show a moderate increase in deaths despite their high population density. Berlin (BE) instead has a high increase in death rate. Schleswig-Holstein (SH) recorded the lowest increase.\nThe correlation coefficient of population density and increase in mortality is:\n\n\n0.0232\n\n\nHence, There is no correlation for this comparison.\n\n\n\n\n\n\nNote\n\n\n\nOnly deaths are evaluated. No other aspects (such as demographic aspects) are taken into account."
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#data-sources",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#data-sources",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Data sources",
    "text": "Data sources\nThe data used here are from the ‚ÄúStatistisches Bundesamt‚Äù (Federal Statistical Office) and are subject to the license ‚Äúdl-de/by-2-0‚Äù. The license text can be found at www.govdata.de/dl-de/by-2-0. The data were modified exclusively within this notebook by executing the specified program code for the purpose of analysis.\n\n[SB21] Statistisches Bundesamt (Destatis), 2021 (published 2012/03/30), Sterbef√§lle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesl√§ndern f√ºr Deutschland 2016 - 2021, visited 2021/04/03, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile\n[SB20] Statistisches Bundesamt (Destatis), 2020 (published 2020/09/02), Bundesl√§nder mit Hauptst√§dten nach Fl√§che, Bev√∂lkerung und Bev√∂lkerungsdichte am 31.12.2019, visited 2021/04/03, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile"
  },
  {
    "objectID": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#other-references",
    "href": "posts/2021-04-06-mortality-germany/2021-04-06-mortality-germany.html#other-references",
    "title": "Deaths by age group and states in Germany from 2016 to 2021",
    "section": "Other references",
    "text": "Other references\nA lot of the coding is derived from various examples of the Altair homepage and from great examples in the coresponding Github Issue tracker answered by https://github.com/jakevdp.\n\n[AA1] https://altair-viz.github.io/gallery/index.html\n[AA2] https://github.com/altair-viz/altair/issues/\n[AG18] A. Gordon, 2018 (published 2018/10/06), Focus: generating an interactive legend in Altair, visited 2021/04/05, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55\n[FP20] fastpages.fast.ai, 2020 (published 2020/02/20), Fastpages Notebook Blog Post, visited 2021/04/05, https://fastpages.fast.ai/jupyter/2020/02/20/test.html"
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/watchtrain.html",
    "href": "posts/2021-06-19-watchtrain/watchtrain.html",
    "title": "Streaming ML training progress to a smart watch",
    "section": "",
    "text": "üèîÔ∏è\nDuring the Covid winter I hardly had any reason to leave the house. It was clear that I actively had to look after my mental and physical well-being. So, I decided to buy a smart watch (Fitbit Versa 3) and take on the 10000 steps per day challenge. Henceforth I spent much more time outside and in the sunlight moving my body.\n‚õÖ\nWhen I registered my new Fitbit I instantly got attracted by the For Developers link. As you might guess my thoughts started spinning like - Aha, they are providing a SDK! I got to try this out at some point and build an app for the watch. - I wanted this app to be related to ML or at least to data somehow. My first thought was - Would it be possible to use the green heart-rate sensor light as an OCR? - Nope, to complicated for a fun project. - Then I went on with the registration of the watch on the webpage.\nüå§Ô∏è\nSome month later, on a usual Sunday, I lay resting on the couch after lunch while the kids and my wife were cleaning the table and kitchen (I did the cooking ;-)). A little bored, I thought of my ML-model that was training on Hotel Room classification for a couple of hours upstairs in the office. I wanted to know how it was preceding. Sneaking upstairs would result in half an hour in front of the computer, followed by some trouble with my wife üòí. - May by I should eventually register at Neptune.ai or wandb.ai, then I could preview my trainings from the couch on my cell phone!? ‚Ä¶ Or may be I now have a new fun project for my new watch üòÑ üí°! -"
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/watchtrain.html#api-server",
    "href": "posts/2021-06-19-watchtrain/watchtrain.html#api-server",
    "title": "Streaming ML training progress to a smart watch",
    "section": "API-Server",
    "text": "API-Server\nThe requirements led to the architecture shown in the diagram. In the center of the application is an API-Server to coordinate the training and the watch. The Training, Watch and Web are client applications connected to the API-Server‚Äôs Watchtrain-Topic. The Topic contains a connection pool for the Training client (data Producer) and another connection pool for the Web and the Watch clients (data Consumer).\n\n\nThe initial idea was to setup a classical Consumer/Producer (Pub/Sub) pattern. But it ended up a bit different. The Topic holds the data in an object rather than a queue-like state and also does some data processing. The Producer and Consumer can still subscribe at any time, but they are also strongly connected via Websockets. I took the chance to play around with websockets, since it is also available on the watch.\n\nFor each client type there is an Agent that processes the data and messages that are send from the clients. The stats and progress data is saved in the topic. The topic generates the metric chart that is send to the Consumers, since I couldn‚Äôt find a charts library in the watch SDK.\nThe Topic-Consumer-Producer-Agent ‚Äúpattern‚Äù with the connection pool handler is set up in a generic way so it‚Äôs easy to develop other applications in the same manner and run them on the API-Server.\nAs API-Server I used FastApi which is easy to start with as shown on the tutorial site or in this video.\nThe communications between the components is done with JSON. Messages start with an action-field followed by the training_id and a more or less complex payload. Depending of the action value different functionalities are triggered, such as sending the metric image to the client or converting batch information into a progress bar."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/watchtrain.html#training",
    "href": "posts/2021-06-19-watchtrain/watchtrain.html#training",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Training",
    "text": "Training\n\nFastai\nThe easiest way to implement the train logging is by using the Fastai Callback infrastructure. So I built a WebsocketLogger which gets past to the training like this:\nlearn = cnn_learner(dls, resnet18, pretrained = False, metrics=[accuracy, \n                                                                Recall(average='macro'), \n                                                                Precision(average='macro')])\n\nlearn.unfreeze\nlearn.fit_one_cycle(10, lr_max = 5e-3, cbs=[WebsocketLogger('ws://myapiserver:8555/ws/watchtrain/producer/12345')])\nStarting out by looking at the source code of the fastai build-in CSVLoggers and ProgressCallback I learned how to track train data (metrics, epoch and batch progress). A bit challenging was the integration of the websocket client. I preferred a permanent connection rather than many one time (open-send-close) connections. Otherwise a simple REST call would have been more suitable. It is also very important that training must not break when the websocket connection is lost or the API-Server isn‚Äôt available anymore.\nThat‚Äôs how it is implemented using the websocket-client library:\n\ndef __init__(self, conn, ...):\n    self.conn = conn\n    ...\n    self.heartbeat = False\n    self._ws_connect()\n    ...\n\n...\n# gets called when a websocket is opened\ndef _on_ws_open(self,ws):\n    # ws connection is now ready => unlock\n    self.ws_ready_lock.release()\n    self.heartbeat = True\n    \ndef _ws_connect(self):\n    \n    self.heartbeat = False\n    \n    # aquire lock until websocket is ready to use\n    self.ws_ready_lock = threading.Lock()\n    self.ws_ready_lock.acquire()\n    \n    print('Connecting websocket ...')\n    \n    self.ws = websocket.WebSocketApp(self.conn,\n                                      on_open = self._on_ws_open,\n                                      on_message = self._on_ws_message,\n                                      on_error = self._on_ws_error,\n                                      on_close = self._on_ws_close)\n\n    # run websocket in background\n    thread.start_new_thread(self.ws.run_forever, ())\n    \n    # wait for websocket to be initialized, \n    # if connection is not possible (e.g. APIServer is down) resume after 3 sec, but heartbeat stays FALSE\n    self.ws_ready_lock.acquire(timeout = 3)\n    \n    print('... websocket connected.')\nThe WebSocketApp runs as a local websocket-handler in the background. The Locks are used to make sure the connection gets properly established before the first messages are send. The heartbeat is introduced to keep the training running even if the websocket connection is broken and could not be reconnected via WebSocketApp.\nIf there is no heartbeat anymore _ws_connect() is called again after any epoch. If the API-Server is still not reachable the training continuous after a 3 second waiting time.\n\n\nPytorch\nI skipped the pytorch implementation until I need it. But it is straight forward. Start a WebSocketApp thread in the background. Send the data from inside of the training/validation/inference-loop."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/watchtrain.html#watch",
    "href": "posts/2021-06-19-watchtrain/watchtrain.html#watch",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Watch",
    "text": "Watch\n\nThe layout is held pretty simple as shown in the picture. There is a progress bar for the epochs and one for the mini batches (train and valid). In the center is the chart of the metrics. And at the bottom are the latest metric values. The cell phone that belongs to the watch establish a websocket connection to the API-Server and puts EventListeners for incoming messages into place. The incoming messages are uploaded to the watch were they can be displayed."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/watchtrain.html#fastapi",
    "href": "posts/2021-06-19-watchtrain/watchtrain.html#fastapi",
    "title": "Streaming ML training progress to a smart watch",
    "section": "FastAPI",
    "text": "FastAPI\nFastAPI is a well-documented and easy to use framework. In the beginning I set it up with HTTPS. There is a tutorial on how to setup FastAPI with Traefik. But since I wanted to run the server at home I had to invest some evenings to figure out, how to set it up by myself. I used mkcert for SSL creation. A docker file to setup an FastAPI-Server at home can now be found here. At the end when I got it working I decided to not use HTTPS for reasons described below, ü§∑‚Äç‚ôÇÔ∏è."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/watchtrain.html#websockets",
    "href": "posts/2021-06-19-watchtrain/watchtrain.html#websockets",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Websockets",
    "text": "Websockets\nThe different components communicate instantly. The data is pushed to the watch, which is the preferred behavior on the receiving site. With the websocket on the training site it is a bit more complicated to be fail safe and pickup communication when the connection is broken for a longer period of time. I might switch this part to a simple REST-post in a later version. But this way it was a fun exersice nevertheless."
  },
  {
    "objectID": "posts/2021-06-19-watchtrain/watchtrain.html#fitbit-sdk",
    "href": "posts/2021-06-19-watchtrain/watchtrain.html#fitbit-sdk",
    "title": "Streaming ML training progress to a smart watch",
    "section": "Fitbit SDK",
    "text": "Fitbit SDK\nThe Fitbit SDK is nice. They provide an online IDE which can easily be connected to your devices. The SDK is documented with a few examples. They also host helpful forum.\nI had a bit of a hard time when I tried to load and display the Metrics chart image to the watch. I had to figure out that there are two types of jpeg, progressive and basic. And only one worked. It also was hard to figure out that the the image needs to have a certain size to be displayed. But that‚Äôs part of the normal learning path with a new technology.\nAnd than, there was this one thing that really upset me (But as fare as I read in forums it is not the Fitbit SDKs fault!). Android doesn‚Äôt allow regular HTTP connection through apps. That‚Äôs why I setup the API-Server with HTTPS. But since I generated the certificate on my own, it wasn‚Äôt a trusted source and therefore Android didn‚Äôt accept it. Then I found some post that showed how to access HTTP from a local net, but only for IP range 192.168.0.x. That meant either building a Reverse Proxy or changing the Subnet of my network. And then finally I needed to deal with the docker net-addresse where the API-Server is running. As suspected, one evening I freaked out - ?#@!, I just want to send a JSON to my cell phone! 30 years of web-development and all we ended up is JavaScript and SSL-certs @!# - That was a good time to go to bed, put the project aside for a few days and celebrate that most of the time I‚Äôm into data instead of GUI üòÅ.\nBesides that I really enjoyed it to build a nice app for my Fitbit."
  },
  {
    "objectID": "posts/2020-01-05-bgml-geotab/index.html",
    "href": "posts/2020-01-05-bgml-geotab/index.html",
    "title": "BigQuery-Geotab Intersection Congestion",
    "section": "",
    "text": "This blog post contains some of my codings for the 2019 kaggle BigQuery-Geotab competition.\nMy submission scored 1st Place in the categorie BigQuery ML Models built in SQL.\nThe challange was to predict six measures for cars approaching intersections in four US cities.\nMy objective in the competition was to tryout BigQuery (BQ) including the basic ML features. Therefore the notebooks rely as much as possible on BQ. All features are generated in BQ with varying SQL-techniques. The prediction model is also build in BQ.\nAdditional resources can be found in my github repo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Art and Diffusion\n\n\n7 min\n\n\n\nArt\n\n\nML\n\n\nSociety\n\n\n\n\nJohannes Tomasoni\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTackling Projects like a ML Challenge\n\n\n9 min\n\n\n\nProject Management\n\n\nML\n\n\nBusiness\n\n\n\n\nJohannes Tomasoni\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlat to hierarchie\n\n\n0 min\n\n\n\nTabular\n\n\nEDA\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning along\n\n\n1 min\n\n\n\nTabular\n\n\nEDA\n\n\nBasics\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nJan 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes about Sequence Modelling\n\n\n3 min\n\n\n\nTime Series\n\n\nML\n\n\nCompetition\n\n\n\n\nJohannes Tomasoni\n\n\nNov 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel allocation\n\n\n6 min\n\n\n\nBasics\n\n\nML\n\n\nEconomics\n\n\n\n\nJohannes Tomasoni\n\n\nAug 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStreaming ML training progress to a smart watch\n\n\n10 min\n\n\n\nAPI\n\n\nML\n\n\nSide Project\n\n\n\n\nJohannes Tomasoni\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifying Hotels\n\n\n1 min\n\n\n\nVision\n\n\nML\n\n\nCompetition\n\n\n\n\nJohannes Tomasoni\n\n\nMay 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBending the space with nonlinearity\n\n\n0 min\n\n\n\nBasics\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nApr 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeaths by age group and states in Germany from 2016 to 2021\n\n\n5 min\n\n\n\nEDA\n\n\nSociety\n\n\n\n\nJohannes Tomasoni\n\n\nApr 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically translate blog posts\n\n\n2 min\n\n\n\nNLP\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nDec 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA handful of bricks - from SQL to Pandas\n\n\n25 min\n\n\n\nSQL\n\n\nPandas\n\n\nBigQuery\n\n\nML\n\n\n\n\nJohannes Tomasoni\n\n\nDec 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBigQuery-Geotab Intersection Congestion\n\n\nüèÜ\n\n\n0 min\n\n\n\nSQL\n\n\nBigQuery\n\n\nML\n\n\nCompetition\n\n\n\n\nJohannes Tomasoni\n\n\nJan 5, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  }
]